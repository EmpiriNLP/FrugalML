# -*- coding: utf-8 -*-
"""FinQA Distill.ipynb

Automatically generated by Colab.

Count of records per file
"""

import glob
import json

for filename in glob.glob('*.json'):
    with open(filename, 'r', encoding='utf-8') as file:
        data = json.load(file)
        print(f'{filename}: {len(data)} entries')

"""Basic EDA"""

import json
import re
from pandas import json_normalize

def basic_json_eda(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f'{file_path}: {len(data)} entries')

    # Flatten JSON data
    df = json_normalize(data)

    # Filter out columns ending with '_\\d+'
    regex = re.compile(r'.*_\d+$')
    filtered_columns = [col for col in df.columns if not regex.match(col)]
    df_filtered = df[filtered_columns]

    print("Filtered DataFrame Shape:", df_filtered.shape)
    print("\nFiltered DataFrame Columns:\n", df_filtered.columns.tolist())

    # Cardinality and type of each column
    print("\nColumn Cardinality and Data Types:")
    for col in df_filtered.columns:
        # Check if column contains lists and handle nunique() accordingly
        if df_filtered[col].apply(lambda x: isinstance(x, list)).any():
            print(f"{col}: Type={df_filtered[col].dtype}, Cardinality=Contains lists (nunique not applicable)")
        else:
            print(f"{col}: Type={df_filtered[col].dtype}, Cardinality={df_filtered[col].nunique()}")

    # First 10 rows of the filtered DataFrame
    # print("\nFirst 10 Rows of Filtered DataFrame:\n", df_filtered.head(10))

# Usage example:
for filename in glob.glob('*.json'):
    with open(filename, 'r', encoding='utf-8') as file:
        basic_json_eda(filename)

"""finqa_clean"""

!pip install dotenv

import json
from dotenv import load_dotenv
import os

load_dotenv()
DATASET_DIR = "dataset/"

#sets = ["train", "dev", "test"]
sets = ["dev"]
for set in sets:
    data_path = DATASET_DIR + f"{set}.json"
    to_path = DATASET_DIR + f"{set}.cleaned.json"

    with open(data_path, "r") as f:
        data = json.load(f)

    print(f"Original {set} data size: {len(data)}")
    cleaned_data = []
    for element in data:
        # Only keep ["qa"]["question"], ["qa"]["answer"], ["table"], ["pre_text"], ["post_text"] if exists
        # Remove if ["qa"]["question"] or ["qa"]["answer"] is empty
        if not element["qa"]["question"].strip() or not element["qa"]["answer"].strip():
            continue

        cleaned_element = {
            "qa": {
                "question": str(element["qa"]["question"]).strip(),
                "answer": str(element["qa"]["answer"]).strip(),
            },
            "table": element.get("table", []),
            "pre_text": element.get("pre_text", []),
            "post_text": element.get("post_text", [])
        }
        cleaned_data.append(cleaned_element)

    print(f"Cleaned {set} data size: {len(cleaned_data)}")

    with open(to_path, "w") as f:
        json.dump(cleaned_data, f, indent=4)

"""Shared functions"""

import json
import re

def extract_number(text):
    match = re.search(r'[-+]?\d*\.?\d+', text)
    return match.group() if match else None

def extract_last_number(text):
    matches = re.findall(r'[-+]?\d*\.?\d+', text)
    return matches[-1] if matches else None

def count_decimal_places(num_str):
    if '.' in num_str:
        return len(num_str.split('.')[1])
    return 0

def match_answers(expect, predict):
    expect_n = extract_last_number(expect.strip())
    predict_n = extract_last_number(predict.strip())
    if expect_n is None or predict_n is None:
        return False

    expect_decimals = count_decimal_places(expect_n)
    predict_decimals = count_decimal_places(predict_n)

    min_decimals = min(expect_decimals, predict_decimals)

    expect_formatted = round(float(expect_n), min_decimals)
    predict_formatted = round(float(predict_n), min_decimals)

    matched = (predict_formatted == expect_formatted)
    return matched


def generate_prompt_from_item(item):
    pre_text = '\n'.join([s for s in item.get('pre_text', []) if s.strip() != "."])
    post_text = '\n'.join([s for s in item.get('post_text', []) if s.strip() != "."])

    table_ori = item.get('table', [])
    table_text = ""
    if table_ori:
        table_text = '\n'.join(' | '.join(row) for row in table_ori)


    question = item.get('qa', {}).get('question', '')
    program = item.get('qa', {}).get('program', '')
    answer = item.get('qa', {}).get('answer', '')

    prompt = (
        f"Please read the following passage and then write Python code to answer the question."
        f"The python code should output ONLY the numeric answer with the appropriate unit with NO OTHER information.\n"
#        f"The python code should be well commented documenting the chain of thought for extracting the data, performing the sum, and choosing the output unit.\n"
        f"Respond ONLY with Python code enclosed in ```python```\n"
        f"DO NOT include any comments in the python\n"
        f"TAKE CARE to answer with a number when necessary, otherwise yes or no\n"
        f"Passage:\n{pre_text}\n{table_text}\n{post_text}\n"
        f"Question: {question}\n"
#        f"Answer Hint: Strictly perform the following calculations to arrive at the answer: {program}\n"
#        f"Answer: {answer}\n"
    )

    return prompt

"""prepare **phi**"""

!pip install -q transformers datasets peft accelerate torch bitsandbytes

!pip install -U bitsandbytes

import torch
import bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

#from huggingface_hub import notebook_login
#notebook_login()

# model_name = "microsoft/Phi-3-mini-4k-instruct"
model_name = "meta-llama/Llama-3.1-8B-Instruct"
from google.colab import userdata
access_token = userdata.get('HF_TOKEN')
#print(access_token)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

'''
# BitsAndBytes config for quantization (optional, recommended for Colab)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
)

# PEFT LoRA config for fine-tuning
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)
'''

# BitsAndBytes for aggressive quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

DEVICE = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)


# Define 4-bit quantization configuration
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enable 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,  # Use FP16 for computation
    bnb_4bit_use_double_quant=True,  # Double quantization for better memory efficiency
    bnb_4bit_quant_type="nf4"  # NormalFloat4, best for Llama models
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map={"": DEVICE},
    quantization_config=quant_config,
    token=access_token
)

# Load model in quantized format with minimal dtype precision
'''
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)
'''

#model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", device_map='auto')


# Smaller LoRA config for lower memory usage
lora_config = LoraConfig(
    r=16,                  # lowered from 16
    lora_alpha=32,        # proportional to r
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

model.to(DEVICE)

"""test phi with no training"""

import json, re, io
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from contextlib import redirect_stdout
import torch, gc

def query_phi3(prompt_text):
    inputs = tokenizer(f"<|user|>\n{prompt_text}<|end|>\n<|assistant|>\n", return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, use_cache=False, max_new_tokens=256)
    #outputs = model.generate(inputs, use_cache=False, max_new_tokens=100)
    #response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
    return response.split("<|assistant|>")[-1].strip()

def query_llama3(prompt_text):

    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=2048).to(DEVICE)
    input_ids = inputs["input_ids"]  # Explicitly access input_ids tensor

    with torch.no_grad():
        outputs = model.generate(**inputs, temperature=0.1, top_k=10, max_new_tokens=256)
    #generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = tokenizer.decode(outputs[:, input_ids.shape[-1]:][0], skip_special_tokens=True).strip()

    return response

#    formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
#    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
#    outputs = model.generate(**inputs, use_cache=True, max_new_tokens=256)
#    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
#    return response

# Simple evaluation function
def evaluate_model():
#    model.eval()

    new_array = []
    match_count = 0
    idx = 0

    with open("test.json", 'r', encoding='utf-8') as f:
        data = json.load(f)

    for item in data[:10]:
        prompt_text = generate_prompt_from_item(item)
        answer = item.get('qa', {}).get('answer', '')
        exe_ans = item.get('qa', {}).get('exe_ans', '')

        response_text = query_llama3(prompt_text).strip()

        #print(response_text)

        code_match = re.search(r"```python(.*?)```", response_text, re.DOTALL)
        python_code = code_match.group(1).strip() if code_match else response_text

        #print("********************")
        #print(python_code)
        #print("********************")

        stdout_capture = io.StringIO()
        success = True
        error_message = None
        try:
            with redirect_stdout(stdout_capture):
                exec(python_code)
        except Exception as e:
            success = False
            error_message = str(e)

        python_output = stdout_capture.getvalue().strip()

        item.update({
            "python_code": python_code,
            "python_output": python_output,
            "python_success": success,
            "python_error_message": error_message
        })
        # Append modified element to the new array
        new_array.append(item)

        matched = match_answers(answer, python_output)
        match_count += matched
        idx += 1

        print(f"Element {idx}: Output={python_output}, Expected={answer}, Match={matched}")
    #    print("-" * 20)

    with open("output_with_phi3.json", "w") as outfile:
        json.dump(new_array, outfile, indent=2)

    print(f"\nTotal matches: {match_count} out of {idx}")
    print("Processing complete. Output saved to test_with_phi3.json.")
#    model.train()

# Load Phi-3 model
#tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
#model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", device_map='auto')
evaluate_model()

# Clear GPU memory
del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()
torch.cuda.ipc_collect()
import os
os.kill(os.getpid(), 9)

"""Distillation"""

!pip install openai

import json
from openai import OpenAI
import re
import io
from contextlib import redirect_stdout

OPENAI_KEY=userdata.get("OPENAI_KEY")
client = OpenAI(api_key=OPENAI_KEY)

import json, re, io
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from contextlib import redirect_stdout
import torch, gc

# Load Phi-3 model
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", device_map='auto')

def query_phi3(prompt_text):
    inputs = tokenizer(f"<|user|>\n{prompt_text}<|end|>\n<|assistant|>\n", return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512)
    #response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
    return response.split("<|assistant|>")[-1].strip()

new_array = []
match_count = 0
count = 0

with open("test.json", 'r', encoding='utf-8') as f:
    data = json.load(f)

fromIdx = 0
toIdx = 1000

for item in data[fromIdx:toIdx]:
    prompt_text = generate_prompt_from_item(item)
    answer = item.get('qa', {}).get('answer', '')
    exe_ans = item.get('qa', {}).get('exe_ans', '')

    #response_text = query_phi3(prompt_text).strip()

    completion = client.chat.completions.create(
      #model="gpt-3.5-turbo",  # ensure you have access
      model="gpt-4o",  # ensure you have access

      messages=[{"role": "user", "content": prompt_text}]
    )
    # print(completion.choices[0].message.content)

    response_text = completion.choices[0].message.content

    #print(response_text)

    code_match = re.search(r"```python(.*?)```", response_text, re.DOTALL)
    python_code = code_match.group(1).strip() if code_match else response_text

    #print("********************")
    #print(python_code)
    #print("********************")

    stdout_capture = io.StringIO()
    success = True
    error_message = None
    try:
        with redirect_stdout(stdout_capture):
            exec(python_code)
    except Exception as e:
        success = False
        error_message = str(e)

    python_output = stdout_capture.getvalue().strip()

    item.update({
        "python_code": python_code,
        "python_output": python_output,
        "python_success": success,
        "python_error_message": error_message
    })
    # Append modified element to the new array
    new_array.append(item)

    matched = match_answers(answer, python_output)
    match_count += matched
    count += 1

    print(f"Element {count}: Output={python_output}, Expected={answer}, Match={matched}")
#    print("-" * 20)

with open("output_with_gpt.json", "w") as outfile:
    json.dump(new_array, outfile, indent=2)

print(f"\nTotal matches: {match_count} out of {count}")
print("Processing complete. Output saved to output_with_gpt.json.")

# Write the updated array to a new JSON file
with open("output_with_python-280.json", "w") as outfile:
    json.dump(new_array, outfile, indent=2)


print(f"\nTotal matches: {match_count} out of {idx}")
print("Processing complete. Output saved to output_with_python.json.")

"""load and tokenize ft data




"""

import torch
from torch.utils.data import DataLoader
from transformers import DataCollatorForLanguageModeling

# Formatting function for prompt-answer pairs
def format_instruction(sample):
    return f"<|user|>\n{sample['prompt']}<|end|>\n<|assistant|>\n{sample['answer']}<|end|>"

# Tokenize the dataset
def tokenize_data(samples):
    formatted_texts = [format_instruction(s) for s in samples]
    tokenized = tokenizer(formatted_texts, padding=True, truncation=True, return_tensors="pt")
    return tokenized

# Create tokenized dataset
#tokenized_dataset = tokenize_data(data)

# DataLoader preparation
#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
#train_loader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)

"""finetune"""

!pip install torch tqdm transformers accelerate

import json
import torch
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm
from transformers import DataCollatorForLanguageModeling

# --- Efficient Data Loading (Generator-based) ---
def prompt_answer_generator(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        items = json.load(file)
    for item in items:

        if item.get('python_success') is True:
            original_code = item.get('python_code', '')
            processed_code = '\n'.join(
                line.split('#', 1)[0].rstrip()
                for line in original_code.splitlines()
                if line.strip() and not line.strip().startswith('#')
            )
            #print("Processed Code:")
            #print(processed_code)
            yield {
                "prompt": generate_prompt_from_item(item),
                "answer": processed_code
            }
            '''
        if item.get('python_success') is True:
            yield {
                "prompt": generate_prompt_from_item(item),
                "answer": item.get('python_code', '')
            }
            '''

def format_instruction(sample):
    return f"<|user|>\n{sample['prompt']}<|end|>\n<|assistant|>\n{sample['answer']}<|end|>"

class PromptDataset(Dataset):
    def __init__(self, filepath, tokenizer):
        self.filepath = filepath
        self.tokenizer = tokenizer
        self.samples = list(prompt_answer_generator(filepath))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        formatted = format_instruction(self.samples[idx])
        tokenized = self.tokenizer(
          formatted,
          truncation=True,
          max_length=512,
          padding='max_length'
        )
        return tokenized

# --- Load dataset with minimal memory ---
dataset = PromptDataset('output_with_gpt.json', tokenizer)
# Add padding token to tokenizer
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer)) # Update the model's embeddings if you're adding a padding token on the fly

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
train_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=data_collator)

optimizer = AdamW(model.parameters(), lr=5e-5)
gradient_accumulation_steps = 8
optimizer.zero_grad()

running_loss = 0.0
for epoch in range(1, 2):
    print(f"Epoch {epoch}")
    for step, batch in enumerate(tqdm(train_loader), 1):
        batch = {k: v.to(model.device) for k, v in batch.items()}
        outputs = model(**batch, use_cache=False)

        loss = outputs.loss / gradient_accumulation_steps
        loss.backward()

        running_loss += loss.item()

        if step % gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
            torch.cuda.empty_cache()

        if step % 100 == 0:
            avg_loss = running_loss / 100
            print(f"Step {step}, Avg Loss: {avg_loss:.4f}")
            running_loss = 0.0

evaluate_model()
