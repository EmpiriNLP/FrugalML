{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects2/aisd/2024/shekchu/miniconda/envs/snlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights are stored in: /cs/student/projects2/aisd/2024/shekchu/snlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.03s/it]\n",
      "/cs/student/projects2/aisd/2024/shekchu/miniconda/envs/snlp/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_113831/2821246660.py:128: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>11.992100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10:\n",
      "  Total FLOPs: 363,207,474,996,226\n",
      "  CUDA Time: 568083631.18 ms\n",
      "  CPU Time: 574030368.80 ms\n",
      "  Memory Usage: 16719.23 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113831/2821246660.py:203: FutureWarning: `cuda_time` is deprecated, please use `device_time` instead.\n",
      "  if hasattr(event, 'cuda_time'):\n",
      "/tmp/ipykernel_113831/2821246660.py:204: FutureWarning: `cuda_time` is deprecated, please use `device_time` instead.\n",
      "  total_cuda_time_ms += event.cuda_time * 1000  # Convert to ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 20:\n",
      "  Total FLOPs: 299,007,716,303,378\n",
      "  CUDA Time: 465780431.31 ms\n",
      "  CPU Time: 479812599.04 ms\n",
      "  Memory Usage: 16719.23 MB\n",
      "\n",
      "Step 30:\n",
      "  Total FLOPs: 271,341,521,949,554\n",
      "  CUDA Time: 419381649.46 ms\n",
      "  CPU Time: 430611644.32 ms\n",
      "  Memory Usage: 16719.23 MB\n",
      "\n",
      "FLOP measurements have been saved to training_flops_profiler_v2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.profiler\n",
    "from typing import Callable\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from functools import wraps\n",
    "import csv\n",
    "\n",
    "\n",
    "# Custom data collator for dynamic padding\n",
    "class CustomDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer, label_pad_token_id=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract and pad labels dynamically\n",
    "        labels = [feature.pop(\"labels\") for feature in features]\n",
    "        batch = self.tokenizer.pad(features, return_tensors=\"pt\")\n",
    "        \n",
    "        max_label_length = max(len(l) for l in labels)\n",
    "        padded_labels = [\n",
    "            l + [self.label_pad_token_id] * (max_label_length - len(l)) \n",
    "            for l in labels\n",
    "        ]\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels)\n",
    "        return batch\n",
    "\n",
    "# -----------------------------\n",
    "# Set cache directory and load model/tokenizer\n",
    "# -----------------------------\n",
    "os.environ['HF_HOME'] = '/cs/student/projects2/aisd/2024/shekchu/snlp'\n",
    "cache_dir = os.getenv('HF_HOME', 'Cache directory not set')\n",
    "print(f\"Model weights are stored in: {cache_dir}\")\n",
    "\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.config.use_cache = False  # Disable caching for gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Setup LoRA configuration\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                   \n",
    "    lora_alpha=32,         \n",
    "    lora_dropout=0.1,      \n",
    "    target_modules=[\"qkv_proj\"]  \n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Dataset\n",
    "# -----------------------------\n",
    "from dataset.dataset import FinQADataset\n",
    "train_dataset = FinQADataset(\"./dataset/train.json\")\n",
    "\n",
    "class TokenizedFinQADataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=4096):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        # Concatenate prompt and label (the answer)\n",
    "        full_text = item[\"prompt\"].strip() + \"\\n\" + item[\"label\"].strip()\n",
    "        \n",
    "        # Tokenize without converting to tensors so the collator can handle padding.\n",
    "        tokenized = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            # No static padding and no return_tensors.\n",
    "        )\n",
    "        \n",
    "        # Create labels from input_ids (copying the list)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "tokenized_train_dataset = TokenizedFinQADataset(train_dataset, tokenizer, max_length=2048)\n",
    "\n",
    "# -----------------------------\n",
    "# Use our custom collator for dynamic padding\n",
    "# -----------------------------\n",
    "data_collator = CustomDataCollatorWithPadding(tokenizer, label_pad_token_id=-100)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Arguments and Trainer\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=30,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Train the model\n",
    "# -----------------------------\n",
    "import torch\n",
    "import torch.profiler\n",
    "import csv\n",
    "import os\n",
    "from typing import Callable\n",
    "from functools import wraps\n",
    "\n",
    "def profile_training_flops_by_steps(csv_path=\"flops_per_step.csv\", log_every_n_steps=10):\n",
    "    \"\"\"\n",
    "    A decorator/wrapper function to profile FLOPs during model training,\n",
    "    logging results every N steps to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to save the CSV log file\n",
    "        log_every_n_steps (int): How frequently to log FLOP measurements\n",
    "        \n",
    "    Returns:\n",
    "        A decorator function that profiles the training process\n",
    "    \"\"\"\n",
    "    def decorator(training_func: Callable):\n",
    "        @wraps(training_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Get the trainer from args (assuming first arg is trainer)\n",
    "            trainer = args[0]\n",
    "            original_training_step = trainer.training_step\n",
    "            \n",
    "            # Create CSV file and write header\n",
    "            os.makedirs(os.path.dirname(csv_path) or '.', exist_ok=True)\n",
    "            with open(csv_path, 'w', newline='') as csvfile:\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                csv_writer.writerow(['Step', 'FLOPs', 'CUDA Time (ms)', 'CPU Time (ms)', 'Memory (MB)'])\n",
    "            \n",
    "            # Step counter\n",
    "            step_counter = [0]  # Using list for nonlocal access\n",
    "            \n",
    "            # Override the training_step method to add profiling\n",
    "            def profiled_training_step(*step_args, **step_kwargs):\n",
    "                step_counter[0] += 1\n",
    "                current_step = step_counter[0]\n",
    "                \n",
    "                # Only profile every N steps\n",
    "                if current_step % log_every_n_steps == 0:\n",
    "                    with torch.profiler.profile(\n",
    "                        activities=[\n",
    "                            torch.profiler.ProfilerActivity.CPU,\n",
    "                            torch.profiler.ProfilerActivity.CUDA\n",
    "                        ],\n",
    "                        record_shapes=True,\n",
    "                        profile_memory=True,\n",
    "                        with_flops=True\n",
    "                    ) as prof:\n",
    "                        result = original_training_step(*step_args, **step_kwargs)\n",
    "                    \n",
    "                    # Calculate total FLOPs\n",
    "                    total_flops = 0\n",
    "                    total_cuda_time_ms = 0\n",
    "                    total_cpu_time_ms = 0\n",
    "                    \n",
    "                    for event in prof.key_averages():\n",
    "                        # Accumulate FLOPs\n",
    "                        if hasattr(event, 'flops') and event.flops > 0:\n",
    "                            total_flops += event.flops\n",
    "                        \n",
    "                        # Accumulate CUDA time (converted to ms)\n",
    "                        if hasattr(event, 'cuda_time'):\n",
    "                            total_cuda_time_ms += event.cuda_time * 1000  # Convert to ms\n",
    "                        \n",
    "                        # Accumulate CPU time (converted to ms)\n",
    "                        if hasattr(event, 'cpu_time'):\n",
    "                            total_cpu_time_ms += event.cpu_time * 1000  # Convert to ms\n",
    "                    \n",
    "                    # Calculate memory usage in MB\n",
    "                    memory_usage_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "                    \n",
    "                    # Log to CSV\n",
    "                    with open(csv_path, 'a', newline='') as csvfile:\n",
    "                        csv_writer = csv.writer(csvfile)\n",
    "                        csv_writer.writerow([\n",
    "                            current_step, \n",
    "                            total_flops, \n",
    "                            total_cuda_time_ms, \n",
    "                            total_cpu_time_ms,\n",
    "                            memory_usage_mb\n",
    "                        ])\n",
    "                    \n",
    "                    # Print to console\n",
    "                    print(f\"\\nStep {current_step}:\")\n",
    "                    print(f\"  Total FLOPs: {total_flops:,}\")\n",
    "                    print(f\"  CUDA Time: {total_cuda_time_ms:.2f} ms\")\n",
    "                    print(f\"  CPU Time: {total_cpu_time_ms:.2f} ms\")\n",
    "                    print(f\"  Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "                    \n",
    "                    # Reset peak memory stats for next iteration\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                    \n",
    "                    return result\n",
    "                else:\n",
    "                    # Run without profiling for other steps\n",
    "                    return original_training_step(*step_args, **step_kwargs)\n",
    "            \n",
    "            # Replace the training_step method\n",
    "            trainer.training_step = profiled_training_step\n",
    "            \n",
    "            try:\n",
    "                # Execute the original training function\n",
    "                result = training_func(*args, **kwargs)\n",
    "                return result\n",
    "            finally:\n",
    "                # Restore original training_step method\n",
    "                trainer.training_step = original_training_step\n",
    "                print(f\"\\nFLOP measurements have been saved to {csv_path}\")\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage with the Transformers Trainer:\n",
    "@profile_training_flops_by_steps(csv_path=\"training_flops_profiler_v2.csv\", log_every_n_steps=10)\n",
    "def train_model(trainer):\n",
    "    return trainer.train()\n",
    "\n",
    "# Replace your existing trainer.train() call with:\n",
    "train_model(trainer)\n",
    "\n",
    "# Save the fine-tuned model (including LoRA adapters)\n",
    "model.save_pretrained(\"./lora_finetuned_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
