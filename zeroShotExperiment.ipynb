{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7df2bf70c594262b3f3fc3d797cfa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n",
      "Dataset loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148719/2634805306.py:120: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n",
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/datasets/load.py:759: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example 0: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 10.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.78 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example 1: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 2: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 3: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 4: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 5: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 6: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 7: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 8: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing example 9: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 50.12 MiB is free. Process 145777 has 12.70 GiB memory in use. Including non-PyTorch memory, this process has 10.74 GiB memory in use. Of the allocated memory 9.19 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 171\u001b[0m\n\u001b[1;32m    168\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(results, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 150\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(test_data, model, tokenizer, num_samples)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Overall Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/cs/student/projects2/aisd/2024/giliev/miniconda3/envs/finqaEnv/lib/python3.9/site-packages/datasets/metric.py:455\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[0;32m--> 455\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/metrics/squad/17251fdf0fee19c22e38bea85ee9d91fba9f81547983e13e6b89b6abfeceeff0/squad.py:108\u001b[0m, in \u001b[0;36mSquad._compute\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m     92\u001b[0m pred_dict \u001b[38;5;241m=\u001b[39m {prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]: prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions}\n\u001b[1;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     94\u001b[0m     {\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     }\n\u001b[1;32m    107\u001b[0m ]\n\u001b[0;32m--> 108\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/metrics/squad/17251fdf0fee19c22e38bea85ee9d91fba9f81547983e13e6b89b6abfeceeff0/evaluate.py:70\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, predictions)\u001b[0m\n\u001b[1;32m     67\u001b[0m             exact_match \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n\u001b[1;32m     68\u001b[0m             f1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n\u001b[0;32m---> 70\u001b[0m exact_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexact_match\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[1;32m     71\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m f1 \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m: exact_match, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1}\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Model initialization\n",
    "def initialize_model():\n",
    "    model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Initialize with 4-bit quantization for better memory efficiency\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "# Data loading\n",
    "def load_finqa_dataset(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_example(example, tokenizer):\n",
    "    \"\"\"Prepare structured input for Phi-3\"\"\"\n",
    "    question = example[\"qa\"].get(\"question\", \"No question available.\")\n",
    "    table = example.get(\"table\", [])\n",
    "    table_str = \"\\n\".join([\" | \".join(row) for row in table])\n",
    "    \n",
    "    pre_text = \" \".join(example.get(\"pre_text\", []))\n",
    "    post_text = \" \".join(example.get(\"post_text\", []))\n",
    "    \n",
    "    gold_inds = example[\"qa\"].get(\"gold_inds\", {})\n",
    "    relevant_info = \"\\n\".join(gold_inds.values())\n",
    "    program = example[\"qa\"].get(\"program\", \"\")\n",
    "    \n",
    "    input_text = (\n",
    "        \"You are a financial calculator. Follow these steps:\\n\"\n",
    "        \"1. Read the question carefully\\n\"\n",
    "        \"2. Look at the relevant information and table data\\n\"\n",
    "        \"3. Follow the mathematical operation exactly\\n\"\n",
    "        \"4. Return ONLY the final numerical answer with no text\\n\\n\"\n",
    "        f\"Relevant Information:\\n{relevant_info}\\n\\n\"\n",
    "        f\"Table Data:\\n{table_str}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Mathematical Operation: {program}\\n\"\n",
    "        \"Final Answer (number only): \"\n",
    "    )\n",
    "\n",
    "    return tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def clean_answer(text):\n",
    "    \"\"\"Extract and format numerical answer\"\"\"\n",
    "    if ':' in text:\n",
    "        text = text.split(':')[-1]\n",
    "    \n",
    "    # Handle percentages first\n",
    "    percent_match = re.search(r'[-+]?\\d*\\.?\\d+\\s*%?', text)\n",
    "    if percent_match:\n",
    "        number = float(percent_match.group(0).replace('%', '').strip())\n",
    "        # If the number is small (likely decimal), convert to percentage\n",
    "        if number < 1:\n",
    "            number *= 100\n",
    "        # Round to one decimal place and add % symbol\n",
    "        return f\"{round(number, 1)}%\"\n",
    "    \n",
    "    # Handle regular numbers\n",
    "    decimal_match = re.search(r'[-+]?\\d*\\.?\\d+', text)\n",
    "    if decimal_match:\n",
    "        number = float(decimal_match.group(0))\n",
    "        # If it's close to an integer, round it\n",
    "        if abs(round(number) - number) < 0.01:\n",
    "            return str(round(number))\n",
    "        # Otherwise, round to one decimal place\n",
    "        return str(round(number, 1))\n",
    "    \n",
    "    # Handle yes/no answers\n",
    "    text = text.lower().strip()\n",
    "    if 'yes' in text or 'true' in text:\n",
    "        return 'yes'\n",
    "    if 'no' in text or 'false' in text:\n",
    "        return 'no'\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def generate_answer(example, model, tokenizer):\n",
    "    \"\"\"Generate answer using Phi-3\"\"\"\n",
    "    inputs = preprocess_example(example, tokenizer)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return clean_answer(generated_text)\n",
    "\n",
    "def evaluate_model(test_data, model, tokenizer, num_samples=10):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    metric = load_metric(\"squad\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for i, example in enumerate(test_data[:num_samples]):\n",
    "        try:\n",
    "            pred_text = generate_answer(example, model, tokenizer)\n",
    "            true_text = example[\"qa\"][\"answer\"]\n",
    "            \n",
    "            predictions.append({\"id\": str(i), \"prediction_text\": pred_text})\n",
    "            references.append({\"id\": str(i), \"answers\": {\"text\": [true_text], \"answer_start\": [0]}})\n",
    "            \n",
    "            em = 1 if pred_text.strip() == true_text.strip() else 0\n",
    "            f1 = metric.compute(\n",
    "                predictions=[{\"id\": str(i), \"prediction_text\": pred_text}],\n",
    "                references=[{\"id\": str(i), \"answers\": {\"text\": [true_text], \"answer_start\": [0]}}]\n",
    "            )[\"f1\"]\n",
    "            \n",
    "            print(f\"\\nðŸ”¹ Example {i+1}\")\n",
    "            print(f\"â“ Question: {example['qa']['question']}\")\n",
    "            print(f\"âœ… Ground Truth: {true_text}\")\n",
    "            print(f\"ðŸ¤– Prediction: {pred_text}\")\n",
    "            print(f\"ðŸ“Š Metrics - Exact Match: {em}, F1: {f1:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    print(\"\\nðŸ“Š Overall Results:\", results)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Initialize model and tokenizer\n",
    "    model, tokenizer = initialize_model()\n",
    "    print(\"Model initialized successfully!\")\n",
    "    \n",
    "    # Load datasets\n",
    "    test_data = load_finqa_dataset(\"/cs/student/projects2/aisd/2024/giliev/FinQA/dataset/test.json\")\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(test_data, model, tokenizer, num_samples=10)\n",
    "    \n",
    "    # Save results\n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finqaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
