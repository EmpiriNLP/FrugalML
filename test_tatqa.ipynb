{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tatqa_dataset(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_data = load_tatqa_dataset(\"/cs/student/projects1/aibh/2024/tpatil/comp0087/FrugalML/datasets/TATQA/tatqa_dataset_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['', '2019 %', '2018 %', '2017 %'], ['Weighted average actuarial assumptions used at 31 March1:', '', '', ''], ['Rate of inflation2', '2.9', '2.9', '3.0'], ['Rate of increase in salaries', '2.7', '2.7', '2.6'], ['Discount rate', '2.3', '2.5', '2.6']]\n"
     ]
    }
   ],
   "source": [
    "examples = train_data[0]\n",
    "\n",
    "print(examples[\"table\"][\"table\"])\n",
    "# for example in examples:\n",
    "#     # print((example[\"table\"]).get(\"table\", []))\n",
    "#     print(example[\"table\"][\"table\"])\n",
    "#     # print(example[\"question\"])\n",
    "#     # print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_sample in train_data:\n",
    "    table = train_sample['table']['table']\n",
    "    paragraphs = train_sample['paragraphs']\n",
    "    questions = train_sample['questions']\n",
    "\n",
    "    for question_answer in questions:\n",
    "        try:\n",
    "            question = question_answer[\"question\"].strip()\n",
    "            answer = question_answer[\"answer\"]\n",
    "            derivation = question_answer[\"derivation\"]\n",
    "            answer_type = question_answer[\"answer_type\"]\n",
    "            answer_from = question_answer[\"answer_from\"]\n",
    "            facts = question_answer[\"facts\"]\n",
    "            answer_mapping = question_answer[\"mapping\"]\n",
    "            scale = question_answer[\"scale\"]\n",
    "        \n",
    "        except RuntimeError as e :\n",
    "            print(f\"run time error:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_instance(table, paragraphs, question, answer, derivation=None):\n",
    "    set_context = \"You are an intelligent financial data analyst. You are given a table with financial data. You are also given a paragraph that provides some context about the data in the table. You are asked a question about the data in the table or paragraph. You are expected to answer the question based on the data in the table and the paragraph.\\n\"\n",
    "    # table_prompt = \"The first element of the table array contains the column names. In the following elements, the first element is the row name and the rest of the elements are the values in the row assigned to the respective columns.\"\n",
    "    table_prompt = \"The table provide the financial data. All the elements in the table are separated by \\\"|\\\". The first row of the table contains the column names. In the following rows, the first column contains the row name and the rest of the elements are the values in the row assigned to the respective columns. Interpret the table and use the data in it to calculate the answer to the provided quesions.\\n\"\n",
    "    paragraph_prompt = \"The paragraphs provides some context about the data in the table. It may contain information that is not present in the table. It may also contain some numbers which might require arithmatic processing to get the answer. There may be multiple paragraphs separated by keyword matching \\\"Paragraph [0-9]+:\\\". Interpret each paragraph and use the data and description in it to infer the answer to the provided quesions.\\n\"\n",
    "    question_prompt = \"The question is asked based on the data in the table and the paragraph. You are expected to answer the question based on the data in the table and the paragraph.\\n\"\n",
    "    answer_prompt = \"\" #\"You are expected to answer the question based on the data in the table and the paragraph. Provide only the answer to the question and do not repeat the question. Use the answers provided as labels to learn the correct way to answer the question.\\n\"\n",
    "    # derivation_prompt = \"The derivation provides the steps to calculate the answer to the question. You can learn how to use the derivation to calculate the answer to the question.\\n\"\n",
    "    # answer_type_prompt = \"The answer type is the type of the answer to the question, and informs how answer was constructed.\\n\"\n",
    "    # answer_from_prompt = \"The answer from is the source of the answer to the question. It can be from table, paragraph, or both.\\n\"\n",
    "    # facts_prompt = \"The facts are the information that is used to answer the question. It can be from table, paragraph, or both.\\n\"\n",
    "    # answer_mapping_prompt = \"The answer mapping is the mapping of the answer to the question. It can be from table, paragraph, or both. If it is from the table, you will be given the index of the relevant values in the table. If it is from the paragraph, you will be given the start and end indexes of the relevant phrase in the paragraph. All indexes begin with 0.\\n\"\n",
    "    # scale_prompt = \"The scale is the scale of the numerical answer to the question. It can be from table, paragraph, or both.\\n\"\n",
    "    answer_instruction_prompt = \"\\nInstruction: Answer the question based on the data in the table and the paragraph. Provide only the answer to the question and do not repeat the question. Use the answers provided as labels to learn the correct way to answer the question.\\n\"\n",
    "\n",
    "    # table_idx = 1\n",
    "    # for table in tables:\n",
    "    table_prompt += f\"\\nTable:\\n\"\n",
    "    for row in table[\"table\"]:\n",
    "        # table_prompt += \"|\"\n",
    "        table_prompt += \"|\".join([str(cell) for cell in row]) + \" \\n\"\n",
    "        # table_idx += 1\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_prompt += f\"\\nParagraph {paragraph['order']}:\\n\"\n",
    "        paragraph_prompt += paragraph['text'] + \" \"\n",
    "\n",
    "    question_prompt += f\"\\nQuestion:\\n {question}\"\n",
    "    answer_prompt += f\"\\nAnswer:\\n {answer}\"\n",
    "    # derivation_prompt += f\"\\nDerivation:\\n {derivation}\"\n",
    "    # answer_type_prompt += f\"\\nAnswer Type:\\n {answer_type}\"\n",
    "    # answer_from_prompt += f\"\\nAnswer From:\\n {answer_from}\"\n",
    "    # facts_prompt += f\"\\nFacts:\\n {facts}\"\n",
    "    # answer_mapping_prompt += f\"\\nAnswer Mapping:\\n {answer_mapping}\"\n",
    "    # scale_prompt += f\"\\nScale:\\n {scale}\"\n",
    "\n",
    "    return set_context, table_prompt, paragraph_prompt, question_prompt, answer_prompt, answer_instruction_prompt #, derivation_prompt, answer_type_prompt, answer_from_prompt, facts_prompt, answer_mapping_prompt, scale_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TATQADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.qa_pairs = []\n",
    "\n",
    "        for item in self.data:\n",
    "        # item = self.data[idx]\n",
    "            tables = item.get('table', [])\n",
    "            paragraphs = item.get('paragraphs', [])\n",
    "            questions = item.get('questions', [])\n",
    "\n",
    "            for question_answer in questions:\n",
    "                question = question_answer[\"question\"].strip()\n",
    "                answer = question_answer[\"answer\"]\n",
    "                # derivation = question_answer.get(\"derivation\", \"\")\n",
    "                # answer_type = question_answer.get(\"answer_type\", \"\")\n",
    "                # answer_from = question_answer.get(\"answer_from\", \"\")\n",
    "                # facts = question_answer.get(\"facts\", \"\")\n",
    "                # answer_mapping = question_answer.get(\"mapping\", \"\")\n",
    "                # scale = question_answer.get(\"scale\", \"\")\n",
    "                # rel_paragraphs = question_answer.get(\"rel_paragraphs\", \"\")\n",
    "                # req_comparison = question_answer.get(\"req_comparison\", \"\")\n",
    "\n",
    "                set_context, table_prompt, paragraph_prompt, question_prompt, answer_prompt, answer_instruction_prompt = create_prompt_instance(tables, paragraphs, question, answer)\n",
    "                input_context = (set_context + table_prompt + paragraph_prompt + question_prompt + answer_instruction_prompt).strip()\n",
    "                # label_text = (answer_prompt + derivation_prompt + answer_type_prompt + answer_from_prompt + facts_prompt + answer_mapping_prompt + scale_prompt).strip()\n",
    "                label_text = answer_prompt.strip()\n",
    "\n",
    "                # qa_pairs.append({\n",
    "                #     \"input_ids\": inputs.input_ids.squeeze(),\n",
    "                #     \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "                #     \"labels\": labels\n",
    "                # })\n",
    "\n",
    "                self.qa_pairs.append((\n",
    "                    input_context,\n",
    "                    label_text\n",
    "                ))\n",
    "        # print(self.qa_pairs[-1])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_context, label_text = self.qa_pairs[idx]\n",
    "\n",
    "        inputs = self.tokenizer( \n",
    "                    input_context, \n",
    "                    # max_length=self.max_length, \n",
    "                    # truncation=True, \n",
    "                    # padding=\"max_length\", \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "                    label_text, \n",
    "                    # max_length=self.max_length, \n",
    "                    # truncation=True, \n",
    "                    # padding=\"max_length\", \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        labels = labels[\"input_ids\"].squeeze(0)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "            # \"input_context\": input_context,\n",
    "            # \"label_text\": label_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects1/aibh/2024/tpatil/venv_comp0188/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "cache_dir = \"/cs/student/projects1/aibh/2024/tpatil/.cache/huggingface\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace with appropriate model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  887,   526,   385, 13052,   296, 18161,   848,  3483,   858, 29889,\n",
       "            887,   526,  2183,   263,  1591,   411, 18161,   848, 29889,   887,\n",
       "            526,   884,  2183,   263, 14880,   393,  8128,   777,  3030,  1048,\n",
       "            278,   848,   297,   278,  1591, 29889,   887,   526,  4433,   263,\n",
       "           1139,  1048,   278,   848,   297,   278,  1591,   470, 14880, 29889,\n",
       "            887,   526,  3806,   304,  1234,   278,  1139,  2729,   373,   278,\n",
       "            848,   297,   278,  1591,   322,   278, 14880, 29889,    13,  1576,\n",
       "           1591,  3867,   278, 18161,   848, 29889,  2178,   278,  3161,   297,\n",
       "            278,  1591,   526, 13055,   491,   376, 29989,  1642,   450,   937,\n",
       "           1948,   310,   278,  1591,  3743,   278,  1897,  2983, 29889,   512,\n",
       "            278,  1494,  4206, 29892,   278,   937,  1897,  3743,   278,  1948,\n",
       "           1024,   322,   278,  1791,   310,   278,  3161,   526,   278,  1819,\n",
       "            297,   278,  1948,  9859,   304,   278, 18067,  4341, 29889,  4124,\n",
       "          19819,   278,  1591,   322,   671,   278,   848,   297,   372,   304,\n",
       "           8147,   278,  1234,   304,   278,  4944,   439,   267,  1080, 29889,\n",
       "             13,    13,  3562, 29901,    13,  8876, 12883,  2796,   287,   891,\n",
       "          29871,    13, 29989, 29967,  1540, 29871, 29941, 29900, 29892,    13,\n",
       "          29906, 29900, 29896, 29929, 29989, 29967,  1540, 29871, 29941, 29900,\n",
       "          29892, 29906, 29900, 29896, 29947, 29989, 29967,  1540, 29871, 29941,\n",
       "          29900, 29892,    13, 29906, 29900, 29896, 29955, 29871,    13,  1666,\n",
       "           2842,   322,  5849, 29871,   891, 29906, 29896, 29889, 29896, 29995,\n",
       "          29989, 29896, 29947, 29889, 29955, 29995, 29989, 29896, 29945, 29889,\n",
       "          29946, 29995, 29871,    13, 29903,  2122,   322,  9999,   292, 29871,\n",
       "            891, 29906, 29947, 29889, 29955, 29995, 29989, 29906, 29955, 29889,\n",
       "          29906, 29995, 29989, 29906, 29953, 29889, 29947, 29995, 29871,    13,\n",
       "          15263,   322, 19185, 29871,   891, 29945, 29889, 29953, 29995, 29989,\n",
       "          29945, 29889, 29906, 29995, 29989, 29953, 29889, 29906, 29995, 29871,\n",
       "             13, 10644, 23493,   322, 13465, 21544, 29892,  7787,   310,   289,\n",
       "           1191,   475, 20590, 11581, 29871,   891, 29900, 29889, 29941, 29995,\n",
       "          29989, 29945, 29889, 29945, 29995, 29989, 29906, 29889, 29906, 29995,\n",
       "          29871,    13, 15078,  1247,  3864, 21090, 29892,  7787,   310, 18764,\n",
       "           1338, 29871,   891, 29900, 29889, 29945, 29995, 29989, 29900, 29889,\n",
       "          29947, 29995, 29989, 29896, 29889, 29945, 29995, 29871,    13,  6833,\n",
       "            441,  2133,   310,   938,   574, 13876, 29871,   891, 29900, 29889,\n",
       "          29953, 29995, 29989, 29900, 29889, 29929, 29995, 29989, 29896, 29889,\n",
       "          29946, 29995, 29871,    13, 11536, 13598,  1518, 11259, 29989, 29945,\n",
       "          29953, 29889, 29947, 29995, 29989, 29945, 29947, 29889, 29941, 29995,\n",
       "          29989, 29945, 29941, 29889, 29945, 29995, 29871,    13,  7094,  1218,\n",
       "            313,  6758, 29897, 17869, 29871,   891, 29898, 29896, 29889, 29945,\n",
       "          29897, 29995, 29989, 29898, 29941, 29889, 29929, 29897, 29995, 29989,\n",
       "          29896, 29889, 29900, 29995, 29871,    13,  1576, 14880, 29879,  8128,\n",
       "            777,  3030,  1048,   278,   848,   297,   278,  1591, 29889,   739,\n",
       "           1122,  1712,  2472,   393,   338,   451,  2198,   297,   278,  1591,\n",
       "          29889,   739,  1122,   884,  1712,   777,  3694,   607,  1795,  1996,\n",
       "            564,   389, 29885,  2454,  9068,   304,   679,   278,  1234, 29889,\n",
       "           1670,  1122,   367,  2999, 14880, 29879, 13055,   491, 13553,  9686,\n",
       "            376,  2177,  9895,   518, 29900, 29899, 29929, 10062, 29901,  1642,\n",
       "           4124, 19819,  1269, 14880,   322,   671,   278,   848,   322,  6139,\n",
       "            297,   372,   304, 10115,   278,  1234,   304,   278,  4944,   439,\n",
       "            267,  1080, 29889,    13,    13,  2177,  9895, 29871, 29896, 29901,\n",
       "             13,  7094,  1218, 12027, 11259, 29871,    13,  2177,  9895, 29871,\n",
       "          29906, 29901,    13,  1576,  1494,  1591, 12141, 29879,  1749, 13598,\n",
       "           1518, 11259,   322, 13598,  6410,   408,   263, 19649,   310,  7787,\n",
       "          20957,  1041, 29901,   450,  1139,   338,  4433,  2729,   373,   278,\n",
       "            848,   297,   278,  1591,   322,   278, 14880, 29889,   887,   526,\n",
       "           3806,   304,  1234,   278,  1139,  2729,   373,   278,   848,   297,\n",
       "            278,  1591,   322,   278, 14880, 29889,    13,    13, 16492, 29901,\n",
       "             13,  8449,  2440,   947,   278,  1591,  3867,  2472,   363,   278,\n",
       "           5001, 29915, 29879, 13598,  1518, 11259,   322, 13598,  6410,   408,\n",
       "            263, 19649,   310,  7787, 20957,  1041, 29973,    13,  3379,  4080,\n",
       "          29901,   673,   278,  1139,  2729,   373,   278,   848,   297,   278,\n",
       "           1591,   322,   278, 14880, 29889,  9133,   680,   871,   278,  1234,\n",
       "            304,   278,  1139,   322,   437,   451, 12312,   278,  1139, 29889,\n",
       "           4803,   278,  6089,  4944,   408, 11073,   304,  5110,   278,  1959,\n",
       "            982,   304,  1234,   278,  1139, 29889]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[  673, 29901,    13,  6024, 29906, 29900, 29896, 29929,   742,   525,\n",
       "          29906, 29900, 29896, 29947,   742,   525, 29906, 29900, 29896, 29955,\n",
       "           2033]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TATQADataset(train_data, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1, \n",
    "    shuffle=True\n",
    "    )\n",
    "# next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_comp0188",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
