{
  "best_metric": 2.984109401702881,
  "best_model_checkpoint": "./models/adapter/checkpoint-24812",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 24812,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016121231662098985,
      "grad_norm": 144.0,
      "learning_rate": 1e-05,
      "loss": 12.05,
      "step": 10
    },
    {
      "epoch": 0.003224246332419797,
      "grad_norm": 69.0,
      "learning_rate": 2e-05,
      "loss": 11.25,
      "step": 20
    },
    {
      "epoch": 0.004836369498629695,
      "grad_norm": 102.5,
      "learning_rate": 3e-05,
      "loss": 7.975,
      "step": 30
    },
    {
      "epoch": 0.006448492664839594,
      "grad_norm": 94.5,
      "learning_rate": 4e-05,
      "loss": 7.3281,
      "step": 40
    },
    {
      "epoch": 0.008060615831049492,
      "grad_norm": 180.0,
      "learning_rate": 5e-05,
      "loss": 6.3555,
      "step": 50
    },
    {
      "epoch": 0.00967273899725939,
      "grad_norm": 56.5,
      "learning_rate": 6e-05,
      "loss": 8.5938,
      "step": 60
    },
    {
      "epoch": 0.01128486216346929,
      "grad_norm": 62.5,
      "learning_rate": 7e-05,
      "loss": 7.1672,
      "step": 70
    },
    {
      "epoch": 0.012896985329679188,
      "grad_norm": 67.5,
      "learning_rate": 8e-05,
      "loss": 8.2832,
      "step": 80
    },
    {
      "epoch": 0.014509108495889086,
      "grad_norm": 87.5,
      "learning_rate": 9e-05,
      "loss": 5.6344,
      "step": 90
    },
    {
      "epoch": 0.016121231662098983,
      "grad_norm": 0.0,
      "learning_rate": 0.0001,
      "loss": 6.7564,
      "step": 100
    },
    {
      "epoch": 0.017733354828308883,
      "grad_norm": 38.5,
      "learning_rate": 9.99892409489483e-05,
      "loss": 7.1434,
      "step": 110
    },
    {
      "epoch": 0.01934547799451878,
      "grad_norm": 51.0,
      "learning_rate": 9.997848189789661e-05,
      "loss": 8.675,
      "step": 120
    },
    {
      "epoch": 0.02095760116072868,
      "grad_norm": 0.0,
      "learning_rate": 9.996772284684491e-05,
      "loss": 6.7684,
      "step": 130
    },
    {
      "epoch": 0.02256972432693858,
      "grad_norm": 13.0625,
      "learning_rate": 9.995696379579322e-05,
      "loss": 6.0563,
      "step": 140
    },
    {
      "epoch": 0.024181847493148476,
      "grad_norm": 26.875,
      "learning_rate": 9.994620474474153e-05,
      "loss": 5.2938,
      "step": 150
    },
    {
      "epoch": 0.025793970659358376,
      "grad_norm": 11.875,
      "learning_rate": 9.993544569368982e-05,
      "loss": 6.9656,
      "step": 160
    },
    {
      "epoch": 0.027406093825568273,
      "grad_norm": 14.875,
      "learning_rate": 9.992468664263812e-05,
      "loss": 5.0875,
      "step": 170
    },
    {
      "epoch": 0.029018216991778173,
      "grad_norm": 16.0,
      "learning_rate": 9.991392759158643e-05,
      "loss": 5.1484,
      "step": 180
    },
    {
      "epoch": 0.03063034015798807,
      "grad_norm": 28.875,
      "learning_rate": 9.990316854053473e-05,
      "loss": 4.7844,
      "step": 190
    },
    {
      "epoch": 0.032242463324197966,
      "grad_norm": 14.8125,
      "learning_rate": 9.989240948948302e-05,
      "loss": 6.1617,
      "step": 200
    },
    {
      "epoch": 0.033854586490407866,
      "grad_norm": 14.0,
      "learning_rate": 9.988165043843133e-05,
      "loss": 5.125,
      "step": 210
    },
    {
      "epoch": 0.035466709656617766,
      "grad_norm": 9.9375,
      "learning_rate": 9.987089138737964e-05,
      "loss": 4.1859,
      "step": 220
    },
    {
      "epoch": 0.037078832822827666,
      "grad_norm": 13.5,
      "learning_rate": 9.986013233632794e-05,
      "loss": 7.325,
      "step": 230
    },
    {
      "epoch": 0.03869095598903756,
      "grad_norm": 8.4375,
      "learning_rate": 9.984937328527625e-05,
      "loss": 5.8016,
      "step": 240
    },
    {
      "epoch": 0.04030307915524746,
      "grad_norm": 9.0,
      "learning_rate": 9.983861423422455e-05,
      "loss": 7.9594,
      "step": 250
    },
    {
      "epoch": 0.04191520232145736,
      "grad_norm": 0.0,
      "learning_rate": 9.982785518317284e-05,
      "loss": 6.6636,
      "step": 260
    },
    {
      "epoch": 0.04352732548766726,
      "grad_norm": 11.25,
      "learning_rate": 9.981709613212114e-05,
      "loss": 6.2656,
      "step": 270
    },
    {
      "epoch": 0.04513944865387716,
      "grad_norm": 23.125,
      "learning_rate": 9.980633708106945e-05,
      "loss": 4.5563,
      "step": 280
    },
    {
      "epoch": 0.04675157182008705,
      "grad_norm": 15.0625,
      "learning_rate": 9.979557803001776e-05,
      "loss": 5.8719,
      "step": 290
    },
    {
      "epoch": 0.04836369498629695,
      "grad_norm": 15.375,
      "learning_rate": 9.978481897896606e-05,
      "loss": 5.7246,
      "step": 300
    },
    {
      "epoch": 0.04997581815250685,
      "grad_norm": 8.625,
      "learning_rate": 9.977405992791437e-05,
      "loss": 5.6266,
      "step": 310
    },
    {
      "epoch": 0.05158794131871675,
      "grad_norm": 8.3125,
      "learning_rate": 9.976330087686266e-05,
      "loss": 5.4641,
      "step": 320
    },
    {
      "epoch": 0.053200064484926646,
      "grad_norm": 17.125,
      "learning_rate": 9.975254182581096e-05,
      "loss": 5.0281,
      "step": 330
    },
    {
      "epoch": 0.054812187651136546,
      "grad_norm": 11.0,
      "learning_rate": 9.974178277475927e-05,
      "loss": 6.4,
      "step": 340
    },
    {
      "epoch": 0.056424310817346446,
      "grad_norm": 8.1875,
      "learning_rate": 9.973102372370757e-05,
      "loss": 7.4453,
      "step": 350
    },
    {
      "epoch": 0.058036433983556346,
      "grad_norm": 6.8125,
      "learning_rate": 9.972026467265588e-05,
      "loss": 5.0812,
      "step": 360
    },
    {
      "epoch": 0.05964855714976624,
      "grad_norm": 11.0625,
      "learning_rate": 9.970950562160419e-05,
      "loss": 5.3469,
      "step": 370
    },
    {
      "epoch": 0.06126068031597614,
      "grad_norm": 9.0,
      "learning_rate": 9.969874657055248e-05,
      "loss": 5.0141,
      "step": 380
    },
    {
      "epoch": 0.06287280348218603,
      "grad_norm": 19.375,
      "learning_rate": 9.968798751950078e-05,
      "loss": 7.2297,
      "step": 390
    },
    {
      "epoch": 0.06448492664839593,
      "grad_norm": 19.0,
      "learning_rate": 9.967722846844909e-05,
      "loss": 6.6609,
      "step": 400
    },
    {
      "epoch": 0.06609704981460583,
      "grad_norm": 7.34375,
      "learning_rate": 9.966646941739739e-05,
      "loss": 5.8766,
      "step": 410
    },
    {
      "epoch": 0.06770917298081573,
      "grad_norm": 15.75,
      "learning_rate": 9.965571036634568e-05,
      "loss": 6.4984,
      "step": 420
    },
    {
      "epoch": 0.06932129614702563,
      "grad_norm": 7.125,
      "learning_rate": 9.9644951315294e-05,
      "loss": 5.3247,
      "step": 430
    },
    {
      "epoch": 0.07093341931323553,
      "grad_norm": 9.4375,
      "learning_rate": 9.96341922642423e-05,
      "loss": 4.7766,
      "step": 440
    },
    {
      "epoch": 0.07254554247944543,
      "grad_norm": 10.3125,
      "learning_rate": 9.96234332131906e-05,
      "loss": 5.2547,
      "step": 450
    },
    {
      "epoch": 0.07415766564565533,
      "grad_norm": 8.6875,
      "learning_rate": 9.961267416213891e-05,
      "loss": 3.9258,
      "step": 460
    },
    {
      "epoch": 0.07576978881186523,
      "grad_norm": 8.5,
      "learning_rate": 9.96019151110872e-05,
      "loss": 7.4156,
      "step": 470
    },
    {
      "epoch": 0.07738191197807512,
      "grad_norm": 5.34375,
      "learning_rate": 9.95911560600355e-05,
      "loss": 6.7195,
      "step": 480
    },
    {
      "epoch": 0.07899403514428502,
      "grad_norm": 17.75,
      "learning_rate": 9.958039700898381e-05,
      "loss": 6.2648,
      "step": 490
    },
    {
      "epoch": 0.08060615831049492,
      "grad_norm": 8.6875,
      "learning_rate": 9.956963795793212e-05,
      "loss": 4.3867,
      "step": 500
    },
    {
      "epoch": 0.08221828147670482,
      "grad_norm": 8.625,
      "learning_rate": 9.955887890688042e-05,
      "loss": 5.7219,
      "step": 510
    },
    {
      "epoch": 0.08383040464291472,
      "grad_norm": 8.0625,
      "learning_rate": 9.954811985582873e-05,
      "loss": 5.5219,
      "step": 520
    },
    {
      "epoch": 0.08544252780912462,
      "grad_norm": 13.75,
      "learning_rate": 9.953736080477703e-05,
      "loss": 4.8063,
      "step": 530
    },
    {
      "epoch": 0.08705465097533452,
      "grad_norm": 12.125,
      "learning_rate": 9.952660175372532e-05,
      "loss": 5.8906,
      "step": 540
    },
    {
      "epoch": 0.08866677414154442,
      "grad_norm": 26.375,
      "learning_rate": 9.951584270267363e-05,
      "loss": 5.7805,
      "step": 550
    },
    {
      "epoch": 0.09027889730775432,
      "grad_norm": 17.125,
      "learning_rate": 9.950508365162193e-05,
      "loss": 3.8891,
      "step": 560
    },
    {
      "epoch": 0.0918910204739642,
      "grad_norm": 11.0625,
      "learning_rate": 9.949432460057024e-05,
      "loss": 4.3609,
      "step": 570
    },
    {
      "epoch": 0.0935031436401741,
      "grad_norm": 8.4375,
      "learning_rate": 9.948356554951855e-05,
      "loss": 6.2172,
      "step": 580
    },
    {
      "epoch": 0.095115266806384,
      "grad_norm": 15.1875,
      "learning_rate": 9.947280649846685e-05,
      "loss": 5.1117,
      "step": 590
    },
    {
      "epoch": 0.0967273899725939,
      "grad_norm": 7.25,
      "learning_rate": 9.946204744741514e-05,
      "loss": 5.9109,
      "step": 600
    },
    {
      "epoch": 0.0983395131388038,
      "grad_norm": 6.03125,
      "learning_rate": 9.945128839636344e-05,
      "loss": 5.2211,
      "step": 610
    },
    {
      "epoch": 0.0999516363050137,
      "grad_norm": 6.3125,
      "learning_rate": 9.944052934531175e-05,
      "loss": 7.0266,
      "step": 620
    },
    {
      "epoch": 0.1015637594712236,
      "grad_norm": 8.1875,
      "learning_rate": 9.942977029426004e-05,
      "loss": 4.675,
      "step": 630
    },
    {
      "epoch": 0.1031758826374335,
      "grad_norm": 14.375,
      "learning_rate": 9.941901124320835e-05,
      "loss": 6.1219,
      "step": 640
    },
    {
      "epoch": 0.10478800580364339,
      "grad_norm": 9.0625,
      "learning_rate": 9.940825219215666e-05,
      "loss": 5.1234,
      "step": 650
    },
    {
      "epoch": 0.10640012896985329,
      "grad_norm": 8.125,
      "learning_rate": 9.939749314110496e-05,
      "loss": 6.0414,
      "step": 660
    },
    {
      "epoch": 0.10801225213606319,
      "grad_norm": 6.90625,
      "learning_rate": 9.938673409005326e-05,
      "loss": 4.6375,
      "step": 670
    },
    {
      "epoch": 0.10962437530227309,
      "grad_norm": 10.875,
      "learning_rate": 9.937597503900157e-05,
      "loss": 6.3609,
      "step": 680
    },
    {
      "epoch": 0.11123649846848299,
      "grad_norm": 11.125,
      "learning_rate": 9.936521598794986e-05,
      "loss": 6.632,
      "step": 690
    },
    {
      "epoch": 0.11284862163469289,
      "grad_norm": 5.78125,
      "learning_rate": 9.935445693689816e-05,
      "loss": 4.2531,
      "step": 700
    },
    {
      "epoch": 0.11446074480090279,
      "grad_norm": 5.75,
      "learning_rate": 9.934369788584647e-05,
      "loss": 3.5398,
      "step": 710
    },
    {
      "epoch": 0.11607286796711269,
      "grad_norm": 10.625,
      "learning_rate": 9.933293883479478e-05,
      "loss": 5.7885,
      "step": 720
    },
    {
      "epoch": 0.11768499113332259,
      "grad_norm": 5.46875,
      "learning_rate": 9.932217978374308e-05,
      "loss": 7.1172,
      "step": 730
    },
    {
      "epoch": 0.11929711429953248,
      "grad_norm": 6.28125,
      "learning_rate": 9.931142073269139e-05,
      "loss": 4.0297,
      "step": 740
    },
    {
      "epoch": 0.12090923746574238,
      "grad_norm": 5.5,
      "learning_rate": 9.930066168163968e-05,
      "loss": 5.2031,
      "step": 750
    },
    {
      "epoch": 0.12252136063195228,
      "grad_norm": 5.125,
      "learning_rate": 9.928990263058798e-05,
      "loss": 5.5906,
      "step": 760
    },
    {
      "epoch": 0.12413348379816218,
      "grad_norm": 5.40625,
      "learning_rate": 9.927914357953629e-05,
      "loss": 5.6406,
      "step": 770
    },
    {
      "epoch": 0.12574560696437206,
      "grad_norm": 5.90625,
      "learning_rate": 9.926838452848459e-05,
      "loss": 6.5305,
      "step": 780
    },
    {
      "epoch": 0.12735773013058196,
      "grad_norm": 5.71875,
      "learning_rate": 9.92576254774329e-05,
      "loss": 5.8281,
      "step": 790
    },
    {
      "epoch": 0.12896985329679186,
      "grad_norm": 5.34375,
      "learning_rate": 9.924686642638121e-05,
      "loss": 7.4227,
      "step": 800
    },
    {
      "epoch": 0.13058197646300176,
      "grad_norm": 5.53125,
      "learning_rate": 9.92361073753295e-05,
      "loss": 4.4633,
      "step": 810
    },
    {
      "epoch": 0.13219409962921166,
      "grad_norm": 6.15625,
      "learning_rate": 9.92253483242778e-05,
      "loss": 3.0922,
      "step": 820
    },
    {
      "epoch": 0.13380622279542156,
      "grad_norm": 12.0,
      "learning_rate": 9.921458927322611e-05,
      "loss": 6.7359,
      "step": 830
    },
    {
      "epoch": 0.13541834596163146,
      "grad_norm": 7.875,
      "learning_rate": 9.920383022217441e-05,
      "loss": 4.2016,
      "step": 840
    },
    {
      "epoch": 0.13703046912784136,
      "grad_norm": 6.90625,
      "learning_rate": 9.91930711711227e-05,
      "loss": 4.5312,
      "step": 850
    },
    {
      "epoch": 0.13864259229405126,
      "grad_norm": 7.15625,
      "learning_rate": 9.918231212007101e-05,
      "loss": 6.2023,
      "step": 860
    },
    {
      "epoch": 0.14025471546026116,
      "grad_norm": 15.1875,
      "learning_rate": 9.917155306901932e-05,
      "loss": 5.6305,
      "step": 870
    },
    {
      "epoch": 0.14186683862647106,
      "grad_norm": 0.0,
      "learning_rate": 9.916079401796762e-05,
      "loss": 7.1775,
      "step": 880
    },
    {
      "epoch": 0.14347896179268096,
      "grad_norm": 15.1875,
      "learning_rate": 9.915003496691593e-05,
      "loss": 6.8906,
      "step": 890
    },
    {
      "epoch": 0.14509108495889086,
      "grad_norm": 8.625,
      "learning_rate": 9.913927591586423e-05,
      "loss": 6.1266,
      "step": 900
    },
    {
      "epoch": 0.14670320812510076,
      "grad_norm": 5.21875,
      "learning_rate": 9.912851686481252e-05,
      "loss": 5.7891,
      "step": 910
    },
    {
      "epoch": 0.14831533129131066,
      "grad_norm": 9.75,
      "learning_rate": 9.911775781376083e-05,
      "loss": 4.4523,
      "step": 920
    },
    {
      "epoch": 0.14992745445752056,
      "grad_norm": 5.375,
      "learning_rate": 9.910699876270913e-05,
      "loss": 5.0625,
      "step": 930
    },
    {
      "epoch": 0.15153957762373046,
      "grad_norm": 11.75,
      "learning_rate": 9.909623971165744e-05,
      "loss": 4.7758,
      "step": 940
    },
    {
      "epoch": 0.15315170078994036,
      "grad_norm": 5.71875,
      "learning_rate": 9.908548066060574e-05,
      "loss": 4.5891,
      "step": 950
    },
    {
      "epoch": 0.15476382395615024,
      "grad_norm": 15.125,
      "learning_rate": 9.907472160955405e-05,
      "loss": 3.3312,
      "step": 960
    },
    {
      "epoch": 0.15637594712236014,
      "grad_norm": 11.0625,
      "learning_rate": 9.906396255850234e-05,
      "loss": 5.7469,
      "step": 970
    },
    {
      "epoch": 0.15798807028857004,
      "grad_norm": 5.875,
      "learning_rate": 9.905320350745064e-05,
      "loss": 6.968,
      "step": 980
    },
    {
      "epoch": 0.15960019345477994,
      "grad_norm": 9.75,
      "learning_rate": 9.904244445639895e-05,
      "loss": 6.6414,
      "step": 990
    },
    {
      "epoch": 0.16121231662098984,
      "grad_norm": 4.75,
      "learning_rate": 9.903168540534725e-05,
      "loss": 3.9023,
      "step": 1000
    },
    {
      "epoch": 0.16282443978719974,
      "grad_norm": 4.6875,
      "learning_rate": 9.902092635429556e-05,
      "loss": 3.7453,
      "step": 1010
    },
    {
      "epoch": 0.16443656295340964,
      "grad_norm": 8.3125,
      "learning_rate": 9.901016730324387e-05,
      "loss": 5.6391,
      "step": 1020
    },
    {
      "epoch": 0.16604868611961954,
      "grad_norm": 14.9375,
      "learning_rate": 9.899940825219216e-05,
      "loss": 4.6227,
      "step": 1030
    },
    {
      "epoch": 0.16766080928582944,
      "grad_norm": 6.5625,
      "learning_rate": 9.898864920114046e-05,
      "loss": 4.5309,
      "step": 1040
    },
    {
      "epoch": 0.16927293245203934,
      "grad_norm": 5.84375,
      "learning_rate": 9.897789015008877e-05,
      "loss": 5.9188,
      "step": 1050
    },
    {
      "epoch": 0.17088505561824924,
      "grad_norm": 13.25,
      "learning_rate": 9.896713109903707e-05,
      "loss": 5.0047,
      "step": 1060
    },
    {
      "epoch": 0.17249717878445914,
      "grad_norm": 11.5625,
      "learning_rate": 9.895637204798536e-05,
      "loss": 6.5187,
      "step": 1070
    },
    {
      "epoch": 0.17410930195066904,
      "grad_norm": 9.0,
      "learning_rate": 9.894561299693367e-05,
      "loss": 3.8641,
      "step": 1080
    },
    {
      "epoch": 0.17572142511687894,
      "grad_norm": 8.4375,
      "learning_rate": 9.893485394588198e-05,
      "loss": 6.7273,
      "step": 1090
    },
    {
      "epoch": 0.17733354828308884,
      "grad_norm": 5.21875,
      "learning_rate": 9.892409489483028e-05,
      "loss": 3.6938,
      "step": 1100
    },
    {
      "epoch": 0.17894567144929874,
      "grad_norm": 12.4375,
      "learning_rate": 9.891333584377859e-05,
      "loss": 4.4508,
      "step": 1110
    },
    {
      "epoch": 0.18055779461550864,
      "grad_norm": 7.96875,
      "learning_rate": 9.890257679272689e-05,
      "loss": 5.3672,
      "step": 1120
    },
    {
      "epoch": 0.1821699177817185,
      "grad_norm": 5.125,
      "learning_rate": 9.889181774167518e-05,
      "loss": 6.4953,
      "step": 1130
    },
    {
      "epoch": 0.1837820409479284,
      "grad_norm": 6.65625,
      "learning_rate": 9.888105869062349e-05,
      "loss": 5.3031,
      "step": 1140
    },
    {
      "epoch": 0.1853941641141383,
      "grad_norm": 10.3125,
      "learning_rate": 9.88702996395718e-05,
      "loss": 4.7523,
      "step": 1150
    },
    {
      "epoch": 0.1870062872803482,
      "grad_norm": 4.78125,
      "learning_rate": 9.88595405885201e-05,
      "loss": 4.9531,
      "step": 1160
    },
    {
      "epoch": 0.1886184104465581,
      "grad_norm": 5.3125,
      "learning_rate": 9.884878153746841e-05,
      "loss": 4.8906,
      "step": 1170
    },
    {
      "epoch": 0.190230533612768,
      "grad_norm": 6.28125,
      "learning_rate": 9.88380224864167e-05,
      "loss": 2.7055,
      "step": 1180
    },
    {
      "epoch": 0.1918426567789779,
      "grad_norm": 9.0,
      "learning_rate": 9.8827263435365e-05,
      "loss": 7.1406,
      "step": 1190
    },
    {
      "epoch": 0.1934547799451878,
      "grad_norm": 6.75,
      "learning_rate": 9.881650438431331e-05,
      "loss": 4.7672,
      "step": 1200
    },
    {
      "epoch": 0.1950669031113977,
      "grad_norm": 4.96875,
      "learning_rate": 9.880574533326161e-05,
      "loss": 4.8438,
      "step": 1210
    },
    {
      "epoch": 0.1966790262776076,
      "grad_norm": 5.875,
      "learning_rate": 9.879498628220992e-05,
      "loss": 4.9305,
      "step": 1220
    },
    {
      "epoch": 0.1982911494438175,
      "grad_norm": 13.875,
      "learning_rate": 9.878422723115823e-05,
      "loss": 3.6434,
      "step": 1230
    },
    {
      "epoch": 0.1999032726100274,
      "grad_norm": 5.125,
      "learning_rate": 9.877346818010652e-05,
      "loss": 6.7891,
      "step": 1240
    },
    {
      "epoch": 0.2015153957762373,
      "grad_norm": 9.1875,
      "learning_rate": 9.876270912905482e-05,
      "loss": 5.4977,
      "step": 1250
    },
    {
      "epoch": 0.2031275189424472,
      "grad_norm": 4.8125,
      "learning_rate": 9.875195007800313e-05,
      "loss": 5.7016,
      "step": 1260
    },
    {
      "epoch": 0.2047396421086571,
      "grad_norm": 4.5,
      "learning_rate": 9.874119102695143e-05,
      "loss": 3.85,
      "step": 1270
    },
    {
      "epoch": 0.206351765274867,
      "grad_norm": 10.6875,
      "learning_rate": 9.873043197589972e-05,
      "loss": 7.9547,
      "step": 1280
    },
    {
      "epoch": 0.2079638884410769,
      "grad_norm": 12.375,
      "learning_rate": 9.871967292484803e-05,
      "loss": 5.0922,
      "step": 1290
    },
    {
      "epoch": 0.20957601160728678,
      "grad_norm": 9.125,
      "learning_rate": 9.870891387379634e-05,
      "loss": 4.2844,
      "step": 1300
    },
    {
      "epoch": 0.21118813477349668,
      "grad_norm": 8.1875,
      "learning_rate": 9.869815482274464e-05,
      "loss": 5.5135,
      "step": 1310
    },
    {
      "epoch": 0.21280025793970658,
      "grad_norm": 0.0,
      "learning_rate": 9.868739577169294e-05,
      "loss": 6.4625,
      "step": 1320
    },
    {
      "epoch": 0.21441238110591648,
      "grad_norm": 13.4375,
      "learning_rate": 9.867663672064125e-05,
      "loss": 6.5648,
      "step": 1330
    },
    {
      "epoch": 0.21602450427212638,
      "grad_norm": 4.46875,
      "learning_rate": 9.866587766958954e-05,
      "loss": 5.0422,
      "step": 1340
    },
    {
      "epoch": 0.21763662743833628,
      "grad_norm": 12.6875,
      "learning_rate": 9.865511861853784e-05,
      "loss": 4.9195,
      "step": 1350
    },
    {
      "epoch": 0.21924875060454618,
      "grad_norm": 4.5625,
      "learning_rate": 9.864435956748615e-05,
      "loss": 5.5828,
      "step": 1360
    },
    {
      "epoch": 0.22086087377075608,
      "grad_norm": 13.125,
      "learning_rate": 9.863360051643446e-05,
      "loss": 5.1766,
      "step": 1370
    },
    {
      "epoch": 0.22247299693696598,
      "grad_norm": 16.5,
      "learning_rate": 9.862284146538276e-05,
      "loss": 5.6992,
      "step": 1380
    },
    {
      "epoch": 0.22408512010317588,
      "grad_norm": 4.53125,
      "learning_rate": 9.861208241433107e-05,
      "loss": 4.4398,
      "step": 1390
    },
    {
      "epoch": 0.22569724326938578,
      "grad_norm": 4.40625,
      "learning_rate": 9.860132336327936e-05,
      "loss": 4.5492,
      "step": 1400
    },
    {
      "epoch": 0.22730936643559568,
      "grad_norm": 13.1875,
      "learning_rate": 9.859056431222766e-05,
      "loss": 4.957,
      "step": 1410
    },
    {
      "epoch": 0.22892148960180558,
      "grad_norm": 12.25,
      "learning_rate": 9.857980526117597e-05,
      "loss": 5.1055,
      "step": 1420
    },
    {
      "epoch": 0.23053361276801548,
      "grad_norm": 4.9375,
      "learning_rate": 9.856904621012427e-05,
      "loss": 6.2719,
      "step": 1430
    },
    {
      "epoch": 0.23214573593422538,
      "grad_norm": 6.3125,
      "learning_rate": 9.855828715907258e-05,
      "loss": 5.4828,
      "step": 1440
    },
    {
      "epoch": 0.23375785910043528,
      "grad_norm": 6.40625,
      "learning_rate": 9.854752810802089e-05,
      "loss": 6.1828,
      "step": 1450
    },
    {
      "epoch": 0.23536998226664518,
      "grad_norm": 18.625,
      "learning_rate": 9.853676905696918e-05,
      "loss": 4.7188,
      "step": 1460
    },
    {
      "epoch": 0.23698210543285508,
      "grad_norm": 0.0,
      "learning_rate": 9.852601000591748e-05,
      "loss": 4.438,
      "step": 1470
    },
    {
      "epoch": 0.23859422859906496,
      "grad_norm": 12.8125,
      "learning_rate": 9.851525095486579e-05,
      "loss": 5.5719,
      "step": 1480
    },
    {
      "epoch": 0.24020635176527486,
      "grad_norm": 12.875,
      "learning_rate": 9.850449190381409e-05,
      "loss": 5.4195,
      "step": 1490
    },
    {
      "epoch": 0.24181847493148476,
      "grad_norm": 9.8125,
      "learning_rate": 9.849373285276238e-05,
      "loss": 4.9386,
      "step": 1500
    },
    {
      "epoch": 0.24343059809769466,
      "grad_norm": 10.5,
      "learning_rate": 9.848297380171069e-05,
      "loss": 7.0617,
      "step": 1510
    },
    {
      "epoch": 0.24504272126390456,
      "grad_norm": 5.4375,
      "learning_rate": 9.8472214750659e-05,
      "loss": 4.382,
      "step": 1520
    },
    {
      "epoch": 0.24665484443011446,
      "grad_norm": 6.1875,
      "learning_rate": 9.84614556996073e-05,
      "loss": 4.9828,
      "step": 1530
    },
    {
      "epoch": 0.24826696759632436,
      "grad_norm": 6.75,
      "learning_rate": 9.845069664855561e-05,
      "loss": 8.6094,
      "step": 1540
    },
    {
      "epoch": 0.24987909076253426,
      "grad_norm": 4.78125,
      "learning_rate": 9.84399375975039e-05,
      "loss": 2.7674,
      "step": 1550
    },
    {
      "epoch": 0.25149121392874413,
      "grad_norm": 8.1875,
      "learning_rate": 9.84291785464522e-05,
      "loss": 4.1813,
      "step": 1560
    },
    {
      "epoch": 0.25310333709495403,
      "grad_norm": 9.1875,
      "learning_rate": 9.841841949540051e-05,
      "loss": 5.675,
      "step": 1570
    },
    {
      "epoch": 0.25471546026116393,
      "grad_norm": 5.09375,
      "learning_rate": 9.840766044434881e-05,
      "loss": 4.8117,
      "step": 1580
    },
    {
      "epoch": 0.25632758342737383,
      "grad_norm": 4.75,
      "learning_rate": 9.839690139329712e-05,
      "loss": 4.007,
      "step": 1590
    },
    {
      "epoch": 0.25793970659358373,
      "grad_norm": 4.90625,
      "learning_rate": 9.838614234224542e-05,
      "loss": 5.1844,
      "step": 1600
    },
    {
      "epoch": 0.25955182975979363,
      "grad_norm": 11.3125,
      "learning_rate": 9.837538329119373e-05,
      "loss": 5.6609,
      "step": 1610
    },
    {
      "epoch": 0.26116395292600353,
      "grad_norm": 6.1875,
      "learning_rate": 9.836462424014202e-05,
      "loss": 5.3719,
      "step": 1620
    },
    {
      "epoch": 0.26277607609221343,
      "grad_norm": 12.9375,
      "learning_rate": 9.835386518909032e-05,
      "loss": 4.9844,
      "step": 1630
    },
    {
      "epoch": 0.26438819925842333,
      "grad_norm": 7.625,
      "learning_rate": 9.834310613803863e-05,
      "loss": 5.3156,
      "step": 1640
    },
    {
      "epoch": 0.26600032242463323,
      "grad_norm": 4.90625,
      "learning_rate": 9.833234708698693e-05,
      "loss": 5.3422,
      "step": 1650
    },
    {
      "epoch": 0.26761244559084313,
      "grad_norm": 9.5,
      "learning_rate": 9.832158803593524e-05,
      "loss": 4.7398,
      "step": 1660
    },
    {
      "epoch": 0.26922456875705303,
      "grad_norm": 4.0625,
      "learning_rate": 9.831082898488355e-05,
      "loss": 5.5797,
      "step": 1670
    },
    {
      "epoch": 0.27083669192326293,
      "grad_norm": 12.375,
      "learning_rate": 9.830006993383184e-05,
      "loss": 4.4414,
      "step": 1680
    },
    {
      "epoch": 0.27244881508947283,
      "grad_norm": 6.25,
      "learning_rate": 9.828931088278014e-05,
      "loss": 4.3172,
      "step": 1690
    },
    {
      "epoch": 0.27406093825568273,
      "grad_norm": 4.84375,
      "learning_rate": 9.827855183172845e-05,
      "loss": 5.425,
      "step": 1700
    },
    {
      "epoch": 0.27567306142189263,
      "grad_norm": 11.8125,
      "learning_rate": 9.826779278067674e-05,
      "loss": 5.7219,
      "step": 1710
    },
    {
      "epoch": 0.27728518458810253,
      "grad_norm": 7.40625,
      "learning_rate": 9.825703372962504e-05,
      "loss": 5.4781,
      "step": 1720
    },
    {
      "epoch": 0.27889730775431243,
      "grad_norm": 7.65625,
      "learning_rate": 9.824627467857335e-05,
      "loss": 5.7337,
      "step": 1730
    },
    {
      "epoch": 0.28050943092052233,
      "grad_norm": 5.5625,
      "learning_rate": 9.823551562752166e-05,
      "loss": 4.2406,
      "step": 1740
    },
    {
      "epoch": 0.28212155408673223,
      "grad_norm": 12.3125,
      "learning_rate": 9.822475657646996e-05,
      "loss": 6.3742,
      "step": 1750
    },
    {
      "epoch": 0.28373367725294213,
      "grad_norm": 5.375,
      "learning_rate": 9.821399752541827e-05,
      "loss": 6.7997,
      "step": 1760
    },
    {
      "epoch": 0.28534580041915203,
      "grad_norm": 5.375,
      "learning_rate": 9.820323847436656e-05,
      "loss": 3.6109,
      "step": 1770
    },
    {
      "epoch": 0.28695792358536193,
      "grad_norm": 10.0625,
      "learning_rate": 9.819247942331486e-05,
      "loss": 4.6944,
      "step": 1780
    },
    {
      "epoch": 0.28857004675157183,
      "grad_norm": 5.96875,
      "learning_rate": 9.818172037226317e-05,
      "loss": 5.0563,
      "step": 1790
    },
    {
      "epoch": 0.29018216991778173,
      "grad_norm": 6.0625,
      "learning_rate": 9.817096132121148e-05,
      "loss": 4.6781,
      "step": 1800
    },
    {
      "epoch": 0.29179429308399163,
      "grad_norm": 3.65625,
      "learning_rate": 9.816020227015978e-05,
      "loss": 4.4484,
      "step": 1810
    },
    {
      "epoch": 0.29340641625020153,
      "grad_norm": 4.34375,
      "learning_rate": 9.814944321910809e-05,
      "loss": 6.3414,
      "step": 1820
    },
    {
      "epoch": 0.29501853941641143,
      "grad_norm": 7.15625,
      "learning_rate": 9.813868416805638e-05,
      "loss": 4.0563,
      "step": 1830
    },
    {
      "epoch": 0.29663066258262133,
      "grad_norm": 6.53125,
      "learning_rate": 9.812792511700468e-05,
      "loss": 5.1977,
      "step": 1840
    },
    {
      "epoch": 0.29824278574883123,
      "grad_norm": 12.6875,
      "learning_rate": 9.811716606595299e-05,
      "loss": 6.4469,
      "step": 1850
    },
    {
      "epoch": 0.29985490891504113,
      "grad_norm": 6.4375,
      "learning_rate": 9.810640701490129e-05,
      "loss": 5.6531,
      "step": 1860
    },
    {
      "epoch": 0.30146703208125103,
      "grad_norm": 6.21875,
      "learning_rate": 9.80956479638496e-05,
      "loss": 5.6195,
      "step": 1870
    },
    {
      "epoch": 0.30307915524746093,
      "grad_norm": 9.875,
      "learning_rate": 9.808488891279791e-05,
      "loss": 6.4875,
      "step": 1880
    },
    {
      "epoch": 0.30469127841367083,
      "grad_norm": 4.375,
      "learning_rate": 9.80741298617462e-05,
      "loss": 4.0719,
      "step": 1890
    },
    {
      "epoch": 0.30630340157988073,
      "grad_norm": 6.9375,
      "learning_rate": 9.80633708106945e-05,
      "loss": 5.4844,
      "step": 1900
    },
    {
      "epoch": 0.3079155247460906,
      "grad_norm": 6.53125,
      "learning_rate": 9.805261175964281e-05,
      "loss": 4.9176,
      "step": 1910
    },
    {
      "epoch": 0.3095276479123005,
      "grad_norm": 13.125,
      "learning_rate": 9.80418527085911e-05,
      "loss": 5.2734,
      "step": 1920
    },
    {
      "epoch": 0.3111397710785104,
      "grad_norm": 7.625,
      "learning_rate": 9.80310936575394e-05,
      "loss": 4.6937,
      "step": 1930
    },
    {
      "epoch": 0.3127518942447203,
      "grad_norm": 4.5625,
      "learning_rate": 9.802033460648771e-05,
      "loss": 5.1827,
      "step": 1940
    },
    {
      "epoch": 0.3143640174109302,
      "grad_norm": 6.5,
      "learning_rate": 9.800957555543602e-05,
      "loss": 4.9625,
      "step": 1950
    },
    {
      "epoch": 0.3159761405771401,
      "grad_norm": 4.40625,
      "learning_rate": 9.799881650438432e-05,
      "loss": 4.457,
      "step": 1960
    },
    {
      "epoch": 0.31758826374335,
      "grad_norm": 4.9375,
      "learning_rate": 9.798805745333262e-05,
      "loss": 4.0336,
      "step": 1970
    },
    {
      "epoch": 0.3192003869095599,
      "grad_norm": 11.5625,
      "learning_rate": 9.797729840228093e-05,
      "loss": 5.6328,
      "step": 1980
    },
    {
      "epoch": 0.3208125100757698,
      "grad_norm": 12.25,
      "learning_rate": 9.796653935122922e-05,
      "loss": 3.8555,
      "step": 1990
    },
    {
      "epoch": 0.3224246332419797,
      "grad_norm": 4.34375,
      "learning_rate": 9.795578030017752e-05,
      "loss": 4.918,
      "step": 2000
    },
    {
      "epoch": 0.3240367564081896,
      "grad_norm": 5.03125,
      "learning_rate": 9.794502124912583e-05,
      "loss": 4.4406,
      "step": 2010
    },
    {
      "epoch": 0.3256488795743995,
      "grad_norm": 6.4375,
      "learning_rate": 9.793426219807414e-05,
      "loss": 5.0469,
      "step": 2020
    },
    {
      "epoch": 0.3272610027406094,
      "grad_norm": 12.0625,
      "learning_rate": 9.792350314702244e-05,
      "loss": 6.2875,
      "step": 2030
    },
    {
      "epoch": 0.3288731259068193,
      "grad_norm": 4.59375,
      "learning_rate": 9.791274409597075e-05,
      "loss": 5.5922,
      "step": 2040
    },
    {
      "epoch": 0.3304852490730292,
      "grad_norm": 12.0625,
      "learning_rate": 9.790198504491904e-05,
      "loss": 4.5195,
      "step": 2050
    },
    {
      "epoch": 0.3320973722392391,
      "grad_norm": 9.75,
      "learning_rate": 9.789122599386734e-05,
      "loss": 4.3477,
      "step": 2060
    },
    {
      "epoch": 0.333709495405449,
      "grad_norm": 9.25,
      "learning_rate": 9.788046694281565e-05,
      "loss": 5.5594,
      "step": 2070
    },
    {
      "epoch": 0.3353216185716589,
      "grad_norm": 10.75,
      "learning_rate": 9.786970789176395e-05,
      "loss": 3.8062,
      "step": 2080
    },
    {
      "epoch": 0.3369337417378688,
      "grad_norm": 4.125,
      "learning_rate": 9.785894884071226e-05,
      "loss": 4.3514,
      "step": 2090
    },
    {
      "epoch": 0.3385458649040787,
      "grad_norm": 13.125,
      "learning_rate": 9.784818978966057e-05,
      "loss": 5.2273,
      "step": 2100
    },
    {
      "epoch": 0.3401579880702886,
      "grad_norm": 11.375,
      "learning_rate": 9.783743073860886e-05,
      "loss": 5.2062,
      "step": 2110
    },
    {
      "epoch": 0.3417701112364985,
      "grad_norm": 10.75,
      "learning_rate": 9.782667168755716e-05,
      "loss": 3.1812,
      "step": 2120
    },
    {
      "epoch": 0.3433822344027084,
      "grad_norm": 4.5625,
      "learning_rate": 9.781591263650547e-05,
      "loss": 5.2406,
      "step": 2130
    },
    {
      "epoch": 0.3449943575689183,
      "grad_norm": 5.875,
      "learning_rate": 9.780515358545377e-05,
      "loss": 4.6125,
      "step": 2140
    },
    {
      "epoch": 0.3466064807351282,
      "grad_norm": 4.75,
      "learning_rate": 9.779439453440206e-05,
      "loss": 5.0453,
      "step": 2150
    },
    {
      "epoch": 0.3482186039013381,
      "grad_norm": 13.1875,
      "learning_rate": 9.778363548335037e-05,
      "loss": 5.0438,
      "step": 2160
    },
    {
      "epoch": 0.349830727067548,
      "grad_norm": 5.8125,
      "learning_rate": 9.777287643229868e-05,
      "loss": 4.0961,
      "step": 2170
    },
    {
      "epoch": 0.3514428502337579,
      "grad_norm": 4.65625,
      "learning_rate": 9.776211738124698e-05,
      "loss": 4.2875,
      "step": 2180
    },
    {
      "epoch": 0.3530549733999678,
      "grad_norm": 8.125,
      "learning_rate": 9.775135833019529e-05,
      "loss": 5.2391,
      "step": 2190
    },
    {
      "epoch": 0.3546670965661777,
      "grad_norm": 7.78125,
      "learning_rate": 9.774059927914359e-05,
      "loss": 5.6641,
      "step": 2200
    },
    {
      "epoch": 0.3562792197323876,
      "grad_norm": 12.5625,
      "learning_rate": 9.772984022809188e-05,
      "loss": 4.2141,
      "step": 2210
    },
    {
      "epoch": 0.3578913428985975,
      "grad_norm": 11.0,
      "learning_rate": 9.771908117704019e-05,
      "loss": 4.7781,
      "step": 2220
    },
    {
      "epoch": 0.3595034660648074,
      "grad_norm": 16.0,
      "learning_rate": 9.770832212598849e-05,
      "loss": 7.0257,
      "step": 2230
    },
    {
      "epoch": 0.3611155892310173,
      "grad_norm": 7.3125,
      "learning_rate": 9.76975630749368e-05,
      "loss": 5.5453,
      "step": 2240
    },
    {
      "epoch": 0.3627277123972272,
      "grad_norm": 5.65625,
      "learning_rate": 9.768680402388511e-05,
      "loss": 5.1688,
      "step": 2250
    },
    {
      "epoch": 0.364339835563437,
      "grad_norm": 4.125,
      "learning_rate": 9.76760449728334e-05,
      "loss": 4.2836,
      "step": 2260
    },
    {
      "epoch": 0.3659519587296469,
      "grad_norm": 4.65625,
      "learning_rate": 9.76652859217817e-05,
      "loss": 4.7406,
      "step": 2270
    },
    {
      "epoch": 0.3675640818958568,
      "grad_norm": 12.25,
      "learning_rate": 9.765452687073e-05,
      "loss": 4.0789,
      "step": 2280
    },
    {
      "epoch": 0.3691762050620667,
      "grad_norm": 7.1875,
      "learning_rate": 9.764376781967831e-05,
      "loss": 5.3914,
      "step": 2290
    },
    {
      "epoch": 0.3707883282282766,
      "grad_norm": 5.78125,
      "learning_rate": 9.76330087686266e-05,
      "loss": 5.8203,
      "step": 2300
    },
    {
      "epoch": 0.3724004513944865,
      "grad_norm": 5.4375,
      "learning_rate": 9.762224971757491e-05,
      "loss": 4.6766,
      "step": 2310
    },
    {
      "epoch": 0.3740125745606964,
      "grad_norm": 6.9375,
      "learning_rate": 9.761149066652322e-05,
      "loss": 4.9875,
      "step": 2320
    },
    {
      "epoch": 0.3756246977269063,
      "grad_norm": 7.15625,
      "learning_rate": 9.760073161547152e-05,
      "loss": 6.4031,
      "step": 2330
    },
    {
      "epoch": 0.3772368208931162,
      "grad_norm": 5.75,
      "learning_rate": 9.758997256441982e-05,
      "loss": 4.8897,
      "step": 2340
    },
    {
      "epoch": 0.3788489440593261,
      "grad_norm": 4.28125,
      "learning_rate": 9.757921351336813e-05,
      "loss": 6.7422,
      "step": 2350
    },
    {
      "epoch": 0.380461067225536,
      "grad_norm": 5.78125,
      "learning_rate": 9.756845446231642e-05,
      "loss": 5.5969,
      "step": 2360
    },
    {
      "epoch": 0.3820731903917459,
      "grad_norm": 4.21875,
      "learning_rate": 9.755769541126472e-05,
      "loss": 4.2914,
      "step": 2370
    },
    {
      "epoch": 0.3836853135579558,
      "grad_norm": 13.6875,
      "learning_rate": 9.754693636021303e-05,
      "loss": 5.7297,
      "step": 2380
    },
    {
      "epoch": 0.3852974367241657,
      "grad_norm": 5.96875,
      "learning_rate": 9.753617730916134e-05,
      "loss": 4.5633,
      "step": 2390
    },
    {
      "epoch": 0.3869095598903756,
      "grad_norm": 6.65625,
      "learning_rate": 9.752541825810964e-05,
      "loss": 5.8422,
      "step": 2400
    },
    {
      "epoch": 0.3885216830565855,
      "grad_norm": 6.28125,
      "learning_rate": 9.751465920705795e-05,
      "loss": 4.6648,
      "step": 2410
    },
    {
      "epoch": 0.3901338062227954,
      "grad_norm": 9.1875,
      "learning_rate": 9.750390015600624e-05,
      "loss": 5.1156,
      "step": 2420
    },
    {
      "epoch": 0.3917459293890053,
      "grad_norm": 4.625,
      "learning_rate": 9.749314110495454e-05,
      "loss": 5.9875,
      "step": 2430
    },
    {
      "epoch": 0.3933580525552152,
      "grad_norm": 12.0625,
      "learning_rate": 9.748238205390285e-05,
      "loss": 5.9313,
      "step": 2440
    },
    {
      "epoch": 0.3949701757214251,
      "grad_norm": 5.53125,
      "learning_rate": 9.747162300285115e-05,
      "loss": 6.0266,
      "step": 2450
    },
    {
      "epoch": 0.396582298887635,
      "grad_norm": 5.5,
      "learning_rate": 9.746086395179946e-05,
      "loss": 5.4492,
      "step": 2460
    },
    {
      "epoch": 0.3981944220538449,
      "grad_norm": 6.53125,
      "learning_rate": 9.745010490074777e-05,
      "loss": 4.3781,
      "step": 2470
    },
    {
      "epoch": 0.3998065452200548,
      "grad_norm": 5.84375,
      "learning_rate": 9.743934584969606e-05,
      "loss": 3.4883,
      "step": 2480
    },
    {
      "epoch": 0.4014186683862647,
      "grad_norm": 4.21875,
      "learning_rate": 9.742858679864436e-05,
      "loss": 4.4313,
      "step": 2490
    },
    {
      "epoch": 0.4030307915524746,
      "grad_norm": 10.8125,
      "learning_rate": 9.741782774759267e-05,
      "loss": 4.7341,
      "step": 2500
    },
    {
      "epoch": 0.4046429147186845,
      "grad_norm": 0.0,
      "learning_rate": 9.740706869654097e-05,
      "loss": 5.6856,
      "step": 2510
    },
    {
      "epoch": 0.4062550378848944,
      "grad_norm": 4.5,
      "learning_rate": 9.739630964548928e-05,
      "loss": 5.9172,
      "step": 2520
    },
    {
      "epoch": 0.4078671610511043,
      "grad_norm": 7.3125,
      "learning_rate": 9.738555059443759e-05,
      "loss": 4.6719,
      "step": 2530
    },
    {
      "epoch": 0.4094792842173142,
      "grad_norm": 11.25,
      "learning_rate": 9.737479154338588e-05,
      "loss": 4.6969,
      "step": 2540
    },
    {
      "epoch": 0.4110914073835241,
      "grad_norm": 0.0,
      "learning_rate": 9.736403249233418e-05,
      "loss": 5.0789,
      "step": 2550
    },
    {
      "epoch": 0.412703530549734,
      "grad_norm": 12.625,
      "learning_rate": 9.735327344128249e-05,
      "loss": 7.1766,
      "step": 2560
    },
    {
      "epoch": 0.4143156537159439,
      "grad_norm": 7.15625,
      "learning_rate": 9.734251439023079e-05,
      "loss": 3.6313,
      "step": 2570
    },
    {
      "epoch": 0.4159277768821538,
      "grad_norm": 5.96875,
      "learning_rate": 9.733175533917908e-05,
      "loss": 5.0367,
      "step": 2580
    },
    {
      "epoch": 0.4175399000483637,
      "grad_norm": 9.6875,
      "learning_rate": 9.732099628812739e-05,
      "loss": 4.093,
      "step": 2590
    },
    {
      "epoch": 0.41915202321457357,
      "grad_norm": 6.625,
      "learning_rate": 9.73102372370757e-05,
      "loss": 5.2391,
      "step": 2600
    },
    {
      "epoch": 0.42076414638078347,
      "grad_norm": 5.375,
      "learning_rate": 9.7299478186024e-05,
      "loss": 5.3016,
      "step": 2610
    },
    {
      "epoch": 0.42237626954699337,
      "grad_norm": 4.4375,
      "learning_rate": 9.72887191349723e-05,
      "loss": 5.0047,
      "step": 2620
    },
    {
      "epoch": 0.42398839271320327,
      "grad_norm": 4.9375,
      "learning_rate": 9.72779600839206e-05,
      "loss": 7.1914,
      "step": 2630
    },
    {
      "epoch": 0.42560051587941317,
      "grad_norm": 9.75,
      "learning_rate": 9.72672010328689e-05,
      "loss": 5.6266,
      "step": 2640
    },
    {
      "epoch": 0.42721263904562307,
      "grad_norm": 4.5,
      "learning_rate": 9.72564419818172e-05,
      "loss": 5.3734,
      "step": 2650
    },
    {
      "epoch": 0.42882476221183297,
      "grad_norm": 5.28125,
      "learning_rate": 9.724568293076551e-05,
      "loss": 6.0953,
      "step": 2660
    },
    {
      "epoch": 0.43043688537804287,
      "grad_norm": 8.625,
      "learning_rate": 9.723492387971382e-05,
      "loss": 5.8688,
      "step": 2670
    },
    {
      "epoch": 0.43204900854425277,
      "grad_norm": 12.625,
      "learning_rate": 9.722416482866212e-05,
      "loss": 4.8117,
      "step": 2680
    },
    {
      "epoch": 0.43366113171046267,
      "grad_norm": 5.375,
      "learning_rate": 9.721340577761043e-05,
      "loss": 3.3242,
      "step": 2690
    },
    {
      "epoch": 0.43527325487667257,
      "grad_norm": 4.28125,
      "learning_rate": 9.720264672655872e-05,
      "loss": 5.2484,
      "step": 2700
    },
    {
      "epoch": 0.43688537804288247,
      "grad_norm": 4.21875,
      "learning_rate": 9.719188767550702e-05,
      "loss": 4.5812,
      "step": 2710
    },
    {
      "epoch": 0.43849750120909237,
      "grad_norm": 4.90625,
      "learning_rate": 9.718112862445533e-05,
      "loss": 4.3688,
      "step": 2720
    },
    {
      "epoch": 0.44010962437530227,
      "grad_norm": 4.0625,
      "learning_rate": 9.717036957340362e-05,
      "loss": 5.1813,
      "step": 2730
    },
    {
      "epoch": 0.44172174754151217,
      "grad_norm": 3.40625,
      "learning_rate": 9.715961052235194e-05,
      "loss": 4.2969,
      "step": 2740
    },
    {
      "epoch": 0.44333387070772207,
      "grad_norm": 10.125,
      "learning_rate": 9.714885147130025e-05,
      "loss": 5.4074,
      "step": 2750
    },
    {
      "epoch": 0.44494599387393197,
      "grad_norm": 12.25,
      "learning_rate": 9.713809242024854e-05,
      "loss": 5.8422,
      "step": 2760
    },
    {
      "epoch": 0.44655811704014187,
      "grad_norm": 3.9375,
      "learning_rate": 9.712733336919684e-05,
      "loss": 4.0906,
      "step": 2770
    },
    {
      "epoch": 0.44817024020635177,
      "grad_norm": 9.9375,
      "learning_rate": 9.711657431814515e-05,
      "loss": 6.0938,
      "step": 2780
    },
    {
      "epoch": 0.44978236337256167,
      "grad_norm": 4.125,
      "learning_rate": 9.710581526709344e-05,
      "loss": 3.0008,
      "step": 2790
    },
    {
      "epoch": 0.45139448653877157,
      "grad_norm": 11.0625,
      "learning_rate": 9.709505621604174e-05,
      "loss": 4.582,
      "step": 2800
    },
    {
      "epoch": 0.45300660970498147,
      "grad_norm": 13.125,
      "learning_rate": 9.708429716499005e-05,
      "loss": 6.0,
      "step": 2810
    },
    {
      "epoch": 0.45461873287119137,
      "grad_norm": 7.125,
      "learning_rate": 9.707353811393836e-05,
      "loss": 3.7945,
      "step": 2820
    },
    {
      "epoch": 0.45623085603740127,
      "grad_norm": 15.5625,
      "learning_rate": 9.706277906288666e-05,
      "loss": 3.7992,
      "step": 2830
    },
    {
      "epoch": 0.45784297920361117,
      "grad_norm": 6.59375,
      "learning_rate": 9.705202001183497e-05,
      "loss": 4.5586,
      "step": 2840
    },
    {
      "epoch": 0.45945510236982107,
      "grad_norm": 11.8125,
      "learning_rate": 9.704126096078326e-05,
      "loss": 4.8891,
      "step": 2850
    },
    {
      "epoch": 0.46106722553603097,
      "grad_norm": 3.984375,
      "learning_rate": 9.703050190973156e-05,
      "loss": 4.2375,
      "step": 2860
    },
    {
      "epoch": 0.46267934870224087,
      "grad_norm": 8.625,
      "learning_rate": 9.701974285867987e-05,
      "loss": 6.5641,
      "step": 2870
    },
    {
      "epoch": 0.46429147186845077,
      "grad_norm": 7.1875,
      "learning_rate": 9.700898380762817e-05,
      "loss": 5.6703,
      "step": 2880
    },
    {
      "epoch": 0.46590359503466067,
      "grad_norm": 5.71875,
      "learning_rate": 9.699822475657648e-05,
      "loss": 5.2625,
      "step": 2890
    },
    {
      "epoch": 0.46751571820087057,
      "grad_norm": 6.28125,
      "learning_rate": 9.698746570552479e-05,
      "loss": 5.7031,
      "step": 2900
    },
    {
      "epoch": 0.46912784136708047,
      "grad_norm": 4.6875,
      "learning_rate": 9.697670665447308e-05,
      "loss": 5.2797,
      "step": 2910
    },
    {
      "epoch": 0.47073996453329037,
      "grad_norm": 9.8125,
      "learning_rate": 9.696594760342138e-05,
      "loss": 5.4617,
      "step": 2920
    },
    {
      "epoch": 0.47235208769950027,
      "grad_norm": 4.53125,
      "learning_rate": 9.695518855236968e-05,
      "loss": 5.3508,
      "step": 2930
    },
    {
      "epoch": 0.47396421086571017,
      "grad_norm": 5.34375,
      "learning_rate": 9.694442950131799e-05,
      "loss": 5.1633,
      "step": 2940
    },
    {
      "epoch": 0.47557633403192,
      "grad_norm": 4.125,
      "learning_rate": 9.693367045026628e-05,
      "loss": 5.0193,
      "step": 2950
    },
    {
      "epoch": 0.4771884571981299,
      "grad_norm": 4.75,
      "learning_rate": 9.69229113992146e-05,
      "loss": 5.608,
      "step": 2960
    },
    {
      "epoch": 0.4788005803643398,
      "grad_norm": 6.5625,
      "learning_rate": 9.69121523481629e-05,
      "loss": 4.0664,
      "step": 2970
    },
    {
      "epoch": 0.4804127035305497,
      "grad_norm": 12.25,
      "learning_rate": 9.69013932971112e-05,
      "loss": 5.7352,
      "step": 2980
    },
    {
      "epoch": 0.4820248266967596,
      "grad_norm": 12.25,
      "learning_rate": 9.68906342460595e-05,
      "loss": 8.1641,
      "step": 2990
    },
    {
      "epoch": 0.4836369498629695,
      "grad_norm": 4.21875,
      "learning_rate": 9.68798751950078e-05,
      "loss": 5.6133,
      "step": 3000
    },
    {
      "epoch": 0.4852490730291794,
      "grad_norm": 5.3125,
      "learning_rate": 9.68691161439561e-05,
      "loss": 7.1844,
      "step": 3010
    },
    {
      "epoch": 0.4868611961953893,
      "grad_norm": 11.6875,
      "learning_rate": 9.68583570929044e-05,
      "loss": 4.525,
      "step": 3020
    },
    {
      "epoch": 0.4884733193615992,
      "grad_norm": 36.5,
      "learning_rate": 9.684759804185271e-05,
      "loss": 4.9139,
      "step": 3030
    },
    {
      "epoch": 0.4900854425278091,
      "grad_norm": 5.625,
      "learning_rate": 9.683683899080102e-05,
      "loss": 4.0297,
      "step": 3040
    },
    {
      "epoch": 0.491697565694019,
      "grad_norm": 12.9375,
      "learning_rate": 9.682607993974932e-05,
      "loss": 8.2477,
      "step": 3050
    },
    {
      "epoch": 0.4933096888602289,
      "grad_norm": 7.40625,
      "learning_rate": 9.681532088869763e-05,
      "loss": 3.0453,
      "step": 3060
    },
    {
      "epoch": 0.4949218120264388,
      "grad_norm": 7.15625,
      "learning_rate": 9.680456183764592e-05,
      "loss": 6.7367,
      "step": 3070
    },
    {
      "epoch": 0.4965339351926487,
      "grad_norm": 5.21875,
      "learning_rate": 9.679380278659422e-05,
      "loss": 6.6539,
      "step": 3080
    },
    {
      "epoch": 0.4981460583588586,
      "grad_norm": 7.15625,
      "learning_rate": 9.678304373554253e-05,
      "loss": 5.9859,
      "step": 3090
    },
    {
      "epoch": 0.4997581815250685,
      "grad_norm": 7.9375,
      "learning_rate": 9.677228468449083e-05,
      "loss": 5.2734,
      "step": 3100
    },
    {
      "epoch": 0.5013703046912784,
      "grad_norm": 4.03125,
      "learning_rate": 9.676152563343914e-05,
      "loss": 4.8766,
      "step": 3110
    },
    {
      "epoch": 0.5029824278574883,
      "grad_norm": 5.21875,
      "learning_rate": 9.675076658238745e-05,
      "loss": 3.8273,
      "step": 3120
    },
    {
      "epoch": 0.5045945510236982,
      "grad_norm": 9.0,
      "learning_rate": 9.674000753133574e-05,
      "loss": 6.325,
      "step": 3130
    },
    {
      "epoch": 0.5062066741899081,
      "grad_norm": 5.875,
      "learning_rate": 9.672924848028404e-05,
      "loss": 5.1109,
      "step": 3140
    },
    {
      "epoch": 0.507818797356118,
      "grad_norm": 4.34375,
      "learning_rate": 9.671848942923235e-05,
      "loss": 4.0445,
      "step": 3150
    },
    {
      "epoch": 0.5094309205223279,
      "grad_norm": 6.03125,
      "learning_rate": 9.670773037818065e-05,
      "loss": 5.4156,
      "step": 3160
    },
    {
      "epoch": 0.5110430436885378,
      "grad_norm": 5.53125,
      "learning_rate": 9.669697132712896e-05,
      "loss": 5.5141,
      "step": 3170
    },
    {
      "epoch": 0.5126551668547477,
      "grad_norm": 9.625,
      "learning_rate": 9.668621227607727e-05,
      "loss": 4.4461,
      "step": 3180
    },
    {
      "epoch": 0.5142672900209576,
      "grad_norm": 9.75,
      "learning_rate": 9.667545322502556e-05,
      "loss": 5.3461,
      "step": 3190
    },
    {
      "epoch": 0.5158794131871675,
      "grad_norm": 7.90625,
      "learning_rate": 9.666469417397386e-05,
      "loss": 3.9703,
      "step": 3200
    },
    {
      "epoch": 0.5174915363533774,
      "grad_norm": 4.40625,
      "learning_rate": 9.665393512292217e-05,
      "loss": 3.4047,
      "step": 3210
    },
    {
      "epoch": 0.5191036595195873,
      "grad_norm": 5.96875,
      "learning_rate": 9.664317607187047e-05,
      "loss": 4.1367,
      "step": 3220
    },
    {
      "epoch": 0.5207157826857972,
      "grad_norm": 7.59375,
      "learning_rate": 9.663241702081876e-05,
      "loss": 7.7125,
      "step": 3230
    },
    {
      "epoch": 0.5223279058520071,
      "grad_norm": 7.75,
      "learning_rate": 9.662165796976707e-05,
      "loss": 4.3086,
      "step": 3240
    },
    {
      "epoch": 0.523940029018217,
      "grad_norm": 6.125,
      "learning_rate": 9.661089891871538e-05,
      "loss": 3.9734,
      "step": 3250
    },
    {
      "epoch": 0.5255521521844269,
      "grad_norm": 4.25,
      "learning_rate": 9.660013986766368e-05,
      "loss": 4.3465,
      "step": 3260
    },
    {
      "epoch": 0.5271642753506368,
      "grad_norm": 14.5,
      "learning_rate": 9.658938081661197e-05,
      "loss": 5.3625,
      "step": 3270
    },
    {
      "epoch": 0.5287763985168467,
      "grad_norm": 8.0625,
      "learning_rate": 9.657862176556029e-05,
      "loss": 5.0266,
      "step": 3280
    },
    {
      "epoch": 0.5303885216830566,
      "grad_norm": 11.0625,
      "learning_rate": 9.656786271450858e-05,
      "loss": 5.6969,
      "step": 3290
    },
    {
      "epoch": 0.5320006448492665,
      "grad_norm": 11.125,
      "learning_rate": 9.655710366345688e-05,
      "loss": 6.0453,
      "step": 3300
    },
    {
      "epoch": 0.5336127680154764,
      "grad_norm": 9.3125,
      "learning_rate": 9.654634461240519e-05,
      "loss": 5.4,
      "step": 3310
    },
    {
      "epoch": 0.5352248911816863,
      "grad_norm": 10.0,
      "learning_rate": 9.65355855613535e-05,
      "loss": 4.6109,
      "step": 3320
    },
    {
      "epoch": 0.5368370143478962,
      "grad_norm": 4.1875,
      "learning_rate": 9.65248265103018e-05,
      "loss": 4.5134,
      "step": 3330
    },
    {
      "epoch": 0.5384491375141061,
      "grad_norm": 6.875,
      "learning_rate": 9.65140674592501e-05,
      "loss": 4.3398,
      "step": 3340
    },
    {
      "epoch": 0.540061260680316,
      "grad_norm": 5.625,
      "learning_rate": 9.65033084081984e-05,
      "loss": 6.0453,
      "step": 3350
    },
    {
      "epoch": 0.5416733838465259,
      "grad_norm": 4.09375,
      "learning_rate": 9.64925493571467e-05,
      "loss": 6.4672,
      "step": 3360
    },
    {
      "epoch": 0.5432855070127358,
      "grad_norm": 6.625,
      "learning_rate": 9.648179030609501e-05,
      "loss": 5.1039,
      "step": 3370
    },
    {
      "epoch": 0.5448976301789457,
      "grad_norm": 11.0,
      "learning_rate": 9.64710312550433e-05,
      "loss": 3.5141,
      "step": 3380
    },
    {
      "epoch": 0.5465097533451556,
      "grad_norm": 5.71875,
      "learning_rate": 9.646027220399161e-05,
      "loss": 4.3148,
      "step": 3390
    },
    {
      "epoch": 0.5481218765113655,
      "grad_norm": 10.9375,
      "learning_rate": 9.644951315293992e-05,
      "loss": 5.4516,
      "step": 3400
    },
    {
      "epoch": 0.5497339996775754,
      "grad_norm": 6.09375,
      "learning_rate": 9.643875410188822e-05,
      "loss": 4.4266,
      "step": 3410
    },
    {
      "epoch": 0.5513461228437853,
      "grad_norm": 6.125,
      "learning_rate": 9.642799505083652e-05,
      "loss": 3.2055,
      "step": 3420
    },
    {
      "epoch": 0.5529582460099952,
      "grad_norm": 8.75,
      "learning_rate": 9.641723599978483e-05,
      "loss": 4.4953,
      "step": 3430
    },
    {
      "epoch": 0.5545703691762051,
      "grad_norm": 15.375,
      "learning_rate": 9.640647694873312e-05,
      "loss": 4.5352,
      "step": 3440
    },
    {
      "epoch": 0.556182492342415,
      "grad_norm": 4.59375,
      "learning_rate": 9.639571789768142e-05,
      "loss": 5.5172,
      "step": 3450
    },
    {
      "epoch": 0.5577946155086249,
      "grad_norm": 11.1875,
      "learning_rate": 9.638495884662973e-05,
      "loss": 5.3,
      "step": 3460
    },
    {
      "epoch": 0.5594067386748347,
      "grad_norm": 8.6875,
      "learning_rate": 9.637419979557804e-05,
      "loss": 5.0992,
      "step": 3470
    },
    {
      "epoch": 0.5610188618410447,
      "grad_norm": 5.84375,
      "learning_rate": 9.636344074452634e-05,
      "loss": 4.2781,
      "step": 3480
    },
    {
      "epoch": 0.5626309850072545,
      "grad_norm": 4.96875,
      "learning_rate": 9.635268169347465e-05,
      "loss": 4.7141,
      "step": 3490
    },
    {
      "epoch": 0.5642431081734645,
      "grad_norm": 3.734375,
      "learning_rate": 9.634192264242294e-05,
      "loss": 4.8164,
      "step": 3500
    },
    {
      "epoch": 0.5658552313396743,
      "grad_norm": 18.375,
      "learning_rate": 9.633116359137124e-05,
      "loss": 4.1344,
      "step": 3510
    },
    {
      "epoch": 0.5674673545058843,
      "grad_norm": 5.4375,
      "learning_rate": 9.632040454031955e-05,
      "loss": 4.8781,
      "step": 3520
    },
    {
      "epoch": 0.5690794776720941,
      "grad_norm": 11.625,
      "learning_rate": 9.630964548926785e-05,
      "loss": 4.0375,
      "step": 3530
    },
    {
      "epoch": 0.5706916008383041,
      "grad_norm": 4.6875,
      "learning_rate": 9.629888643821616e-05,
      "loss": 3.025,
      "step": 3540
    },
    {
      "epoch": 0.5723037240045139,
      "grad_norm": 3.703125,
      "learning_rate": 9.628812738716447e-05,
      "loss": 3.425,
      "step": 3550
    },
    {
      "epoch": 0.5739158471707239,
      "grad_norm": 13.0625,
      "learning_rate": 9.627736833611276e-05,
      "loss": 4.8539,
      "step": 3560
    },
    {
      "epoch": 0.5755279703369337,
      "grad_norm": 4.25,
      "learning_rate": 9.626660928506106e-05,
      "loss": 5.2701,
      "step": 3570
    },
    {
      "epoch": 0.5771400935031437,
      "grad_norm": 8.875,
      "learning_rate": 9.625585023400937e-05,
      "loss": 4.1172,
      "step": 3580
    },
    {
      "epoch": 0.5787522166693535,
      "grad_norm": 4.1875,
      "learning_rate": 9.624509118295767e-05,
      "loss": 3.7031,
      "step": 3590
    },
    {
      "epoch": 0.5803643398355635,
      "grad_norm": 7.9375,
      "learning_rate": 9.623433213190596e-05,
      "loss": 5.7797,
      "step": 3600
    },
    {
      "epoch": 0.5819764630017733,
      "grad_norm": 9.8125,
      "learning_rate": 9.622357308085427e-05,
      "loss": 5.0039,
      "step": 3610
    },
    {
      "epoch": 0.5835885861679833,
      "grad_norm": 7.15625,
      "learning_rate": 9.621281402980258e-05,
      "loss": 6.9188,
      "step": 3620
    },
    {
      "epoch": 0.5852007093341931,
      "grad_norm": 11.0,
      "learning_rate": 9.620205497875088e-05,
      "loss": 4.6047,
      "step": 3630
    },
    {
      "epoch": 0.5868128325004031,
      "grad_norm": 4.21875,
      "learning_rate": 9.619129592769918e-05,
      "loss": 3.4867,
      "step": 3640
    },
    {
      "epoch": 0.5884249556666129,
      "grad_norm": 4.15625,
      "learning_rate": 9.618053687664749e-05,
      "loss": 5.9951,
      "step": 3650
    },
    {
      "epoch": 0.5900370788328229,
      "grad_norm": 12.25,
      "learning_rate": 9.616977782559578e-05,
      "loss": 4.6039,
      "step": 3660
    },
    {
      "epoch": 0.5916492019990327,
      "grad_norm": 4.25,
      "learning_rate": 9.615901877454408e-05,
      "loss": 4.6141,
      "step": 3670
    },
    {
      "epoch": 0.5932613251652427,
      "grad_norm": 4.40625,
      "learning_rate": 9.614825972349239e-05,
      "loss": 5.325,
      "step": 3680
    },
    {
      "epoch": 0.5948734483314525,
      "grad_norm": 6.78125,
      "learning_rate": 9.61375006724407e-05,
      "loss": 6.6437,
      "step": 3690
    },
    {
      "epoch": 0.5964855714976625,
      "grad_norm": 6.625,
      "learning_rate": 9.6126741621389e-05,
      "loss": 3.2922,
      "step": 3700
    },
    {
      "epoch": 0.5980976946638723,
      "grad_norm": 6.59375,
      "learning_rate": 9.61159825703373e-05,
      "loss": 6.0594,
      "step": 3710
    },
    {
      "epoch": 0.5997098178300823,
      "grad_norm": 5.625,
      "learning_rate": 9.61052235192856e-05,
      "loss": 5.2062,
      "step": 3720
    },
    {
      "epoch": 0.6013219409962921,
      "grad_norm": 12.5,
      "learning_rate": 9.60944644682339e-05,
      "loss": 5.3625,
      "step": 3730
    },
    {
      "epoch": 0.6029340641625021,
      "grad_norm": 15.4375,
      "learning_rate": 9.608370541718221e-05,
      "loss": 3.7052,
      "step": 3740
    },
    {
      "epoch": 0.6045461873287119,
      "grad_norm": 5.84375,
      "learning_rate": 9.60729463661305e-05,
      "loss": 6.8219,
      "step": 3750
    },
    {
      "epoch": 0.6061583104949219,
      "grad_norm": 5.875,
      "learning_rate": 9.606218731507882e-05,
      "loss": 5.1859,
      "step": 3760
    },
    {
      "epoch": 0.6077704336611317,
      "grad_norm": 4.15625,
      "learning_rate": 9.605142826402713e-05,
      "loss": 4.9612,
      "step": 3770
    },
    {
      "epoch": 0.6093825568273417,
      "grad_norm": 6.8125,
      "learning_rate": 9.604066921297542e-05,
      "loss": 4.7148,
      "step": 3780
    },
    {
      "epoch": 0.6109946799935515,
      "grad_norm": 6.59375,
      "learning_rate": 9.602991016192372e-05,
      "loss": 4.1761,
      "step": 3790
    },
    {
      "epoch": 0.6126068031597615,
      "grad_norm": 6.28125,
      "learning_rate": 9.601915111087203e-05,
      "loss": 3.6875,
      "step": 3800
    },
    {
      "epoch": 0.6142189263259713,
      "grad_norm": 16.875,
      "learning_rate": 9.600839205982032e-05,
      "loss": 3.5217,
      "step": 3810
    },
    {
      "epoch": 0.6158310494921811,
      "grad_norm": 6.59375,
      "learning_rate": 9.599763300876862e-05,
      "loss": 3.4383,
      "step": 3820
    },
    {
      "epoch": 0.6174431726583911,
      "grad_norm": 9.1875,
      "learning_rate": 9.598687395771695e-05,
      "loss": 3.402,
      "step": 3830
    },
    {
      "epoch": 0.619055295824601,
      "grad_norm": 5.65625,
      "learning_rate": 9.597611490666524e-05,
      "loss": 3.2852,
      "step": 3840
    },
    {
      "epoch": 0.6206674189908109,
      "grad_norm": 4.0,
      "learning_rate": 9.596535585561354e-05,
      "loss": 4.2313,
      "step": 3850
    },
    {
      "epoch": 0.6222795421570207,
      "grad_norm": 16.0,
      "learning_rate": 9.595459680456185e-05,
      "loss": 4.3102,
      "step": 3860
    },
    {
      "epoch": 0.6238916653232307,
      "grad_norm": 12.1875,
      "learning_rate": 9.594383775351014e-05,
      "loss": 8.3517,
      "step": 3870
    },
    {
      "epoch": 0.6255037884894405,
      "grad_norm": 11.125,
      "learning_rate": 9.593307870245844e-05,
      "loss": 3.8586,
      "step": 3880
    },
    {
      "epoch": 0.6271159116556505,
      "grad_norm": 4.28125,
      "learning_rate": 9.592231965140675e-05,
      "loss": 4.0477,
      "step": 3890
    },
    {
      "epoch": 0.6287280348218603,
      "grad_norm": 10.25,
      "learning_rate": 9.591156060035506e-05,
      "loss": 3.9195,
      "step": 3900
    },
    {
      "epoch": 0.6303401579880703,
      "grad_norm": 6.5,
      "learning_rate": 9.590080154930336e-05,
      "loss": 5.5156,
      "step": 3910
    },
    {
      "epoch": 0.6319522811542801,
      "grad_norm": 7.125,
      "learning_rate": 9.589004249825167e-05,
      "loss": 7.607,
      "step": 3920
    },
    {
      "epoch": 0.6335644043204901,
      "grad_norm": 5.90625,
      "learning_rate": 9.587928344719996e-05,
      "loss": 3.2469,
      "step": 3930
    },
    {
      "epoch": 0.6351765274867,
      "grad_norm": 5.96875,
      "learning_rate": 9.586852439614826e-05,
      "loss": 3.95,
      "step": 3940
    },
    {
      "epoch": 0.6367886506529099,
      "grad_norm": 7.625,
      "learning_rate": 9.585776534509656e-05,
      "loss": 6.4102,
      "step": 3950
    },
    {
      "epoch": 0.6384007738191197,
      "grad_norm": 4.09375,
      "learning_rate": 9.584700629404487e-05,
      "loss": 4.3438,
      "step": 3960
    },
    {
      "epoch": 0.6400128969853297,
      "grad_norm": 6.71875,
      "learning_rate": 9.583624724299318e-05,
      "loss": 3.6531,
      "step": 3970
    },
    {
      "epoch": 0.6416250201515395,
      "grad_norm": 12.625,
      "learning_rate": 9.582548819194147e-05,
      "loss": 4.9672,
      "step": 3980
    },
    {
      "epoch": 0.6432371433177495,
      "grad_norm": 8.5625,
      "learning_rate": 9.581472914088978e-05,
      "loss": 5.7719,
      "step": 3990
    },
    {
      "epoch": 0.6448492664839593,
      "grad_norm": 6.34375,
      "learning_rate": 9.580397008983808e-05,
      "loss": 4.2578,
      "step": 4000
    },
    {
      "epoch": 0.6464613896501693,
      "grad_norm": 17.25,
      "learning_rate": 9.579321103878638e-05,
      "loss": 3.6266,
      "step": 4010
    },
    {
      "epoch": 0.6480735128163791,
      "grad_norm": 15.125,
      "learning_rate": 9.578245198773469e-05,
      "loss": 4.6651,
      "step": 4020
    },
    {
      "epoch": 0.6496856359825891,
      "grad_norm": 11.1875,
      "learning_rate": 9.577169293668298e-05,
      "loss": 7.1953,
      "step": 4030
    },
    {
      "epoch": 0.651297759148799,
      "grad_norm": 7.0625,
      "learning_rate": 9.57609338856313e-05,
      "loss": 3.5945,
      "step": 4040
    },
    {
      "epoch": 0.6529098823150089,
      "grad_norm": 7.0625,
      "learning_rate": 9.57501748345796e-05,
      "loss": 3.9807,
      "step": 4050
    },
    {
      "epoch": 0.6545220054812187,
      "grad_norm": 4.46875,
      "learning_rate": 9.57394157835279e-05,
      "loss": 4.2508,
      "step": 4060
    },
    {
      "epoch": 0.6561341286474287,
      "grad_norm": 8.5,
      "learning_rate": 9.57286567324762e-05,
      "loss": 5.2711,
      "step": 4070
    },
    {
      "epoch": 0.6577462518136385,
      "grad_norm": 8.375,
      "learning_rate": 9.57178976814245e-05,
      "loss": 4.8,
      "step": 4080
    },
    {
      "epoch": 0.6593583749798485,
      "grad_norm": 11.6875,
      "learning_rate": 9.57071386303728e-05,
      "loss": 5.8031,
      "step": 4090
    },
    {
      "epoch": 0.6609704981460583,
      "grad_norm": 5.8125,
      "learning_rate": 9.56963795793211e-05,
      "loss": 4.6328,
      "step": 4100
    },
    {
      "epoch": 0.6625826213122683,
      "grad_norm": 0.0,
      "learning_rate": 9.568562052826941e-05,
      "loss": 6.8063,
      "step": 4110
    },
    {
      "epoch": 0.6641947444784781,
      "grad_norm": 3.78125,
      "learning_rate": 9.567486147721772e-05,
      "loss": 4.6758,
      "step": 4120
    },
    {
      "epoch": 0.6658068676446881,
      "grad_norm": 5.9375,
      "learning_rate": 9.566410242616602e-05,
      "loss": 3.2344,
      "step": 4130
    },
    {
      "epoch": 0.667418990810898,
      "grad_norm": 4.125,
      "learning_rate": 9.565334337511433e-05,
      "loss": 4.1797,
      "step": 4140
    },
    {
      "epoch": 0.6690311139771079,
      "grad_norm": 5.5625,
      "learning_rate": 9.564258432406262e-05,
      "loss": 3.2945,
      "step": 4150
    },
    {
      "epoch": 0.6706432371433177,
      "grad_norm": 5.34375,
      "learning_rate": 9.563182527301092e-05,
      "loss": 5.3279,
      "step": 4160
    },
    {
      "epoch": 0.6722553603095276,
      "grad_norm": 5.96875,
      "learning_rate": 9.562106622195923e-05,
      "loss": 3.7953,
      "step": 4170
    },
    {
      "epoch": 0.6738674834757375,
      "grad_norm": 14.0625,
      "learning_rate": 9.561030717090753e-05,
      "loss": 4.5656,
      "step": 4180
    },
    {
      "epoch": 0.6754796066419474,
      "grad_norm": 6.03125,
      "learning_rate": 9.559954811985584e-05,
      "loss": 3.4297,
      "step": 4190
    },
    {
      "epoch": 0.6770917298081573,
      "grad_norm": 11.0,
      "learning_rate": 9.558878906880415e-05,
      "loss": 4.7391,
      "step": 4200
    },
    {
      "epoch": 0.6787038529743672,
      "grad_norm": 11.4375,
      "learning_rate": 9.557803001775244e-05,
      "loss": 6.5797,
      "step": 4210
    },
    {
      "epoch": 0.6803159761405771,
      "grad_norm": 4.21875,
      "learning_rate": 9.556727096670074e-05,
      "loss": 3.9672,
      "step": 4220
    },
    {
      "epoch": 0.681928099306787,
      "grad_norm": 5.9375,
      "learning_rate": 9.555651191564905e-05,
      "loss": 3.9813,
      "step": 4230
    },
    {
      "epoch": 0.683540222472997,
      "grad_norm": 11.6875,
      "learning_rate": 9.554575286459735e-05,
      "loss": 6.0148,
      "step": 4240
    },
    {
      "epoch": 0.6851523456392068,
      "grad_norm": 5.09375,
      "learning_rate": 9.553499381354564e-05,
      "loss": 6.15,
      "step": 4250
    },
    {
      "epoch": 0.6867644688054168,
      "grad_norm": 5.90625,
      "learning_rate": 9.552423476249395e-05,
      "loss": 3.7195,
      "step": 4260
    },
    {
      "epoch": 0.6883765919716266,
      "grad_norm": 6.3125,
      "learning_rate": 9.551347571144226e-05,
      "loss": 4.1609,
      "step": 4270
    },
    {
      "epoch": 0.6899887151378366,
      "grad_norm": 7.5,
      "learning_rate": 9.550271666039056e-05,
      "loss": 4.6097,
      "step": 4280
    },
    {
      "epoch": 0.6916008383040464,
      "grad_norm": 11.5625,
      "learning_rate": 9.549195760933886e-05,
      "loss": 6.875,
      "step": 4290
    },
    {
      "epoch": 0.6932129614702564,
      "grad_norm": 19.375,
      "learning_rate": 9.548119855828717e-05,
      "loss": 5.0152,
      "step": 4300
    },
    {
      "epoch": 0.6948250846364662,
      "grad_norm": 6.40625,
      "learning_rate": 9.547043950723546e-05,
      "loss": 2.8883,
      "step": 4310
    },
    {
      "epoch": 0.6964372078026762,
      "grad_norm": 4.15625,
      "learning_rate": 9.545968045618376e-05,
      "loss": 5.1141,
      "step": 4320
    },
    {
      "epoch": 0.698049330968886,
      "grad_norm": 11.8125,
      "learning_rate": 9.544892140513207e-05,
      "loss": 4.3656,
      "step": 4330
    },
    {
      "epoch": 0.699661454135096,
      "grad_norm": 7.40625,
      "learning_rate": 9.543816235408038e-05,
      "loss": 4.2687,
      "step": 4340
    },
    {
      "epoch": 0.7012735773013058,
      "grad_norm": 3.96875,
      "learning_rate": 9.542740330302867e-05,
      "loss": 3.9734,
      "step": 4350
    },
    {
      "epoch": 0.7028857004675158,
      "grad_norm": 4.0625,
      "learning_rate": 9.541664425197698e-05,
      "loss": 3.1008,
      "step": 4360
    },
    {
      "epoch": 0.7044978236337256,
      "grad_norm": 6.3125,
      "learning_rate": 9.540588520092528e-05,
      "loss": 5.032,
      "step": 4370
    },
    {
      "epoch": 0.7061099467999356,
      "grad_norm": 21.75,
      "learning_rate": 9.539512614987358e-05,
      "loss": 4.5211,
      "step": 4380
    },
    {
      "epoch": 0.7077220699661454,
      "grad_norm": 4.40625,
      "learning_rate": 9.538436709882189e-05,
      "loss": 6.6359,
      "step": 4390
    },
    {
      "epoch": 0.7093341931323554,
      "grad_norm": 4.71875,
      "learning_rate": 9.537360804777018e-05,
      "loss": 5.0445,
      "step": 4400
    },
    {
      "epoch": 0.7109463162985652,
      "grad_norm": 4.46875,
      "learning_rate": 9.53628489967185e-05,
      "loss": 4.8812,
      "step": 4410
    },
    {
      "epoch": 0.7125584394647752,
      "grad_norm": 11.625,
      "learning_rate": 9.53520899456668e-05,
      "loss": 6.2234,
      "step": 4420
    },
    {
      "epoch": 0.714170562630985,
      "grad_norm": 3.953125,
      "learning_rate": 9.53413308946151e-05,
      "loss": 3.3258,
      "step": 4430
    },
    {
      "epoch": 0.715782685797195,
      "grad_norm": 12.4375,
      "learning_rate": 9.53305718435634e-05,
      "loss": 5.5245,
      "step": 4440
    },
    {
      "epoch": 0.7173948089634048,
      "grad_norm": 4.25,
      "learning_rate": 9.531981279251171e-05,
      "loss": 3.9523,
      "step": 4450
    },
    {
      "epoch": 0.7190069321296148,
      "grad_norm": 4.03125,
      "learning_rate": 9.530905374146e-05,
      "loss": 3.9727,
      "step": 4460
    },
    {
      "epoch": 0.7206190552958246,
      "grad_norm": 5.59375,
      "learning_rate": 9.52982946904083e-05,
      "loss": 4.8127,
      "step": 4470
    },
    {
      "epoch": 0.7222311784620346,
      "grad_norm": 12.125,
      "learning_rate": 9.528753563935662e-05,
      "loss": 4.9891,
      "step": 4480
    },
    {
      "epoch": 0.7238433016282444,
      "grad_norm": 13.1875,
      "learning_rate": 9.527677658830492e-05,
      "loss": 5.2539,
      "step": 4490
    },
    {
      "epoch": 0.7254554247944544,
      "grad_norm": 4.28125,
      "learning_rate": 9.526601753725322e-05,
      "loss": 4.025,
      "step": 4500
    },
    {
      "epoch": 0.7270675479606642,
      "grad_norm": 11.1875,
      "learning_rate": 9.525525848620153e-05,
      "loss": 4.6415,
      "step": 4510
    },
    {
      "epoch": 0.728679671126874,
      "grad_norm": 5.03125,
      "learning_rate": 9.524449943514982e-05,
      "loss": 3.6906,
      "step": 4520
    },
    {
      "epoch": 0.730291794293084,
      "grad_norm": 14.5625,
      "learning_rate": 9.523374038409812e-05,
      "loss": 5.0727,
      "step": 4530
    },
    {
      "epoch": 0.7319039174592938,
      "grad_norm": 12.1875,
      "learning_rate": 9.522298133304643e-05,
      "loss": 4.2883,
      "step": 4540
    },
    {
      "epoch": 0.7335160406255038,
      "grad_norm": 6.65625,
      "learning_rate": 9.521222228199474e-05,
      "loss": 4.9023,
      "step": 4550
    },
    {
      "epoch": 0.7351281637917136,
      "grad_norm": 4.0625,
      "learning_rate": 9.520146323094304e-05,
      "loss": 4.8438,
      "step": 4560
    },
    {
      "epoch": 0.7367402869579236,
      "grad_norm": 15.0625,
      "learning_rate": 9.519070417989135e-05,
      "loss": 4.2234,
      "step": 4570
    },
    {
      "epoch": 0.7383524101241334,
      "grad_norm": 9.9375,
      "learning_rate": 9.517994512883964e-05,
      "loss": 5.6391,
      "step": 4580
    },
    {
      "epoch": 0.7399645332903434,
      "grad_norm": 10.375,
      "learning_rate": 9.516918607778794e-05,
      "loss": 6.4859,
      "step": 4590
    },
    {
      "epoch": 0.7415766564565532,
      "grad_norm": 4.15625,
      "learning_rate": 9.515842702673624e-05,
      "loss": 4.5695,
      "step": 4600
    },
    {
      "epoch": 0.7431887796227632,
      "grad_norm": 6.15625,
      "learning_rate": 9.514766797568455e-05,
      "loss": 3.3211,
      "step": 4610
    },
    {
      "epoch": 0.744800902788973,
      "grad_norm": 15.1875,
      "learning_rate": 9.513690892463286e-05,
      "loss": 3.8424,
      "step": 4620
    },
    {
      "epoch": 0.746413025955183,
      "grad_norm": 7.75,
      "learning_rate": 9.512614987358115e-05,
      "loss": 3.7391,
      "step": 4630
    },
    {
      "epoch": 0.7480251491213928,
      "grad_norm": 5.0625,
      "learning_rate": 9.511539082252946e-05,
      "loss": 5.2734,
      "step": 4640
    },
    {
      "epoch": 0.7496372722876028,
      "grad_norm": 5.6875,
      "learning_rate": 9.510463177147776e-05,
      "loss": 3.0953,
      "step": 4650
    },
    {
      "epoch": 0.7512493954538126,
      "grad_norm": 21.25,
      "learning_rate": 9.509387272042606e-05,
      "loss": 4.1445,
      "step": 4660
    },
    {
      "epoch": 0.7528615186200226,
      "grad_norm": 4.6875,
      "learning_rate": 9.508311366937437e-05,
      "loss": 4.2602,
      "step": 4670
    },
    {
      "epoch": 0.7544736417862324,
      "grad_norm": 10.875,
      "learning_rate": 9.507235461832266e-05,
      "loss": 6.8906,
      "step": 4680
    },
    {
      "epoch": 0.7560857649524424,
      "grad_norm": 4.96875,
      "learning_rate": 9.506159556727097e-05,
      "loss": 5.8539,
      "step": 4690
    },
    {
      "epoch": 0.7576978881186522,
      "grad_norm": 5.90625,
      "learning_rate": 9.505083651621928e-05,
      "loss": 4.2883,
      "step": 4700
    },
    {
      "epoch": 0.7593100112848622,
      "grad_norm": 6.65625,
      "learning_rate": 9.504007746516758e-05,
      "loss": 4.6055,
      "step": 4710
    },
    {
      "epoch": 0.760922134451072,
      "grad_norm": 4.09375,
      "learning_rate": 9.502931841411588e-05,
      "loss": 3.3281,
      "step": 4720
    },
    {
      "epoch": 0.762534257617282,
      "grad_norm": 11.9375,
      "learning_rate": 9.501855936306419e-05,
      "loss": 3.7367,
      "step": 4730
    },
    {
      "epoch": 0.7641463807834918,
      "grad_norm": 6.3125,
      "learning_rate": 9.500780031201248e-05,
      "loss": 4.6312,
      "step": 4740
    },
    {
      "epoch": 0.7657585039497018,
      "grad_norm": 5.9375,
      "learning_rate": 9.499704126096078e-05,
      "loss": 4.5914,
      "step": 4750
    },
    {
      "epoch": 0.7673706271159116,
      "grad_norm": 6.21875,
      "learning_rate": 9.498628220990909e-05,
      "loss": 4.7703,
      "step": 4760
    },
    {
      "epoch": 0.7689827502821216,
      "grad_norm": 7.0625,
      "learning_rate": 9.49755231588574e-05,
      "loss": 5.47,
      "step": 4770
    },
    {
      "epoch": 0.7705948734483314,
      "grad_norm": 12.75,
      "learning_rate": 9.49647641078057e-05,
      "loss": 5.6068,
      "step": 4780
    },
    {
      "epoch": 0.7722069966145414,
      "grad_norm": 21.75,
      "learning_rate": 9.4954005056754e-05,
      "loss": 7.7266,
      "step": 4790
    },
    {
      "epoch": 0.7738191197807512,
      "grad_norm": 3.9375,
      "learning_rate": 9.49432460057023e-05,
      "loss": 6.0881,
      "step": 4800
    },
    {
      "epoch": 0.7754312429469612,
      "grad_norm": 7.09375,
      "learning_rate": 9.49324869546506e-05,
      "loss": 5.7422,
      "step": 4810
    },
    {
      "epoch": 0.777043366113171,
      "grad_norm": 7.21875,
      "learning_rate": 9.492172790359891e-05,
      "loss": 4.0422,
      "step": 4820
    },
    {
      "epoch": 0.778655489279381,
      "grad_norm": 5.53125,
      "learning_rate": 9.49109688525472e-05,
      "loss": 3.7773,
      "step": 4830
    },
    {
      "epoch": 0.7802676124455908,
      "grad_norm": 6.9375,
      "learning_rate": 9.490020980149552e-05,
      "loss": 5.6609,
      "step": 4840
    },
    {
      "epoch": 0.7818797356118008,
      "grad_norm": 10.75,
      "learning_rate": 9.488945075044383e-05,
      "loss": 4.8656,
      "step": 4850
    },
    {
      "epoch": 0.7834918587780106,
      "grad_norm": 7.71875,
      "learning_rate": 9.487869169939212e-05,
      "loss": 5.5398,
      "step": 4860
    },
    {
      "epoch": 0.7851039819442205,
      "grad_norm": 12.5,
      "learning_rate": 9.486793264834042e-05,
      "loss": 6.0812,
      "step": 4870
    },
    {
      "epoch": 0.7867161051104304,
      "grad_norm": 4.5,
      "learning_rate": 9.485717359728873e-05,
      "loss": 3.0984,
      "step": 4880
    },
    {
      "epoch": 0.7883282282766403,
      "grad_norm": 12.5,
      "learning_rate": 9.484641454623702e-05,
      "loss": 4.3594,
      "step": 4890
    },
    {
      "epoch": 0.7899403514428502,
      "grad_norm": 11.9375,
      "learning_rate": 9.483565549518532e-05,
      "loss": 3.7008,
      "step": 4900
    },
    {
      "epoch": 0.7915524746090601,
      "grad_norm": 7.59375,
      "learning_rate": 9.482489644413363e-05,
      "loss": 4.0258,
      "step": 4910
    },
    {
      "epoch": 0.79316459777527,
      "grad_norm": 11.9375,
      "learning_rate": 9.481413739308194e-05,
      "loss": 5.7484,
      "step": 4920
    },
    {
      "epoch": 0.7947767209414799,
      "grad_norm": 18.125,
      "learning_rate": 9.480337834203024e-05,
      "loss": 4.3083,
      "step": 4930
    },
    {
      "epoch": 0.7963888441076898,
      "grad_norm": 4.40625,
      "learning_rate": 9.479261929097853e-05,
      "loss": 3.8797,
      "step": 4940
    },
    {
      "epoch": 0.7980009672738997,
      "grad_norm": 4.65625,
      "learning_rate": 9.478186023992684e-05,
      "loss": 3.4125,
      "step": 4950
    },
    {
      "epoch": 0.7996130904401096,
      "grad_norm": 0.0,
      "learning_rate": 9.477110118887514e-05,
      "loss": 4.3261,
      "step": 4960
    },
    {
      "epoch": 0.8012252136063195,
      "grad_norm": 6.15625,
      "learning_rate": 9.476034213782344e-05,
      "loss": 4.7094,
      "step": 4970
    },
    {
      "epoch": 0.8028373367725294,
      "grad_norm": 7.21875,
      "learning_rate": 9.474958308677175e-05,
      "loss": 2.8477,
      "step": 4980
    },
    {
      "epoch": 0.8044494599387393,
      "grad_norm": 5.46875,
      "learning_rate": 9.473882403572006e-05,
      "loss": 4.1312,
      "step": 4990
    },
    {
      "epoch": 0.8060615831049492,
      "grad_norm": 6.71875,
      "learning_rate": 9.472806498466835e-05,
      "loss": 5.3789,
      "step": 5000
    },
    {
      "epoch": 0.8076737062711591,
      "grad_norm": 10.625,
      "learning_rate": 9.471730593361666e-05,
      "loss": 6.2062,
      "step": 5010
    },
    {
      "epoch": 0.809285829437369,
      "grad_norm": 7.53125,
      "learning_rate": 9.470654688256496e-05,
      "loss": 4.9484,
      "step": 5020
    },
    {
      "epoch": 0.8108979526035789,
      "grad_norm": 4.34375,
      "learning_rate": 9.469578783151326e-05,
      "loss": 6.1742,
      "step": 5030
    },
    {
      "epoch": 0.8125100757697888,
      "grad_norm": 15.375,
      "learning_rate": 9.468502878046157e-05,
      "loss": 5.5594,
      "step": 5040
    },
    {
      "epoch": 0.8141221989359987,
      "grad_norm": 0.0,
      "learning_rate": 9.467426972940986e-05,
      "loss": 3.6455,
      "step": 5050
    },
    {
      "epoch": 0.8157343221022086,
      "grad_norm": 20.125,
      "learning_rate": 9.466351067835817e-05,
      "loss": 6.7414,
      "step": 5060
    },
    {
      "epoch": 0.8173464452684185,
      "grad_norm": 15.3125,
      "learning_rate": 9.465275162730648e-05,
      "loss": 3.9758,
      "step": 5070
    },
    {
      "epoch": 0.8189585684346284,
      "grad_norm": 6.28125,
      "learning_rate": 9.464199257625478e-05,
      "loss": 5.3844,
      "step": 5080
    },
    {
      "epoch": 0.8205706916008383,
      "grad_norm": 5.8125,
      "learning_rate": 9.463123352520308e-05,
      "loss": 3.9547,
      "step": 5090
    },
    {
      "epoch": 0.8221828147670482,
      "grad_norm": 7.21875,
      "learning_rate": 9.462047447415139e-05,
      "loss": 4.4781,
      "step": 5100
    },
    {
      "epoch": 0.8237949379332581,
      "grad_norm": 5.21875,
      "learning_rate": 9.460971542309968e-05,
      "loss": 6.7375,
      "step": 5110
    },
    {
      "epoch": 0.825407061099468,
      "grad_norm": 16.5,
      "learning_rate": 9.459895637204798e-05,
      "loss": 6.1578,
      "step": 5120
    },
    {
      "epoch": 0.8270191842656779,
      "grad_norm": 8.1875,
      "learning_rate": 9.45881973209963e-05,
      "loss": 5.593,
      "step": 5130
    },
    {
      "epoch": 0.8286313074318878,
      "grad_norm": 4.3125,
      "learning_rate": 9.45774382699446e-05,
      "loss": 5.0344,
      "step": 5140
    },
    {
      "epoch": 0.8302434305980977,
      "grad_norm": 11.8125,
      "learning_rate": 9.45666792188929e-05,
      "loss": 5.3336,
      "step": 5150
    },
    {
      "epoch": 0.8318555537643076,
      "grad_norm": 5.875,
      "learning_rate": 9.45559201678412e-05,
      "loss": 4.1141,
      "step": 5160
    },
    {
      "epoch": 0.8334676769305175,
      "grad_norm": 7.0,
      "learning_rate": 9.45451611167895e-05,
      "loss": 4.8078,
      "step": 5170
    },
    {
      "epoch": 0.8350798000967274,
      "grad_norm": 6.03125,
      "learning_rate": 9.45344020657378e-05,
      "loss": 5.3148,
      "step": 5180
    },
    {
      "epoch": 0.8366919232629373,
      "grad_norm": 4.03125,
      "learning_rate": 9.452364301468611e-05,
      "loss": 5.2883,
      "step": 5190
    },
    {
      "epoch": 0.8383040464291471,
      "grad_norm": 4.4375,
      "learning_rate": 9.451288396363442e-05,
      "loss": 4.2891,
      "step": 5200
    },
    {
      "epoch": 0.8399161695953571,
      "grad_norm": 4.28125,
      "learning_rate": 9.450212491258272e-05,
      "loss": 4.325,
      "step": 5210
    },
    {
      "epoch": 0.8415282927615669,
      "grad_norm": 7.0,
      "learning_rate": 9.449136586153103e-05,
      "loss": 2.6461,
      "step": 5220
    },
    {
      "epoch": 0.8431404159277769,
      "grad_norm": 20.5,
      "learning_rate": 9.448060681047932e-05,
      "loss": 4.6539,
      "step": 5230
    },
    {
      "epoch": 0.8447525390939867,
      "grad_norm": 13.0,
      "learning_rate": 9.446984775942762e-05,
      "loss": 5.3586,
      "step": 5240
    },
    {
      "epoch": 0.8463646622601967,
      "grad_norm": 6.03125,
      "learning_rate": 9.445908870837593e-05,
      "loss": 4.2938,
      "step": 5250
    },
    {
      "epoch": 0.8479767854264065,
      "grad_norm": 14.5625,
      "learning_rate": 9.444832965732423e-05,
      "loss": 3.307,
      "step": 5260
    },
    {
      "epoch": 0.8495889085926165,
      "grad_norm": 4.84375,
      "learning_rate": 9.443757060627254e-05,
      "loss": 4.3312,
      "step": 5270
    },
    {
      "epoch": 0.8512010317588263,
      "grad_norm": 16.0,
      "learning_rate": 9.442681155522083e-05,
      "loss": 5.9328,
      "step": 5280
    },
    {
      "epoch": 0.8528131549250363,
      "grad_norm": 12.4375,
      "learning_rate": 9.441605250416914e-05,
      "loss": 3.9164,
      "step": 5290
    },
    {
      "epoch": 0.8544252780912461,
      "grad_norm": 21.25,
      "learning_rate": 9.440529345311744e-05,
      "loss": 5.9383,
      "step": 5300
    },
    {
      "epoch": 0.8560374012574561,
      "grad_norm": 39.0,
      "learning_rate": 9.439453440206574e-05,
      "loss": 6.3809,
      "step": 5310
    },
    {
      "epoch": 0.8576495244236659,
      "grad_norm": 11.0625,
      "learning_rate": 9.438377535101405e-05,
      "loss": 5.5242,
      "step": 5320
    },
    {
      "epoch": 0.8592616475898759,
      "grad_norm": 19.5,
      "learning_rate": 9.437301629996234e-05,
      "loss": 5.7922,
      "step": 5330
    },
    {
      "epoch": 0.8608737707560857,
      "grad_norm": 4.5,
      "learning_rate": 9.436225724891065e-05,
      "loss": 3.3196,
      "step": 5340
    },
    {
      "epoch": 0.8624858939222957,
      "grad_norm": 4.375,
      "learning_rate": 9.435149819785896e-05,
      "loss": 3.8977,
      "step": 5350
    },
    {
      "epoch": 0.8640980170885055,
      "grad_norm": 17.625,
      "learning_rate": 9.434073914680726e-05,
      "loss": 4.5883,
      "step": 5360
    },
    {
      "epoch": 0.8657101402547155,
      "grad_norm": 18.0,
      "learning_rate": 9.432998009575556e-05,
      "loss": 5.0922,
      "step": 5370
    },
    {
      "epoch": 0.8673222634209253,
      "grad_norm": 7.90625,
      "learning_rate": 9.431922104470387e-05,
      "loss": 3.8219,
      "step": 5380
    },
    {
      "epoch": 0.8689343865871353,
      "grad_norm": 20.5,
      "learning_rate": 9.430846199365216e-05,
      "loss": 5.691,
      "step": 5390
    },
    {
      "epoch": 0.8705465097533451,
      "grad_norm": 13.25,
      "learning_rate": 9.429770294260046e-05,
      "loss": 5.0047,
      "step": 5400
    },
    {
      "epoch": 0.8721586329195551,
      "grad_norm": 15.4375,
      "learning_rate": 9.428694389154877e-05,
      "loss": 3.3119,
      "step": 5410
    },
    {
      "epoch": 0.8737707560857649,
      "grad_norm": 14.9375,
      "learning_rate": 9.427618484049708e-05,
      "loss": 4.5906,
      "step": 5420
    },
    {
      "epoch": 0.8753828792519749,
      "grad_norm": 4.71875,
      "learning_rate": 9.426542578944537e-05,
      "loss": 4.3789,
      "step": 5430
    },
    {
      "epoch": 0.8769950024181847,
      "grad_norm": 4.375,
      "learning_rate": 9.425466673839368e-05,
      "loss": 5.7883,
      "step": 5440
    },
    {
      "epoch": 0.8786071255843947,
      "grad_norm": 17.625,
      "learning_rate": 9.424390768734198e-05,
      "loss": 5.0398,
      "step": 5450
    },
    {
      "epoch": 0.8802192487506045,
      "grad_norm": 8.75,
      "learning_rate": 9.423314863629028e-05,
      "loss": 4.6781,
      "step": 5460
    },
    {
      "epoch": 0.8818313719168145,
      "grad_norm": 9.5625,
      "learning_rate": 9.422238958523859e-05,
      "loss": 4.2867,
      "step": 5470
    },
    {
      "epoch": 0.8834434950830243,
      "grad_norm": 5.9375,
      "learning_rate": 9.421163053418688e-05,
      "loss": 4.8625,
      "step": 5480
    },
    {
      "epoch": 0.8850556182492343,
      "grad_norm": 5.625,
      "learning_rate": 9.42008714831352e-05,
      "loss": 4.8164,
      "step": 5490
    },
    {
      "epoch": 0.8866677414154441,
      "grad_norm": 4.15625,
      "learning_rate": 9.41901124320835e-05,
      "loss": 6.2062,
      "step": 5500
    },
    {
      "epoch": 0.8882798645816541,
      "grad_norm": 12.1875,
      "learning_rate": 9.41793533810318e-05,
      "loss": 3.6727,
      "step": 5510
    },
    {
      "epoch": 0.8898919877478639,
      "grad_norm": 9.125,
      "learning_rate": 9.41685943299801e-05,
      "loss": 4.1727,
      "step": 5520
    },
    {
      "epoch": 0.8915041109140739,
      "grad_norm": 12.75,
      "learning_rate": 9.415783527892841e-05,
      "loss": 4.8234,
      "step": 5530
    },
    {
      "epoch": 0.8931162340802837,
      "grad_norm": 0.0,
      "learning_rate": 9.41470762278767e-05,
      "loss": 4.3691,
      "step": 5540
    },
    {
      "epoch": 0.8947283572464936,
      "grad_norm": 4.375,
      "learning_rate": 9.4136317176825e-05,
      "loss": 4.9344,
      "step": 5550
    },
    {
      "epoch": 0.8963404804127035,
      "grad_norm": 5.59375,
      "learning_rate": 9.412555812577331e-05,
      "loss": 4.368,
      "step": 5560
    },
    {
      "epoch": 0.8979526035789134,
      "grad_norm": 11.0625,
      "learning_rate": 9.411479907472162e-05,
      "loss": 3.0305,
      "step": 5570
    },
    {
      "epoch": 0.8995647267451233,
      "grad_norm": 12.6875,
      "learning_rate": 9.410404002366992e-05,
      "loss": 6.0211,
      "step": 5580
    },
    {
      "epoch": 0.9011768499113332,
      "grad_norm": 13.625,
      "learning_rate": 9.409328097261821e-05,
      "loss": 3.9359,
      "step": 5590
    },
    {
      "epoch": 0.9027889730775431,
      "grad_norm": 4.03125,
      "learning_rate": 9.408252192156652e-05,
      "loss": 5.9586,
      "step": 5600
    },
    {
      "epoch": 0.904401096243753,
      "grad_norm": 14.5625,
      "learning_rate": 9.407176287051482e-05,
      "loss": 4.5,
      "step": 5610
    },
    {
      "epoch": 0.9060132194099629,
      "grad_norm": 7.21875,
      "learning_rate": 9.406100381946312e-05,
      "loss": 4.2414,
      "step": 5620
    },
    {
      "epoch": 0.9076253425761728,
      "grad_norm": 5.15625,
      "learning_rate": 9.405024476841143e-05,
      "loss": 4.5336,
      "step": 5630
    },
    {
      "epoch": 0.9092374657423827,
      "grad_norm": 12.9375,
      "learning_rate": 9.403948571735974e-05,
      "loss": 2.7914,
      "step": 5640
    },
    {
      "epoch": 0.9108495889085926,
      "grad_norm": 9.6875,
      "learning_rate": 9.402872666630803e-05,
      "loss": 6.7703,
      "step": 5650
    },
    {
      "epoch": 0.9124617120748025,
      "grad_norm": 8.0625,
      "learning_rate": 9.401796761525634e-05,
      "loss": 3.75,
      "step": 5660
    },
    {
      "epoch": 0.9140738352410124,
      "grad_norm": 12.625,
      "learning_rate": 9.400720856420464e-05,
      "loss": 4.4445,
      "step": 5670
    },
    {
      "epoch": 0.9156859584072223,
      "grad_norm": 14.9375,
      "learning_rate": 9.399644951315294e-05,
      "loss": 4.607,
      "step": 5680
    },
    {
      "epoch": 0.9172980815734322,
      "grad_norm": 6.25,
      "learning_rate": 9.398569046210125e-05,
      "loss": 4.2023,
      "step": 5690
    },
    {
      "epoch": 0.9189102047396421,
      "grad_norm": 5.9375,
      "learning_rate": 9.397493141104954e-05,
      "loss": 5.9922,
      "step": 5700
    },
    {
      "epoch": 0.920522327905852,
      "grad_norm": 12.1875,
      "learning_rate": 9.396417235999785e-05,
      "loss": 4.9664,
      "step": 5710
    },
    {
      "epoch": 0.9221344510720619,
      "grad_norm": 4.4375,
      "learning_rate": 9.395341330894616e-05,
      "loss": 5.038,
      "step": 5720
    },
    {
      "epoch": 0.9237465742382718,
      "grad_norm": 5.28125,
      "learning_rate": 9.394265425789446e-05,
      "loss": 4.4,
      "step": 5730
    },
    {
      "epoch": 0.9253586974044817,
      "grad_norm": 10.75,
      "learning_rate": 9.393189520684276e-05,
      "loss": 3.932,
      "step": 5740
    },
    {
      "epoch": 0.9269708205706916,
      "grad_norm": 14.3125,
      "learning_rate": 9.392113615579107e-05,
      "loss": 7.1109,
      "step": 5750
    },
    {
      "epoch": 0.9285829437369015,
      "grad_norm": 6.28125,
      "learning_rate": 9.391037710473936e-05,
      "loss": 4.0078,
      "step": 5760
    },
    {
      "epoch": 0.9301950669031114,
      "grad_norm": 6.78125,
      "learning_rate": 9.389961805368766e-05,
      "loss": 4.4172,
      "step": 5770
    },
    {
      "epoch": 0.9318071900693213,
      "grad_norm": 7.71875,
      "learning_rate": 9.388885900263597e-05,
      "loss": 5.2344,
      "step": 5780
    },
    {
      "epoch": 0.9334193132355312,
      "grad_norm": 6.3125,
      "learning_rate": 9.387809995158428e-05,
      "loss": 4.8469,
      "step": 5790
    },
    {
      "epoch": 0.9350314364017411,
      "grad_norm": 4.53125,
      "learning_rate": 9.386734090053258e-05,
      "loss": 5.8156,
      "step": 5800
    },
    {
      "epoch": 0.936643559567951,
      "grad_norm": 7.96875,
      "learning_rate": 9.385658184948089e-05,
      "loss": 4.8187,
      "step": 5810
    },
    {
      "epoch": 0.9382556827341609,
      "grad_norm": 15.5625,
      "learning_rate": 9.384582279842918e-05,
      "loss": 6.6648,
      "step": 5820
    },
    {
      "epoch": 0.9398678059003708,
      "grad_norm": 17.375,
      "learning_rate": 9.383506374737748e-05,
      "loss": 6.6531,
      "step": 5830
    },
    {
      "epoch": 0.9414799290665807,
      "grad_norm": 12.5625,
      "learning_rate": 9.382430469632579e-05,
      "loss": 6.3086,
      "step": 5840
    },
    {
      "epoch": 0.9430920522327906,
      "grad_norm": 15.875,
      "learning_rate": 9.38135456452741e-05,
      "loss": 5.3453,
      "step": 5850
    },
    {
      "epoch": 0.9447041753990005,
      "grad_norm": 14.125,
      "learning_rate": 9.38027865942224e-05,
      "loss": 5.1802,
      "step": 5860
    },
    {
      "epoch": 0.9463162985652104,
      "grad_norm": 12.75,
      "learning_rate": 9.37920275431707e-05,
      "loss": 4.5867,
      "step": 5870
    },
    {
      "epoch": 0.9479284217314203,
      "grad_norm": 17.375,
      "learning_rate": 9.3781268492119e-05,
      "loss": 5.5687,
      "step": 5880
    },
    {
      "epoch": 0.9495405448976302,
      "grad_norm": 31.375,
      "learning_rate": 9.37705094410673e-05,
      "loss": 5.7016,
      "step": 5890
    },
    {
      "epoch": 0.95115266806384,
      "grad_norm": 9.25,
      "learning_rate": 9.375975039001561e-05,
      "loss": 4.3594,
      "step": 5900
    },
    {
      "epoch": 0.95276479123005,
      "grad_norm": 4.59375,
      "learning_rate": 9.37489913389639e-05,
      "loss": 5.5234,
      "step": 5910
    },
    {
      "epoch": 0.9543769143962598,
      "grad_norm": 5.3125,
      "learning_rate": 9.373823228791222e-05,
      "loss": 3.8586,
      "step": 5920
    },
    {
      "epoch": 0.9559890375624698,
      "grad_norm": 12.5,
      "learning_rate": 9.372747323686051e-05,
      "loss": 4.2313,
      "step": 5930
    },
    {
      "epoch": 0.9576011607286796,
      "grad_norm": 14.75,
      "learning_rate": 9.371671418580882e-05,
      "loss": 5.675,
      "step": 5940
    },
    {
      "epoch": 0.9592132838948896,
      "grad_norm": 9.1875,
      "learning_rate": 9.370595513475712e-05,
      "loss": 5.8406,
      "step": 5950
    },
    {
      "epoch": 0.9608254070610994,
      "grad_norm": 9.9375,
      "learning_rate": 9.369519608370541e-05,
      "loss": 3.4217,
      "step": 5960
    },
    {
      "epoch": 0.9624375302273094,
      "grad_norm": 4.78125,
      "learning_rate": 9.368443703265372e-05,
      "loss": 3.968,
      "step": 5970
    },
    {
      "epoch": 0.9640496533935192,
      "grad_norm": 10.1875,
      "learning_rate": 9.367367798160202e-05,
      "loss": 3.7773,
      "step": 5980
    },
    {
      "epoch": 0.9656617765597292,
      "grad_norm": 4.4375,
      "learning_rate": 9.366291893055033e-05,
      "loss": 3.6664,
      "step": 5990
    },
    {
      "epoch": 0.967273899725939,
      "grad_norm": 16.75,
      "learning_rate": 9.365215987949864e-05,
      "loss": 4.0406,
      "step": 6000
    },
    {
      "epoch": 0.968886022892149,
      "grad_norm": 6.875,
      "learning_rate": 9.364140082844694e-05,
      "loss": 4.6914,
      "step": 6010
    },
    {
      "epoch": 0.9704981460583588,
      "grad_norm": 4.0,
      "learning_rate": 9.363064177739523e-05,
      "loss": 3.8555,
      "step": 6020
    },
    {
      "epoch": 0.9721102692245688,
      "grad_norm": 7.03125,
      "learning_rate": 9.361988272634354e-05,
      "loss": 3.4656,
      "step": 6030
    },
    {
      "epoch": 0.9737223923907786,
      "grad_norm": 6.40625,
      "learning_rate": 9.360912367529184e-05,
      "loss": 4.1492,
      "step": 6040
    },
    {
      "epoch": 0.9753345155569886,
      "grad_norm": 7.53125,
      "learning_rate": 9.359836462424014e-05,
      "loss": 4.1261,
      "step": 6050
    },
    {
      "epoch": 0.9769466387231984,
      "grad_norm": 14.25,
      "learning_rate": 9.358760557318845e-05,
      "loss": 5.0539,
      "step": 6060
    },
    {
      "epoch": 0.9785587618894084,
      "grad_norm": 10.0,
      "learning_rate": 9.357684652213676e-05,
      "loss": 5.6906,
      "step": 6070
    },
    {
      "epoch": 0.9801708850556182,
      "grad_norm": 4.625,
      "learning_rate": 9.356608747108505e-05,
      "loss": 3.0898,
      "step": 6080
    },
    {
      "epoch": 0.9817830082218282,
      "grad_norm": 5.3125,
      "learning_rate": 9.355532842003336e-05,
      "loss": 3.8914,
      "step": 6090
    },
    {
      "epoch": 0.983395131388038,
      "grad_norm": 5.75,
      "learning_rate": 9.354456936898166e-05,
      "loss": 3.7047,
      "step": 6100
    },
    {
      "epoch": 0.985007254554248,
      "grad_norm": 12.8125,
      "learning_rate": 9.353381031792996e-05,
      "loss": 6.2122,
      "step": 6110
    },
    {
      "epoch": 0.9866193777204578,
      "grad_norm": 7.3125,
      "learning_rate": 9.352305126687827e-05,
      "loss": 2.9824,
      "step": 6120
    },
    {
      "epoch": 0.9882315008866678,
      "grad_norm": 7.1875,
      "learning_rate": 9.351229221582656e-05,
      "loss": 3.3781,
      "step": 6130
    },
    {
      "epoch": 0.9898436240528776,
      "grad_norm": 7.28125,
      "learning_rate": 9.350153316477487e-05,
      "loss": 4.3996,
      "step": 6140
    },
    {
      "epoch": 0.9914557472190876,
      "grad_norm": 5.625,
      "learning_rate": 9.349077411372318e-05,
      "loss": 3.6422,
      "step": 6150
    },
    {
      "epoch": 0.9930678703852974,
      "grad_norm": 7.75,
      "learning_rate": 9.348001506267148e-05,
      "loss": 4.807,
      "step": 6160
    },
    {
      "epoch": 0.9946799935515074,
      "grad_norm": 14.5,
      "learning_rate": 9.346925601161978e-05,
      "loss": 5.2203,
      "step": 6170
    },
    {
      "epoch": 0.9962921167177172,
      "grad_norm": 11.8125,
      "learning_rate": 9.345849696056809e-05,
      "loss": 4.0734,
      "step": 6180
    },
    {
      "epoch": 0.9979042398839272,
      "grad_norm": 9.5,
      "learning_rate": 9.344773790951638e-05,
      "loss": 4.4281,
      "step": 6190
    },
    {
      "epoch": 0.999516363050137,
      "grad_norm": 11.5,
      "learning_rate": 9.343697885846468e-05,
      "loss": 3.1797,
      "step": 6200
    },
    {
      "epoch": 1.0,
      "eval_loss": 4.47726583480835,
      "eval_runtime": 23.98,
      "eval_samples_per_second": 4.17,
      "eval_steps_per_second": 4.17,
      "step": 6203
    },
    {
      "epoch": 1.001128486216347,
      "grad_norm": 22.75,
      "learning_rate": 9.342621980741299e-05,
      "loss": 4.6891,
      "step": 6210
    },
    {
      "epoch": 1.0027406093825568,
      "grad_norm": 6.09375,
      "learning_rate": 9.34154607563613e-05,
      "loss": 4.0039,
      "step": 6220
    },
    {
      "epoch": 1.0043527325487667,
      "grad_norm": 15.875,
      "learning_rate": 9.34047017053096e-05,
      "loss": 7.2516,
      "step": 6230
    },
    {
      "epoch": 1.0059648557149767,
      "grad_norm": 8.625,
      "learning_rate": 9.33939426542579e-05,
      "loss": 3.0305,
      "step": 6240
    },
    {
      "epoch": 1.0075769788811866,
      "grad_norm": 13.6875,
      "learning_rate": 9.33831836032062e-05,
      "loss": 4.0719,
      "step": 6250
    },
    {
      "epoch": 1.0091891020473964,
      "grad_norm": 14.6875,
      "learning_rate": 9.33724245521545e-05,
      "loss": 4.6562,
      "step": 6260
    },
    {
      "epoch": 1.0108012252136063,
      "grad_norm": 10.5,
      "learning_rate": 9.33616655011028e-05,
      "loss": 3.5719,
      "step": 6270
    },
    {
      "epoch": 1.0124133483798161,
      "grad_norm": 16.25,
      "learning_rate": 9.33509064500511e-05,
      "loss": 3.3508,
      "step": 6280
    },
    {
      "epoch": 1.0140254715460262,
      "grad_norm": 6.8125,
      "learning_rate": 9.334014739899942e-05,
      "loss": 5.875,
      "step": 6290
    },
    {
      "epoch": 1.015637594712236,
      "grad_norm": 5.8125,
      "learning_rate": 9.332938834794771e-05,
      "loss": 3.8773,
      "step": 6300
    },
    {
      "epoch": 1.0172497178784459,
      "grad_norm": 20.875,
      "learning_rate": 9.331862929689602e-05,
      "loss": 4.8656,
      "step": 6310
    },
    {
      "epoch": 1.0188618410446557,
      "grad_norm": 19.875,
      "learning_rate": 9.330787024584432e-05,
      "loss": 4.4705,
      "step": 6320
    },
    {
      "epoch": 1.0204739642108658,
      "grad_norm": 9.0,
      "learning_rate": 9.329711119479262e-05,
      "loss": 4.9938,
      "step": 6330
    },
    {
      "epoch": 1.0220860873770756,
      "grad_norm": 17.25,
      "learning_rate": 9.328635214374093e-05,
      "loss": 4.4008,
      "step": 6340
    },
    {
      "epoch": 1.0236982105432855,
      "grad_norm": 7.21875,
      "learning_rate": 9.327559309268922e-05,
      "loss": 6.1143,
      "step": 6350
    },
    {
      "epoch": 1.0253103337094953,
      "grad_norm": 24.625,
      "learning_rate": 9.326483404163753e-05,
      "loss": 5.5656,
      "step": 6360
    },
    {
      "epoch": 1.0269224568757054,
      "grad_norm": 6.96875,
      "learning_rate": 9.325407499058584e-05,
      "loss": 4.8125,
      "step": 6370
    },
    {
      "epoch": 1.0285345800419152,
      "grad_norm": 5.1875,
      "learning_rate": 9.324331593953414e-05,
      "loss": 3.8805,
      "step": 6380
    },
    {
      "epoch": 1.030146703208125,
      "grad_norm": 17.875,
      "learning_rate": 9.323255688848244e-05,
      "loss": 4.0023,
      "step": 6390
    },
    {
      "epoch": 1.031758826374335,
      "grad_norm": 6.5625,
      "learning_rate": 9.322179783743075e-05,
      "loss": 4.5844,
      "step": 6400
    },
    {
      "epoch": 1.033370949540545,
      "grad_norm": 17.75,
      "learning_rate": 9.321103878637904e-05,
      "loss": 4.5984,
      "step": 6410
    },
    {
      "epoch": 1.0349830727067548,
      "grad_norm": 11.875,
      "learning_rate": 9.320027973532734e-05,
      "loss": 3.0039,
      "step": 6420
    },
    {
      "epoch": 1.0365951958729647,
      "grad_norm": 17.25,
      "learning_rate": 9.318952068427565e-05,
      "loss": 3.7013,
      "step": 6430
    },
    {
      "epoch": 1.0382073190391745,
      "grad_norm": 6.875,
      "learning_rate": 9.317876163322396e-05,
      "loss": 4.1492,
      "step": 6440
    },
    {
      "epoch": 1.0398194422053846,
      "grad_norm": 9.125,
      "learning_rate": 9.316800258217226e-05,
      "loss": 4.8102,
      "step": 6450
    },
    {
      "epoch": 1.0414315653715944,
      "grad_norm": 0.0,
      "learning_rate": 9.315724353112057e-05,
      "loss": 4.4284,
      "step": 6460
    },
    {
      "epoch": 1.0430436885378043,
      "grad_norm": 8.8125,
      "learning_rate": 9.314648448006886e-05,
      "loss": 5.19,
      "step": 6470
    },
    {
      "epoch": 1.0446558117040141,
      "grad_norm": 0.0,
      "learning_rate": 9.313572542901716e-05,
      "loss": 4.3751,
      "step": 6480
    },
    {
      "epoch": 1.0462679348702242,
      "grad_norm": 18.0,
      "learning_rate": 9.312496637796547e-05,
      "loss": 3.1977,
      "step": 6490
    },
    {
      "epoch": 1.047880058036434,
      "grad_norm": 12.75,
      "learning_rate": 9.311420732691378e-05,
      "loss": 4.0375,
      "step": 6500
    },
    {
      "epoch": 1.0494921812026439,
      "grad_norm": 4.90625,
      "learning_rate": 9.310344827586207e-05,
      "loss": 3.25,
      "step": 6510
    },
    {
      "epoch": 1.0511043043688537,
      "grad_norm": 14.125,
      "learning_rate": 9.309268922481038e-05,
      "loss": 3.2656,
      "step": 6520
    },
    {
      "epoch": 1.0527164275350638,
      "grad_norm": 8.1875,
      "learning_rate": 9.308193017375868e-05,
      "loss": 4.8492,
      "step": 6530
    },
    {
      "epoch": 1.0543285507012736,
      "grad_norm": 28.5,
      "learning_rate": 9.307117112270698e-05,
      "loss": 4.0922,
      "step": 6540
    },
    {
      "epoch": 1.0559406738674835,
      "grad_norm": 12.8125,
      "learning_rate": 9.306041207165529e-05,
      "loss": 5.4734,
      "step": 6550
    },
    {
      "epoch": 1.0575527970336933,
      "grad_norm": 5.0625,
      "learning_rate": 9.304965302060358e-05,
      "loss": 3.7008,
      "step": 6560
    },
    {
      "epoch": 1.0591649201999034,
      "grad_norm": 5.0,
      "learning_rate": 9.30388939695519e-05,
      "loss": 2.9422,
      "step": 6570
    },
    {
      "epoch": 1.0607770433661132,
      "grad_norm": 7.375,
      "learning_rate": 9.30281349185002e-05,
      "loss": 5.72,
      "step": 6580
    },
    {
      "epoch": 1.062389166532323,
      "grad_norm": 13.1875,
      "learning_rate": 9.30173758674485e-05,
      "loss": 5.7029,
      "step": 6590
    },
    {
      "epoch": 1.064001289698533,
      "grad_norm": 16.5,
      "learning_rate": 9.30066168163968e-05,
      "loss": 4.15,
      "step": 6600
    },
    {
      "epoch": 1.0656134128647428,
      "grad_norm": 8.375,
      "learning_rate": 9.29958577653451e-05,
      "loss": 4.2524,
      "step": 6610
    },
    {
      "epoch": 1.0672255360309528,
      "grad_norm": 6.8125,
      "learning_rate": 9.29850987142934e-05,
      "loss": 3.2695,
      "step": 6620
    },
    {
      "epoch": 1.0688376591971627,
      "grad_norm": 8.5,
      "learning_rate": 9.29743396632417e-05,
      "loss": 3.7961,
      "step": 6630
    },
    {
      "epoch": 1.0704497823633725,
      "grad_norm": 7.84375,
      "learning_rate": 9.296358061219001e-05,
      "loss": 3.4617,
      "step": 6640
    },
    {
      "epoch": 1.0720619055295824,
      "grad_norm": 7.625,
      "learning_rate": 9.295282156113832e-05,
      "loss": 2.9977,
      "step": 6650
    },
    {
      "epoch": 1.0736740286957924,
      "grad_norm": 5.5,
      "learning_rate": 9.294206251008662e-05,
      "loss": 3.5514,
      "step": 6660
    },
    {
      "epoch": 1.0752861518620023,
      "grad_norm": 17.75,
      "learning_rate": 9.293130345903491e-05,
      "loss": 5.0578,
      "step": 6670
    },
    {
      "epoch": 1.0768982750282121,
      "grad_norm": 7.03125,
      "learning_rate": 9.292054440798322e-05,
      "loss": 2.934,
      "step": 6680
    },
    {
      "epoch": 1.078510398194422,
      "grad_norm": 7.0625,
      "learning_rate": 9.290978535693152e-05,
      "loss": 4.0391,
      "step": 6690
    },
    {
      "epoch": 1.080122521360632,
      "grad_norm": 8.0625,
      "learning_rate": 9.289902630587982e-05,
      "loss": 3.8609,
      "step": 6700
    },
    {
      "epoch": 1.0817346445268419,
      "grad_norm": 7.96875,
      "learning_rate": 9.288826725482813e-05,
      "loss": 3.553,
      "step": 6710
    },
    {
      "epoch": 1.0833467676930517,
      "grad_norm": 12.8125,
      "learning_rate": 9.287750820377644e-05,
      "loss": 4.9648,
      "step": 6720
    },
    {
      "epoch": 1.0849588908592616,
      "grad_norm": 10.4375,
      "learning_rate": 9.286674915272473e-05,
      "loss": 3.7484,
      "step": 6730
    },
    {
      "epoch": 1.0865710140254716,
      "grad_norm": 21.375,
      "learning_rate": 9.285599010167304e-05,
      "loss": 6.425,
      "step": 6740
    },
    {
      "epoch": 1.0881831371916815,
      "grad_norm": 9.9375,
      "learning_rate": 9.284523105062134e-05,
      "loss": 3.9016,
      "step": 6750
    },
    {
      "epoch": 1.0897952603578913,
      "grad_norm": 13.5,
      "learning_rate": 9.283447199956964e-05,
      "loss": 3.7563,
      "step": 6760
    },
    {
      "epoch": 1.0914073835241012,
      "grad_norm": 45.0,
      "learning_rate": 9.282371294851795e-05,
      "loss": 3.8258,
      "step": 6770
    },
    {
      "epoch": 1.0930195066903112,
      "grad_norm": 38.75,
      "learning_rate": 9.281295389746624e-05,
      "loss": 5.2227,
      "step": 6780
    },
    {
      "epoch": 1.094631629856521,
      "grad_norm": 31.125,
      "learning_rate": 9.280219484641455e-05,
      "loss": 5.8656,
      "step": 6790
    },
    {
      "epoch": 1.096243753022731,
      "grad_norm": 12.5,
      "learning_rate": 9.279143579536286e-05,
      "loss": 4.1617,
      "step": 6800
    },
    {
      "epoch": 1.0978558761889408,
      "grad_norm": 12.5,
      "learning_rate": 9.278067674431116e-05,
      "loss": 3.8953,
      "step": 6810
    },
    {
      "epoch": 1.0994679993551508,
      "grad_norm": 4.09375,
      "learning_rate": 9.276991769325946e-05,
      "loss": 5.1063,
      "step": 6820
    },
    {
      "epoch": 1.1010801225213607,
      "grad_norm": 9.5,
      "learning_rate": 9.275915864220777e-05,
      "loss": 5.9016,
      "step": 6830
    },
    {
      "epoch": 1.1026922456875705,
      "grad_norm": 6.625,
      "learning_rate": 9.274839959115606e-05,
      "loss": 4.0734,
      "step": 6840
    },
    {
      "epoch": 1.1043043688537804,
      "grad_norm": 22.5,
      "learning_rate": 9.273764054010436e-05,
      "loss": 5.5297,
      "step": 6850
    },
    {
      "epoch": 1.1059164920199904,
      "grad_norm": 5.65625,
      "learning_rate": 9.272688148905267e-05,
      "loss": 5.8553,
      "step": 6860
    },
    {
      "epoch": 1.1075286151862003,
      "grad_norm": 8.5625,
      "learning_rate": 9.271612243800098e-05,
      "loss": 6.1656,
      "step": 6870
    },
    {
      "epoch": 1.1091407383524101,
      "grad_norm": 26.5,
      "learning_rate": 9.270536338694928e-05,
      "loss": 6.6172,
      "step": 6880
    },
    {
      "epoch": 1.11075286151862,
      "grad_norm": 15.0625,
      "learning_rate": 9.269460433589759e-05,
      "loss": 4.5281,
      "step": 6890
    },
    {
      "epoch": 1.11236498468483,
      "grad_norm": 34.5,
      "learning_rate": 9.268384528484588e-05,
      "loss": 3.8094,
      "step": 6900
    },
    {
      "epoch": 1.1139771078510399,
      "grad_norm": 7.625,
      "learning_rate": 9.267308623379418e-05,
      "loss": 4.9469,
      "step": 6910
    },
    {
      "epoch": 1.1155892310172497,
      "grad_norm": 18.0,
      "learning_rate": 9.266232718274248e-05,
      "loss": 5.0384,
      "step": 6920
    },
    {
      "epoch": 1.1172013541834596,
      "grad_norm": 4.96875,
      "learning_rate": 9.265156813169079e-05,
      "loss": 3.318,
      "step": 6930
    },
    {
      "epoch": 1.1188134773496694,
      "grad_norm": 6.6875,
      "learning_rate": 9.26408090806391e-05,
      "loss": 4.4961,
      "step": 6940
    },
    {
      "epoch": 1.1204256005158795,
      "grad_norm": 6.25,
      "learning_rate": 9.263005002958739e-05,
      "loss": 4.5867,
      "step": 6950
    },
    {
      "epoch": 1.1220377236820893,
      "grad_norm": 7.03125,
      "learning_rate": 9.26192909785357e-05,
      "loss": 4.2062,
      "step": 6960
    },
    {
      "epoch": 1.1236498468482992,
      "grad_norm": 25.625,
      "learning_rate": 9.2608531927484e-05,
      "loss": 4.8859,
      "step": 6970
    },
    {
      "epoch": 1.1252619700145092,
      "grad_norm": 7.3125,
      "learning_rate": 9.25977728764323e-05,
      "loss": 5.1992,
      "step": 6980
    },
    {
      "epoch": 1.126874093180719,
      "grad_norm": 20.125,
      "learning_rate": 9.25870138253806e-05,
      "loss": 3.8891,
      "step": 6990
    },
    {
      "epoch": 1.128486216346929,
      "grad_norm": 40.75,
      "learning_rate": 9.25762547743289e-05,
      "loss": 4.2211,
      "step": 7000
    },
    {
      "epoch": 1.1300983395131388,
      "grad_norm": 7.3125,
      "learning_rate": 9.256549572327721e-05,
      "loss": 4.4,
      "step": 7010
    },
    {
      "epoch": 1.1317104626793486,
      "grad_norm": 6.9375,
      "learning_rate": 9.255473667222552e-05,
      "loss": 4.3438,
      "step": 7020
    },
    {
      "epoch": 1.1333225858455587,
      "grad_norm": 4.9375,
      "learning_rate": 9.254397762117382e-05,
      "loss": 3.5875,
      "step": 7030
    },
    {
      "epoch": 1.1349347090117685,
      "grad_norm": 17.625,
      "learning_rate": 9.253321857012211e-05,
      "loss": 5.2361,
      "step": 7040
    },
    {
      "epoch": 1.1365468321779784,
      "grad_norm": 9.3125,
      "learning_rate": 9.252245951907042e-05,
      "loss": 4.3984,
      "step": 7050
    },
    {
      "epoch": 1.1381589553441882,
      "grad_norm": 14.0,
      "learning_rate": 9.251170046801872e-05,
      "loss": 4.9406,
      "step": 7060
    },
    {
      "epoch": 1.1397710785103983,
      "grad_norm": 9.75,
      "learning_rate": 9.250094141696702e-05,
      "loss": 4.4773,
      "step": 7070
    },
    {
      "epoch": 1.1413832016766081,
      "grad_norm": 13.0,
      "learning_rate": 9.249018236591533e-05,
      "loss": 4.75,
      "step": 7080
    },
    {
      "epoch": 1.142995324842818,
      "grad_norm": 7.40625,
      "learning_rate": 9.247942331486364e-05,
      "loss": 3.7039,
      "step": 7090
    },
    {
      "epoch": 1.1446074480090278,
      "grad_norm": 25.625,
      "learning_rate": 9.246866426381193e-05,
      "loss": 3.6391,
      "step": 7100
    },
    {
      "epoch": 1.1462195711752379,
      "grad_norm": 11.5625,
      "learning_rate": 9.245790521276024e-05,
      "loss": 3.5641,
      "step": 7110
    },
    {
      "epoch": 1.1478316943414477,
      "grad_norm": 7.5,
      "learning_rate": 9.244714616170854e-05,
      "loss": 4.4251,
      "step": 7120
    },
    {
      "epoch": 1.1494438175076576,
      "grad_norm": 22.125,
      "learning_rate": 9.243638711065684e-05,
      "loss": 5.6375,
      "step": 7130
    },
    {
      "epoch": 1.1510559406738674,
      "grad_norm": 28.25,
      "learning_rate": 9.242562805960515e-05,
      "loss": 3.482,
      "step": 7140
    },
    {
      "epoch": 1.1526680638400775,
      "grad_norm": 7.28125,
      "learning_rate": 9.241486900855344e-05,
      "loss": 4.5078,
      "step": 7150
    },
    {
      "epoch": 1.1542801870062873,
      "grad_norm": 25.375,
      "learning_rate": 9.240410995750175e-05,
      "loss": 5.782,
      "step": 7160
    },
    {
      "epoch": 1.1558923101724972,
      "grad_norm": 13.625,
      "learning_rate": 9.239335090645006e-05,
      "loss": 3.8883,
      "step": 7170
    },
    {
      "epoch": 1.157504433338707,
      "grad_norm": 22.875,
      "learning_rate": 9.238259185539836e-05,
      "loss": 2.7648,
      "step": 7180
    },
    {
      "epoch": 1.159116556504917,
      "grad_norm": 9.625,
      "learning_rate": 9.237183280434666e-05,
      "loss": 4.6375,
      "step": 7190
    },
    {
      "epoch": 1.160728679671127,
      "grad_norm": 15.1875,
      "learning_rate": 9.236107375329497e-05,
      "loss": 6.2477,
      "step": 7200
    },
    {
      "epoch": 1.1623408028373368,
      "grad_norm": 8.875,
      "learning_rate": 9.235031470224326e-05,
      "loss": 3.0305,
      "step": 7210
    },
    {
      "epoch": 1.1639529260035466,
      "grad_norm": 8.8125,
      "learning_rate": 9.233955565119157e-05,
      "loss": 4.2109,
      "step": 7220
    },
    {
      "epoch": 1.1655650491697567,
      "grad_norm": 52.0,
      "learning_rate": 9.232879660013988e-05,
      "loss": 3.8359,
      "step": 7230
    },
    {
      "epoch": 1.1671771723359665,
      "grad_norm": 13.4375,
      "learning_rate": 9.231803754908818e-05,
      "loss": 4.3758,
      "step": 7240
    },
    {
      "epoch": 1.1687892955021764,
      "grad_norm": 15.75,
      "learning_rate": 9.230727849803648e-05,
      "loss": 4.5438,
      "step": 7250
    },
    {
      "epoch": 1.1704014186683862,
      "grad_norm": 10.9375,
      "learning_rate": 9.229651944698477e-05,
      "loss": 4.5578,
      "step": 7260
    },
    {
      "epoch": 1.172013541834596,
      "grad_norm": 17.875,
      "learning_rate": 9.228576039593308e-05,
      "loss": 5.5469,
      "step": 7270
    },
    {
      "epoch": 1.1736256650008061,
      "grad_norm": 6.9375,
      "learning_rate": 9.227500134488138e-05,
      "loss": 5.1141,
      "step": 7280
    },
    {
      "epoch": 1.175237788167016,
      "grad_norm": 23.125,
      "learning_rate": 9.226424229382969e-05,
      "loss": 3.3461,
      "step": 7290
    },
    {
      "epoch": 1.1768499113332258,
      "grad_norm": 36.5,
      "learning_rate": 9.2253483242778e-05,
      "loss": 4.4568,
      "step": 7300
    },
    {
      "epoch": 1.1784620344994359,
      "grad_norm": 7.375,
      "learning_rate": 9.22427241917263e-05,
      "loss": 5.3,
      "step": 7310
    },
    {
      "epoch": 1.1800741576656457,
      "grad_norm": 6.0625,
      "learning_rate": 9.223196514067459e-05,
      "loss": 5.1781,
      "step": 7320
    },
    {
      "epoch": 1.1816862808318556,
      "grad_norm": 5.90625,
      "learning_rate": 9.22212060896229e-05,
      "loss": 4.182,
      "step": 7330
    },
    {
      "epoch": 1.1832984039980654,
      "grad_norm": 14.375,
      "learning_rate": 9.22104470385712e-05,
      "loss": 4.4367,
      "step": 7340
    },
    {
      "epoch": 1.1849105271642753,
      "grad_norm": 11.1875,
      "learning_rate": 9.21996879875195e-05,
      "loss": 3.6164,
      "step": 7350
    },
    {
      "epoch": 1.1865226503304853,
      "grad_norm": 26.75,
      "learning_rate": 9.21889289364678e-05,
      "loss": 6.0184,
      "step": 7360
    },
    {
      "epoch": 1.1881347734966952,
      "grad_norm": 18.125,
      "learning_rate": 9.217816988541612e-05,
      "loss": 4.0896,
      "step": 7370
    },
    {
      "epoch": 1.189746896662905,
      "grad_norm": 14.625,
      "learning_rate": 9.216741083436441e-05,
      "loss": 4.3586,
      "step": 7380
    },
    {
      "epoch": 1.1913590198291149,
      "grad_norm": 10.9375,
      "learning_rate": 9.215665178331272e-05,
      "loss": 2.8451,
      "step": 7390
    },
    {
      "epoch": 1.192971142995325,
      "grad_norm": 23.75,
      "learning_rate": 9.214589273226102e-05,
      "loss": 6.632,
      "step": 7400
    },
    {
      "epoch": 1.1945832661615348,
      "grad_norm": 4.46875,
      "learning_rate": 9.213513368120932e-05,
      "loss": 4.9437,
      "step": 7410
    },
    {
      "epoch": 1.1961953893277446,
      "grad_norm": 10.375,
      "learning_rate": 9.212437463015763e-05,
      "loss": 4.8305,
      "step": 7420
    },
    {
      "epoch": 1.1978075124939545,
      "grad_norm": 0.0,
      "learning_rate": 9.211361557910592e-05,
      "loss": 3.1548,
      "step": 7430
    },
    {
      "epoch": 1.1994196356601645,
      "grad_norm": 33.5,
      "learning_rate": 9.210285652805423e-05,
      "loss": 5.3797,
      "step": 7440
    },
    {
      "epoch": 1.2010317588263744,
      "grad_norm": 10.875,
      "learning_rate": 9.209209747700254e-05,
      "loss": 3.0852,
      "step": 7450
    },
    {
      "epoch": 1.2026438819925842,
      "grad_norm": 18.125,
      "learning_rate": 9.208133842595084e-05,
      "loss": 5.068,
      "step": 7460
    },
    {
      "epoch": 1.204256005158794,
      "grad_norm": 11.5625,
      "learning_rate": 9.207057937489914e-05,
      "loss": 3.475,
      "step": 7470
    },
    {
      "epoch": 1.2058681283250041,
      "grad_norm": 5.78125,
      "learning_rate": 9.205982032384745e-05,
      "loss": 4.2984,
      "step": 7480
    },
    {
      "epoch": 1.207480251491214,
      "grad_norm": 15.125,
      "learning_rate": 9.204906127279574e-05,
      "loss": 2.5133,
      "step": 7490
    },
    {
      "epoch": 1.2090923746574238,
      "grad_norm": 12.0625,
      "learning_rate": 9.203830222174404e-05,
      "loss": 3.1906,
      "step": 7500
    },
    {
      "epoch": 1.2107044978236337,
      "grad_norm": 18.75,
      "learning_rate": 9.202754317069235e-05,
      "loss": 5.037,
      "step": 7510
    },
    {
      "epoch": 1.2123166209898437,
      "grad_norm": 7.40625,
      "learning_rate": 9.201678411964066e-05,
      "loss": 4.2953,
      "step": 7520
    },
    {
      "epoch": 1.2139287441560536,
      "grad_norm": 17.625,
      "learning_rate": 9.200602506858896e-05,
      "loss": 4.7438,
      "step": 7530
    },
    {
      "epoch": 1.2155408673222634,
      "grad_norm": 32.0,
      "learning_rate": 9.199526601753727e-05,
      "loss": 4.8172,
      "step": 7540
    },
    {
      "epoch": 1.2171529904884733,
      "grad_norm": 7.71875,
      "learning_rate": 9.198450696648556e-05,
      "loss": 3.1484,
      "step": 7550
    },
    {
      "epoch": 1.2187651136546833,
      "grad_norm": 32.0,
      "learning_rate": 9.197374791543386e-05,
      "loss": 4.3016,
      "step": 7560
    },
    {
      "epoch": 1.2203772368208932,
      "grad_norm": 0.0,
      "learning_rate": 9.196298886438217e-05,
      "loss": 4.883,
      "step": 7570
    },
    {
      "epoch": 1.221989359987103,
      "grad_norm": 5.25,
      "learning_rate": 9.195222981333046e-05,
      "loss": 6.775,
      "step": 7580
    },
    {
      "epoch": 1.2236014831533129,
      "grad_norm": 15.25,
      "learning_rate": 9.194147076227877e-05,
      "loss": 3.5633,
      "step": 7590
    },
    {
      "epoch": 1.2252136063195227,
      "grad_norm": 9.1875,
      "learning_rate": 9.193071171122707e-05,
      "loss": 5.3039,
      "step": 7600
    },
    {
      "epoch": 1.2268257294857328,
      "grad_norm": 25.0,
      "learning_rate": 9.191995266017538e-05,
      "loss": 3.5031,
      "step": 7610
    },
    {
      "epoch": 1.2284378526519426,
      "grad_norm": 25.5,
      "learning_rate": 9.190919360912368e-05,
      "loss": 4.707,
      "step": 7620
    },
    {
      "epoch": 1.2300499758181525,
      "grad_norm": 7.5,
      "learning_rate": 9.189843455807197e-05,
      "loss": 5.4135,
      "step": 7630
    },
    {
      "epoch": 1.2316620989843625,
      "grad_norm": 33.5,
      "learning_rate": 9.188767550702028e-05,
      "loss": 3.9539,
      "step": 7640
    },
    {
      "epoch": 1.2332742221505724,
      "grad_norm": 7.375,
      "learning_rate": 9.187691645596858e-05,
      "loss": 5.7906,
      "step": 7650
    },
    {
      "epoch": 1.2348863453167822,
      "grad_norm": 13.25,
      "learning_rate": 9.186615740491689e-05,
      "loss": 5.143,
      "step": 7660
    },
    {
      "epoch": 1.236498468482992,
      "grad_norm": 14.625,
      "learning_rate": 9.18553983538652e-05,
      "loss": 4.1516,
      "step": 7670
    },
    {
      "epoch": 1.238110591649202,
      "grad_norm": 0.0,
      "learning_rate": 9.18446393028135e-05,
      "loss": 4.907,
      "step": 7680
    },
    {
      "epoch": 1.239722714815412,
      "grad_norm": 6.84375,
      "learning_rate": 9.18338802517618e-05,
      "loss": 3.7516,
      "step": 7690
    },
    {
      "epoch": 1.2413348379816218,
      "grad_norm": 23.25,
      "learning_rate": 9.18231212007101e-05,
      "loss": 3.8281,
      "step": 7700
    },
    {
      "epoch": 1.2429469611478317,
      "grad_norm": 7.5,
      "learning_rate": 9.18123621496584e-05,
      "loss": 3.8602,
      "step": 7710
    },
    {
      "epoch": 1.2445590843140415,
      "grad_norm": 4.75,
      "learning_rate": 9.18016030986067e-05,
      "loss": 5.4594,
      "step": 7720
    },
    {
      "epoch": 1.2461712074802516,
      "grad_norm": 14.125,
      "learning_rate": 9.179084404755501e-05,
      "loss": 4.3508,
      "step": 7730
    },
    {
      "epoch": 1.2477833306464614,
      "grad_norm": 34.0,
      "learning_rate": 9.178008499650332e-05,
      "loss": 3.175,
      "step": 7740
    },
    {
      "epoch": 1.2493954538126713,
      "grad_norm": 5.40625,
      "learning_rate": 9.176932594545161e-05,
      "loss": 5.3281,
      "step": 7750
    },
    {
      "epoch": 1.251007576978881,
      "grad_norm": 5.59375,
      "learning_rate": 9.175856689439992e-05,
      "loss": 3.3117,
      "step": 7760
    },
    {
      "epoch": 1.2526197001450912,
      "grad_norm": 6.21875,
      "learning_rate": 9.174780784334822e-05,
      "loss": 5.8557,
      "step": 7770
    },
    {
      "epoch": 1.254231823311301,
      "grad_norm": 8.875,
      "learning_rate": 9.173704879229652e-05,
      "loss": 5.2414,
      "step": 7780
    },
    {
      "epoch": 1.2558439464775109,
      "grad_norm": 5.0,
      "learning_rate": 9.172628974124483e-05,
      "loss": 4.6477,
      "step": 7790
    },
    {
      "epoch": 1.2574560696437207,
      "grad_norm": 9.8125,
      "learning_rate": 9.171553069019312e-05,
      "loss": 3.9148,
      "step": 7800
    },
    {
      "epoch": 1.2590681928099308,
      "grad_norm": 11.3125,
      "learning_rate": 9.170477163914143e-05,
      "loss": 3.7781,
      "step": 7810
    },
    {
      "epoch": 1.2606803159761406,
      "grad_norm": 4.625,
      "learning_rate": 9.169401258808974e-05,
      "loss": 3.2654,
      "step": 7820
    },
    {
      "epoch": 1.2622924391423505,
      "grad_norm": 4.84375,
      "learning_rate": 9.168325353703804e-05,
      "loss": 5.4922,
      "step": 7830
    },
    {
      "epoch": 1.2639045623085603,
      "grad_norm": 27.125,
      "learning_rate": 9.167249448598634e-05,
      "loss": 2.7021,
      "step": 7840
    },
    {
      "epoch": 1.2655166854747701,
      "grad_norm": 23.875,
      "learning_rate": 9.166173543493465e-05,
      "loss": 5.1172,
      "step": 7850
    },
    {
      "epoch": 1.2671288086409802,
      "grad_norm": 9.875,
      "learning_rate": 9.165097638388294e-05,
      "loss": 3.6998,
      "step": 7860
    },
    {
      "epoch": 1.26874093180719,
      "grad_norm": 5.0,
      "learning_rate": 9.164021733283125e-05,
      "loss": 5.1508,
      "step": 7870
    },
    {
      "epoch": 1.2703530549734,
      "grad_norm": 6.03125,
      "learning_rate": 9.162945828177956e-05,
      "loss": 4.0526,
      "step": 7880
    },
    {
      "epoch": 1.27196517813961,
      "grad_norm": 19.875,
      "learning_rate": 9.161869923072786e-05,
      "loss": 4.3953,
      "step": 7890
    },
    {
      "epoch": 1.2735773013058198,
      "grad_norm": 14.125,
      "learning_rate": 9.160794017967616e-05,
      "loss": 5.4445,
      "step": 7900
    },
    {
      "epoch": 1.2751894244720297,
      "grad_norm": 28.625,
      "learning_rate": 9.159718112862447e-05,
      "loss": 3.9375,
      "step": 7910
    },
    {
      "epoch": 1.2768015476382395,
      "grad_norm": 19.875,
      "learning_rate": 9.158642207757276e-05,
      "loss": 5.7414,
      "step": 7920
    },
    {
      "epoch": 1.2784136708044493,
      "grad_norm": 38.25,
      "learning_rate": 9.157566302652106e-05,
      "loss": 4.2016,
      "step": 7930
    },
    {
      "epoch": 1.2800257939706594,
      "grad_norm": 14.8125,
      "learning_rate": 9.156490397546937e-05,
      "loss": 6.1539,
      "step": 7940
    },
    {
      "epoch": 1.2816379171368693,
      "grad_norm": 7.625,
      "learning_rate": 9.155414492441768e-05,
      "loss": 3.3094,
      "step": 7950
    },
    {
      "epoch": 1.283250040303079,
      "grad_norm": 17.625,
      "learning_rate": 9.154338587336598e-05,
      "loss": 5.0109,
      "step": 7960
    },
    {
      "epoch": 1.2848621634692892,
      "grad_norm": 6.21875,
      "learning_rate": 9.153262682231427e-05,
      "loss": 4.5199,
      "step": 7970
    },
    {
      "epoch": 1.286474286635499,
      "grad_norm": 24.5,
      "learning_rate": 9.152186777126258e-05,
      "loss": 3.3453,
      "step": 7980
    },
    {
      "epoch": 1.2880864098017089,
      "grad_norm": 24.125,
      "learning_rate": 9.151110872021088e-05,
      "loss": 5.1141,
      "step": 7990
    },
    {
      "epoch": 1.2896985329679187,
      "grad_norm": 24.375,
      "learning_rate": 9.150034966915918e-05,
      "loss": 4.6555,
      "step": 8000
    },
    {
      "epoch": 1.2913106561341285,
      "grad_norm": 7.4375,
      "learning_rate": 9.148959061810749e-05,
      "loss": 3.7148,
      "step": 8010
    },
    {
      "epoch": 1.2929227793003386,
      "grad_norm": 27.875,
      "learning_rate": 9.14788315670558e-05,
      "loss": 4.0758,
      "step": 8020
    },
    {
      "epoch": 1.2945349024665485,
      "grad_norm": 25.625,
      "learning_rate": 9.146807251600409e-05,
      "loss": 5.1578,
      "step": 8030
    },
    {
      "epoch": 1.2961470256327583,
      "grad_norm": 24.875,
      "learning_rate": 9.14573134649524e-05,
      "loss": 4.0367,
      "step": 8040
    },
    {
      "epoch": 1.2977591487989684,
      "grad_norm": 14.5,
      "learning_rate": 9.14465544139007e-05,
      "loss": 3.4086,
      "step": 8050
    },
    {
      "epoch": 1.2993712719651782,
      "grad_norm": 11.0625,
      "learning_rate": 9.1435795362849e-05,
      "loss": 6.0281,
      "step": 8060
    },
    {
      "epoch": 1.300983395131388,
      "grad_norm": 8.5625,
      "learning_rate": 9.14250363117973e-05,
      "loss": 6.2367,
      "step": 8070
    },
    {
      "epoch": 1.302595518297598,
      "grad_norm": 24.5,
      "learning_rate": 9.14142772607456e-05,
      "loss": 5.2953,
      "step": 8080
    },
    {
      "epoch": 1.3042076414638077,
      "grad_norm": 9.5625,
      "learning_rate": 9.140351820969391e-05,
      "loss": 5.703,
      "step": 8090
    },
    {
      "epoch": 1.3058197646300178,
      "grad_norm": 15.3125,
      "learning_rate": 9.139275915864222e-05,
      "loss": 4.068,
      "step": 8100
    },
    {
      "epoch": 1.3074318877962277,
      "grad_norm": 16.25,
      "learning_rate": 9.138200010759052e-05,
      "loss": 5.5328,
      "step": 8110
    },
    {
      "epoch": 1.3090440109624375,
      "grad_norm": 9.125,
      "learning_rate": 9.137124105653881e-05,
      "loss": 4.5219,
      "step": 8120
    },
    {
      "epoch": 1.3106561341286476,
      "grad_norm": 26.0,
      "learning_rate": 9.136048200548712e-05,
      "loss": 4.6641,
      "step": 8130
    },
    {
      "epoch": 1.3122682572948574,
      "grad_norm": 15.0,
      "learning_rate": 9.134972295443542e-05,
      "loss": 3.1273,
      "step": 8140
    },
    {
      "epoch": 1.3138803804610673,
      "grad_norm": 11.8125,
      "learning_rate": 9.133896390338372e-05,
      "loss": 3.3703,
      "step": 8150
    },
    {
      "epoch": 1.315492503627277,
      "grad_norm": 5.5625,
      "learning_rate": 9.132820485233203e-05,
      "loss": 3.4312,
      "step": 8160
    },
    {
      "epoch": 1.317104626793487,
      "grad_norm": 17.375,
      "learning_rate": 9.131744580128034e-05,
      "loss": 5.0078,
      "step": 8170
    },
    {
      "epoch": 1.3187167499596968,
      "grad_norm": 7.875,
      "learning_rate": 9.130668675022863e-05,
      "loss": 5.2,
      "step": 8180
    },
    {
      "epoch": 1.3203288731259069,
      "grad_norm": 14.5625,
      "learning_rate": 9.129592769917694e-05,
      "loss": 4.1555,
      "step": 8190
    },
    {
      "epoch": 1.3219409962921167,
      "grad_norm": 10.1875,
      "learning_rate": 9.128516864812524e-05,
      "loss": 5.7438,
      "step": 8200
    },
    {
      "epoch": 1.3235531194583265,
      "grad_norm": 28.75,
      "learning_rate": 9.127440959707354e-05,
      "loss": 6.2062,
      "step": 8210
    },
    {
      "epoch": 1.3251652426245366,
      "grad_norm": 5.53125,
      "learning_rate": 9.126365054602185e-05,
      "loss": 4.5445,
      "step": 8220
    },
    {
      "epoch": 1.3267773657907465,
      "grad_norm": 12.8125,
      "learning_rate": 9.125289149497014e-05,
      "loss": 2.9605,
      "step": 8230
    },
    {
      "epoch": 1.3283894889569563,
      "grad_norm": 5.5,
      "learning_rate": 9.124213244391845e-05,
      "loss": 3.6781,
      "step": 8240
    },
    {
      "epoch": 1.3300016121231661,
      "grad_norm": 38.75,
      "learning_rate": 9.123137339286675e-05,
      "loss": 4.8617,
      "step": 8250
    },
    {
      "epoch": 1.331613735289376,
      "grad_norm": 11.4375,
      "learning_rate": 9.122061434181506e-05,
      "loss": 4.4641,
      "step": 8260
    },
    {
      "epoch": 1.333225858455586,
      "grad_norm": 22.0,
      "learning_rate": 9.120985529076336e-05,
      "loss": 3.5797,
      "step": 8270
    },
    {
      "epoch": 1.334837981621796,
      "grad_norm": 7.9375,
      "learning_rate": 9.119909623971165e-05,
      "loss": 4.2574,
      "step": 8280
    },
    {
      "epoch": 1.3364501047880057,
      "grad_norm": 5.46875,
      "learning_rate": 9.118833718865996e-05,
      "loss": 3.743,
      "step": 8290
    },
    {
      "epoch": 1.3380622279542158,
      "grad_norm": 19.125,
      "learning_rate": 9.117757813760826e-05,
      "loss": 3.8922,
      "step": 8300
    },
    {
      "epoch": 1.3396743511204257,
      "grad_norm": 7.59375,
      "learning_rate": 9.116681908655657e-05,
      "loss": 3.9469,
      "step": 8310
    },
    {
      "epoch": 1.3412864742866355,
      "grad_norm": 9.1875,
      "learning_rate": 9.115606003550488e-05,
      "loss": 4.3266,
      "step": 8320
    },
    {
      "epoch": 1.3428985974528453,
      "grad_norm": 9.125,
      "learning_rate": 9.114530098445318e-05,
      "loss": 3.8105,
      "step": 8330
    },
    {
      "epoch": 1.3445107206190552,
      "grad_norm": 20.875,
      "learning_rate": 9.113454193340147e-05,
      "loss": 4.0398,
      "step": 8340
    },
    {
      "epoch": 1.3461228437852653,
      "grad_norm": 7.6875,
      "learning_rate": 9.112378288234978e-05,
      "loss": 2.9547,
      "step": 8350
    },
    {
      "epoch": 1.347734966951475,
      "grad_norm": 7.1875,
      "learning_rate": 9.111302383129808e-05,
      "loss": 5.3875,
      "step": 8360
    },
    {
      "epoch": 1.349347090117685,
      "grad_norm": 6.71875,
      "learning_rate": 9.110226478024638e-05,
      "loss": 5.3047,
      "step": 8370
    },
    {
      "epoch": 1.350959213283895,
      "grad_norm": 17.25,
      "learning_rate": 9.109150572919469e-05,
      "loss": 2.9758,
      "step": 8380
    },
    {
      "epoch": 1.3525713364501049,
      "grad_norm": 8.75,
      "learning_rate": 9.1080746678143e-05,
      "loss": 4.0125,
      "step": 8390
    },
    {
      "epoch": 1.3541834596163147,
      "grad_norm": 6.0,
      "learning_rate": 9.106998762709129e-05,
      "loss": 3.9992,
      "step": 8400
    },
    {
      "epoch": 1.3557955827825245,
      "grad_norm": 7.28125,
      "learning_rate": 9.10592285760396e-05,
      "loss": 4.1148,
      "step": 8410
    },
    {
      "epoch": 1.3574077059487344,
      "grad_norm": 8.6875,
      "learning_rate": 9.10484695249879e-05,
      "loss": 3.8967,
      "step": 8420
    },
    {
      "epoch": 1.3590198291149445,
      "grad_norm": 19.75,
      "learning_rate": 9.10377104739362e-05,
      "loss": 3.4945,
      "step": 8430
    },
    {
      "epoch": 1.3606319522811543,
      "grad_norm": 24.875,
      "learning_rate": 9.10269514228845e-05,
      "loss": 3.3078,
      "step": 8440
    },
    {
      "epoch": 1.3622440754473641,
      "grad_norm": 18.875,
      "learning_rate": 9.10161923718328e-05,
      "loss": 3.9258,
      "step": 8450
    },
    {
      "epoch": 1.3638561986135742,
      "grad_norm": 8.8125,
      "learning_rate": 9.100543332078111e-05,
      "loss": 6.5164,
      "step": 8460
    },
    {
      "epoch": 1.365468321779784,
      "grad_norm": 10.125,
      "learning_rate": 9.099467426972942e-05,
      "loss": 4.2779,
      "step": 8470
    },
    {
      "epoch": 1.367080444945994,
      "grad_norm": 8.5,
      "learning_rate": 9.098391521867772e-05,
      "loss": 4.4384,
      "step": 8480
    },
    {
      "epoch": 1.3686925681122037,
      "grad_norm": 6.5,
      "learning_rate": 9.097315616762602e-05,
      "loss": 4.6,
      "step": 8490
    },
    {
      "epoch": 1.3703046912784136,
      "grad_norm": 13.5,
      "learning_rate": 9.096239711657433e-05,
      "loss": 5.8406,
      "step": 8500
    },
    {
      "epoch": 1.3719168144446234,
      "grad_norm": 11.8125,
      "learning_rate": 9.095163806552262e-05,
      "loss": 4.368,
      "step": 8510
    },
    {
      "epoch": 1.3735289376108335,
      "grad_norm": 9.5,
      "learning_rate": 9.094087901447092e-05,
      "loss": 3.9766,
      "step": 8520
    },
    {
      "epoch": 1.3751410607770433,
      "grad_norm": 8.0625,
      "learning_rate": 9.093011996341924e-05,
      "loss": 2.5078,
      "step": 8530
    },
    {
      "epoch": 1.3767531839432532,
      "grad_norm": 25.0,
      "learning_rate": 9.091936091236754e-05,
      "loss": 4.4953,
      "step": 8540
    },
    {
      "epoch": 1.3783653071094633,
      "grad_norm": 9.625,
      "learning_rate": 9.090860186131584e-05,
      "loss": 3.9703,
      "step": 8550
    },
    {
      "epoch": 1.379977430275673,
      "grad_norm": 20.875,
      "learning_rate": 9.089784281026415e-05,
      "loss": 2.8,
      "step": 8560
    },
    {
      "epoch": 1.381589553441883,
      "grad_norm": 0.0,
      "learning_rate": 9.088708375921244e-05,
      "loss": 5.6452,
      "step": 8570
    },
    {
      "epoch": 1.3832016766080928,
      "grad_norm": 21.75,
      "learning_rate": 9.087632470816074e-05,
      "loss": 4.474,
      "step": 8580
    },
    {
      "epoch": 1.3848137997743026,
      "grad_norm": 8.5625,
      "learning_rate": 9.086556565710905e-05,
      "loss": 6.1063,
      "step": 8590
    },
    {
      "epoch": 1.3864259229405127,
      "grad_norm": 14.0625,
      "learning_rate": 9.085480660605736e-05,
      "loss": 3.3297,
      "step": 8600
    },
    {
      "epoch": 1.3880380461067225,
      "grad_norm": 16.875,
      "learning_rate": 9.084404755500566e-05,
      "loss": 4.3789,
      "step": 8610
    },
    {
      "epoch": 1.3896501692729324,
      "grad_norm": 16.375,
      "learning_rate": 9.083328850395395e-05,
      "loss": 5.2164,
      "step": 8620
    },
    {
      "epoch": 1.3912622924391425,
      "grad_norm": 9.875,
      "learning_rate": 9.082252945290226e-05,
      "loss": 5.2484,
      "step": 8630
    },
    {
      "epoch": 1.3928744156053523,
      "grad_norm": 31.125,
      "learning_rate": 9.081177040185056e-05,
      "loss": 5.2016,
      "step": 8640
    },
    {
      "epoch": 1.3944865387715621,
      "grad_norm": 18.5,
      "learning_rate": 9.080101135079885e-05,
      "loss": 4.9289,
      "step": 8650
    },
    {
      "epoch": 1.396098661937772,
      "grad_norm": 8.0,
      "learning_rate": 9.079025229974716e-05,
      "loss": 4.9,
      "step": 8660
    },
    {
      "epoch": 1.3977107851039818,
      "grad_norm": 11.9375,
      "learning_rate": 9.077949324869547e-05,
      "loss": 3.7453,
      "step": 8670
    },
    {
      "epoch": 1.399322908270192,
      "grad_norm": 11.25,
      "learning_rate": 9.076873419764377e-05,
      "loss": 3.4125,
      "step": 8680
    },
    {
      "epoch": 1.4009350314364017,
      "grad_norm": 14.5625,
      "learning_rate": 9.075797514659208e-05,
      "loss": 2.5848,
      "step": 8690
    },
    {
      "epoch": 1.4025471546026116,
      "grad_norm": 9.5625,
      "learning_rate": 9.074721609554038e-05,
      "loss": 5.0,
      "step": 8700
    },
    {
      "epoch": 1.4041592777688217,
      "grad_norm": 19.75,
      "learning_rate": 9.073645704448867e-05,
      "loss": 5.6961,
      "step": 8710
    },
    {
      "epoch": 1.4057714009350315,
      "grad_norm": 53.0,
      "learning_rate": 9.072569799343698e-05,
      "loss": 3.9303,
      "step": 8720
    },
    {
      "epoch": 1.4073835241012413,
      "grad_norm": 11.0625,
      "learning_rate": 9.071493894238528e-05,
      "loss": 3.7641,
      "step": 8730
    },
    {
      "epoch": 1.4089956472674512,
      "grad_norm": 8.3125,
      "learning_rate": 9.070417989133359e-05,
      "loss": 4.4437,
      "step": 8740
    },
    {
      "epoch": 1.410607770433661,
      "grad_norm": 11.625,
      "learning_rate": 9.06934208402819e-05,
      "loss": 6.4469,
      "step": 8750
    },
    {
      "epoch": 1.412219893599871,
      "grad_norm": 8.4375,
      "learning_rate": 9.06826617892302e-05,
      "loss": 4.8727,
      "step": 8760
    },
    {
      "epoch": 1.413832016766081,
      "grad_norm": 12.625,
      "learning_rate": 9.06719027381785e-05,
      "loss": 3.9688,
      "step": 8770
    },
    {
      "epoch": 1.4154441399322908,
      "grad_norm": 9.5625,
      "learning_rate": 9.06611436871268e-05,
      "loss": 5.393,
      "step": 8780
    },
    {
      "epoch": 1.4170562630985009,
      "grad_norm": 21.5,
      "learning_rate": 9.06503846360751e-05,
      "loss": 5.4141,
      "step": 8790
    },
    {
      "epoch": 1.4186683862647107,
      "grad_norm": 34.5,
      "learning_rate": 9.06396255850234e-05,
      "loss": 3.9234,
      "step": 8800
    },
    {
      "epoch": 1.4202805094309205,
      "grad_norm": 21.0,
      "learning_rate": 9.062886653397171e-05,
      "loss": 5.2172,
      "step": 8810
    },
    {
      "epoch": 1.4218926325971304,
      "grad_norm": 21.0,
      "learning_rate": 9.061810748292002e-05,
      "loss": 4.2258,
      "step": 8820
    },
    {
      "epoch": 1.4235047557633402,
      "grad_norm": 8.4375,
      "learning_rate": 9.060734843186831e-05,
      "loss": 5.1203,
      "step": 8830
    },
    {
      "epoch": 1.42511687892955,
      "grad_norm": 8.125,
      "learning_rate": 9.059658938081662e-05,
      "loss": 4.7617,
      "step": 8840
    },
    {
      "epoch": 1.4267290020957601,
      "grad_norm": 9.0625,
      "learning_rate": 9.058583032976492e-05,
      "loss": 3.8852,
      "step": 8850
    },
    {
      "epoch": 1.42834112526197,
      "grad_norm": 6.8125,
      "learning_rate": 9.057507127871322e-05,
      "loss": 2.7574,
      "step": 8860
    },
    {
      "epoch": 1.4299532484281798,
      "grad_norm": 24.625,
      "learning_rate": 9.056431222766153e-05,
      "loss": 4.5563,
      "step": 8870
    },
    {
      "epoch": 1.43156537159439,
      "grad_norm": 19.625,
      "learning_rate": 9.055355317660982e-05,
      "loss": 3.2375,
      "step": 8880
    },
    {
      "epoch": 1.4331774947605997,
      "grad_norm": 19.0,
      "learning_rate": 9.054279412555813e-05,
      "loss": 4.3105,
      "step": 8890
    },
    {
      "epoch": 1.4347896179268096,
      "grad_norm": 12.0625,
      "learning_rate": 9.053203507450644e-05,
      "loss": 4.1672,
      "step": 8900
    },
    {
      "epoch": 1.4364017410930194,
      "grad_norm": 7.3125,
      "learning_rate": 9.052127602345474e-05,
      "loss": 4.6523,
      "step": 8910
    },
    {
      "epoch": 1.4380138642592293,
      "grad_norm": 5.59375,
      "learning_rate": 9.051051697240304e-05,
      "loss": 2.6641,
      "step": 8920
    },
    {
      "epoch": 1.4396259874254393,
      "grad_norm": 15.875,
      "learning_rate": 9.049975792135133e-05,
      "loss": 6.6516,
      "step": 8930
    },
    {
      "epoch": 1.4412381105916492,
      "grad_norm": 6.59375,
      "learning_rate": 9.048899887029964e-05,
      "loss": 5.2086,
      "step": 8940
    },
    {
      "epoch": 1.442850233757859,
      "grad_norm": 17.625,
      "learning_rate": 9.047823981924794e-05,
      "loss": 4.9156,
      "step": 8950
    },
    {
      "epoch": 1.444462356924069,
      "grad_norm": 14.0,
      "learning_rate": 9.046748076819625e-05,
      "loss": 5.0623,
      "step": 8960
    },
    {
      "epoch": 1.446074480090279,
      "grad_norm": 20.375,
      "learning_rate": 9.045672171714456e-05,
      "loss": 4.7703,
      "step": 8970
    },
    {
      "epoch": 1.4476866032564888,
      "grad_norm": 7.65625,
      "learning_rate": 9.044596266609286e-05,
      "loss": 3.0391,
      "step": 8980
    },
    {
      "epoch": 1.4492987264226986,
      "grad_norm": 24.375,
      "learning_rate": 9.043520361504115e-05,
      "loss": 5.4289,
      "step": 8990
    },
    {
      "epoch": 1.4509108495889085,
      "grad_norm": 20.25,
      "learning_rate": 9.042444456398946e-05,
      "loss": 5.9656,
      "step": 9000
    },
    {
      "epoch": 1.4525229727551185,
      "grad_norm": 18.625,
      "learning_rate": 9.041368551293776e-05,
      "loss": 3.8617,
      "step": 9010
    },
    {
      "epoch": 1.4541350959213284,
      "grad_norm": 7.9375,
      "learning_rate": 9.040292646188606e-05,
      "loss": 3.8258,
      "step": 9020
    },
    {
      "epoch": 1.4557472190875382,
      "grad_norm": 5.5,
      "learning_rate": 9.039216741083437e-05,
      "loss": 3.5383,
      "step": 9030
    },
    {
      "epoch": 1.4573593422537483,
      "grad_norm": 31.0,
      "learning_rate": 9.038140835978268e-05,
      "loss": 4.0711,
      "step": 9040
    },
    {
      "epoch": 1.4589714654199581,
      "grad_norm": 26.0,
      "learning_rate": 9.037064930873097e-05,
      "loss": 3.5672,
      "step": 9050
    },
    {
      "epoch": 1.460583588586168,
      "grad_norm": 27.625,
      "learning_rate": 9.035989025767928e-05,
      "loss": 3.6117,
      "step": 9060
    },
    {
      "epoch": 1.4621957117523778,
      "grad_norm": 5.8125,
      "learning_rate": 9.034913120662758e-05,
      "loss": 3.7633,
      "step": 9070
    },
    {
      "epoch": 1.4638078349185877,
      "grad_norm": 7.90625,
      "learning_rate": 9.033837215557588e-05,
      "loss": 2.6551,
      "step": 9080
    },
    {
      "epoch": 1.4654199580847977,
      "grad_norm": 12.0625,
      "learning_rate": 9.032761310452419e-05,
      "loss": 6.1273,
      "step": 9090
    },
    {
      "epoch": 1.4670320812510076,
      "grad_norm": 3.625,
      "learning_rate": 9.031685405347248e-05,
      "loss": 5.0336,
      "step": 9100
    },
    {
      "epoch": 1.4686442044172174,
      "grad_norm": 12.75,
      "learning_rate": 9.030609500242079e-05,
      "loss": 4.5852,
      "step": 9110
    },
    {
      "epoch": 1.4702563275834275,
      "grad_norm": 8.5,
      "learning_rate": 9.02953359513691e-05,
      "loss": 4.5781,
      "step": 9120
    },
    {
      "epoch": 1.4718684507496373,
      "grad_norm": 22.625,
      "learning_rate": 9.02845769003174e-05,
      "loss": 2.9977,
      "step": 9130
    },
    {
      "epoch": 1.4734805739158472,
      "grad_norm": 33.0,
      "learning_rate": 9.02738178492657e-05,
      "loss": 3.4453,
      "step": 9140
    },
    {
      "epoch": 1.475092697082057,
      "grad_norm": 8.8125,
      "learning_rate": 9.0263058798214e-05,
      "loss": 3.1,
      "step": 9150
    },
    {
      "epoch": 1.4767048202482669,
      "grad_norm": 13.25,
      "learning_rate": 9.02522997471623e-05,
      "loss": 2.5031,
      "step": 9160
    },
    {
      "epoch": 1.478316943414477,
      "grad_norm": 28.875,
      "learning_rate": 9.02415406961106e-05,
      "loss": 2.9937,
      "step": 9170
    },
    {
      "epoch": 1.4799290665806868,
      "grad_norm": 8.0,
      "learning_rate": 9.023078164505892e-05,
      "loss": 3.9636,
      "step": 9180
    },
    {
      "epoch": 1.4815411897468966,
      "grad_norm": 13.875,
      "learning_rate": 9.022002259400722e-05,
      "loss": 3.5305,
      "step": 9190
    },
    {
      "epoch": 1.4831533129131065,
      "grad_norm": 11.0625,
      "learning_rate": 9.020926354295551e-05,
      "loss": 3.982,
      "step": 9200
    },
    {
      "epoch": 1.4847654360793165,
      "grad_norm": 8.4375,
      "learning_rate": 9.019850449190382e-05,
      "loss": 4.1969,
      "step": 9210
    },
    {
      "epoch": 1.4863775592455264,
      "grad_norm": 12.1875,
      "learning_rate": 9.018774544085212e-05,
      "loss": 4.2047,
      "step": 9220
    },
    {
      "epoch": 1.4879896824117362,
      "grad_norm": 28.375,
      "learning_rate": 9.017698638980042e-05,
      "loss": 3.9961,
      "step": 9230
    },
    {
      "epoch": 1.489601805577946,
      "grad_norm": 9.875,
      "learning_rate": 9.016622733874873e-05,
      "loss": 2.6234,
      "step": 9240
    },
    {
      "epoch": 1.491213928744156,
      "grad_norm": 5.96875,
      "learning_rate": 9.015546828769704e-05,
      "loss": 4.5391,
      "step": 9250
    },
    {
      "epoch": 1.492826051910366,
      "grad_norm": 8.6875,
      "learning_rate": 9.014470923664533e-05,
      "loss": 4.4992,
      "step": 9260
    },
    {
      "epoch": 1.4944381750765758,
      "grad_norm": 6.1875,
      "learning_rate": 9.013395018559363e-05,
      "loss": 4.2039,
      "step": 9270
    },
    {
      "epoch": 1.4960502982427857,
      "grad_norm": 10.125,
      "learning_rate": 9.012319113454194e-05,
      "loss": 3.7961,
      "step": 9280
    },
    {
      "epoch": 1.4976624214089957,
      "grad_norm": 20.375,
      "learning_rate": 9.011243208349024e-05,
      "loss": 4.0687,
      "step": 9290
    },
    {
      "epoch": 1.4992745445752056,
      "grad_norm": 51.25,
      "learning_rate": 9.010167303243853e-05,
      "loss": 3.4136,
      "step": 9300
    },
    {
      "epoch": 1.5008866677414154,
      "grad_norm": 12.875,
      "learning_rate": 9.009091398138684e-05,
      "loss": 5.3922,
      "step": 9310
    },
    {
      "epoch": 1.5024987909076253,
      "grad_norm": 102.5,
      "learning_rate": 9.008015493033515e-05,
      "loss": 5.6453,
      "step": 9320
    },
    {
      "epoch": 1.5041109140738351,
      "grad_norm": 13.125,
      "learning_rate": 9.006939587928345e-05,
      "loss": 3.8695,
      "step": 9330
    },
    {
      "epoch": 1.5057230372400452,
      "grad_norm": 80.5,
      "learning_rate": 9.005863682823176e-05,
      "loss": 4.4352,
      "step": 9340
    },
    {
      "epoch": 1.507335160406255,
      "grad_norm": 31.75,
      "learning_rate": 9.004787777718006e-05,
      "loss": 4.8797,
      "step": 9350
    },
    {
      "epoch": 1.5089472835724649,
      "grad_norm": 6.34375,
      "learning_rate": 9.003711872612835e-05,
      "loss": 4.0531,
      "step": 9360
    },
    {
      "epoch": 1.510559406738675,
      "grad_norm": 13.375,
      "learning_rate": 9.002635967507666e-05,
      "loss": 4.1859,
      "step": 9370
    },
    {
      "epoch": 1.5121715299048848,
      "grad_norm": 28.625,
      "learning_rate": 9.001560062402496e-05,
      "loss": 4.0867,
      "step": 9380
    },
    {
      "epoch": 1.5137836530710946,
      "grad_norm": 17.125,
      "learning_rate": 9.000484157297327e-05,
      "loss": 5.0312,
      "step": 9390
    },
    {
      "epoch": 1.5153957762373045,
      "grad_norm": 9.4375,
      "learning_rate": 8.999408252192158e-05,
      "loss": 4.418,
      "step": 9400
    },
    {
      "epoch": 1.5170078994035143,
      "grad_norm": 108.0,
      "learning_rate": 8.998332347086988e-05,
      "loss": 4.2719,
      "step": 9410
    },
    {
      "epoch": 1.5186200225697242,
      "grad_norm": 9.0625,
      "learning_rate": 8.997256441981817e-05,
      "loss": 3.0867,
      "step": 9420
    },
    {
      "epoch": 1.5202321457359342,
      "grad_norm": 10.5625,
      "learning_rate": 8.996180536876648e-05,
      "loss": 4.4555,
      "step": 9430
    },
    {
      "epoch": 1.521844268902144,
      "grad_norm": 8.375,
      "learning_rate": 8.995104631771478e-05,
      "loss": 3.3258,
      "step": 9440
    },
    {
      "epoch": 1.5234563920683541,
      "grad_norm": 10.0,
      "learning_rate": 8.994028726666308e-05,
      "loss": 3.4305,
      "step": 9450
    },
    {
      "epoch": 1.525068515234564,
      "grad_norm": 23.625,
      "learning_rate": 8.992952821561139e-05,
      "loss": 4.6781,
      "step": 9460
    },
    {
      "epoch": 1.5266806384007738,
      "grad_norm": 5.03125,
      "learning_rate": 8.99187691645597e-05,
      "loss": 2.4961,
      "step": 9470
    },
    {
      "epoch": 1.5282927615669837,
      "grad_norm": 8.25,
      "learning_rate": 8.990801011350799e-05,
      "loss": 4.1855,
      "step": 9480
    },
    {
      "epoch": 1.5299048847331935,
      "grad_norm": 22.25,
      "learning_rate": 8.98972510624563e-05,
      "loss": 3.6398,
      "step": 9490
    },
    {
      "epoch": 1.5315170078994034,
      "grad_norm": 12.0625,
      "learning_rate": 8.98864920114046e-05,
      "loss": 4.7875,
      "step": 9500
    },
    {
      "epoch": 1.5331291310656134,
      "grad_norm": 10.0,
      "learning_rate": 8.98757329603529e-05,
      "loss": 5.025,
      "step": 9510
    },
    {
      "epoch": 1.5347412542318233,
      "grad_norm": 14.5625,
      "learning_rate": 8.98649739093012e-05,
      "loss": 4.8641,
      "step": 9520
    },
    {
      "epoch": 1.5363533773980333,
      "grad_norm": 10.6875,
      "learning_rate": 8.98542148582495e-05,
      "loss": 4.6227,
      "step": 9530
    },
    {
      "epoch": 1.5379655005642432,
      "grad_norm": 13.0625,
      "learning_rate": 8.984345580719781e-05,
      "loss": 4.4313,
      "step": 9540
    },
    {
      "epoch": 1.539577623730453,
      "grad_norm": 6.21875,
      "learning_rate": 8.983269675614612e-05,
      "loss": 5.1258,
      "step": 9550
    },
    {
      "epoch": 1.5411897468966629,
      "grad_norm": 20.25,
      "learning_rate": 8.982193770509442e-05,
      "loss": 4.9203,
      "step": 9560
    },
    {
      "epoch": 1.5428018700628727,
      "grad_norm": 12.0,
      "learning_rate": 8.981117865404272e-05,
      "loss": 5.1109,
      "step": 9570
    },
    {
      "epoch": 1.5444139932290826,
      "grad_norm": 58.75,
      "learning_rate": 8.980041960299101e-05,
      "loss": 2.8057,
      "step": 9580
    },
    {
      "epoch": 1.5460261163952926,
      "grad_norm": 19.25,
      "learning_rate": 8.978966055193932e-05,
      "loss": 4.9461,
      "step": 9590
    },
    {
      "epoch": 1.5476382395615025,
      "grad_norm": 9.1875,
      "learning_rate": 8.977890150088762e-05,
      "loss": 2.5898,
      "step": 9600
    },
    {
      "epoch": 1.5492503627277125,
      "grad_norm": 9.6875,
      "learning_rate": 8.976814244983593e-05,
      "loss": 3.7313,
      "step": 9610
    },
    {
      "epoch": 1.5508624858939224,
      "grad_norm": 7.5,
      "learning_rate": 8.975738339878424e-05,
      "loss": 3.4094,
      "step": 9620
    },
    {
      "epoch": 1.5524746090601322,
      "grad_norm": 17.125,
      "learning_rate": 8.974662434773254e-05,
      "loss": 3.782,
      "step": 9630
    },
    {
      "epoch": 1.554086732226342,
      "grad_norm": 13.4375,
      "learning_rate": 8.973586529668083e-05,
      "loss": 6.2148,
      "step": 9640
    },
    {
      "epoch": 1.555698855392552,
      "grad_norm": 5.8125,
      "learning_rate": 8.972510624562914e-05,
      "loss": 2.3242,
      "step": 9650
    },
    {
      "epoch": 1.5573109785587618,
      "grad_norm": 28.0,
      "learning_rate": 8.971434719457744e-05,
      "loss": 4.0305,
      "step": 9660
    },
    {
      "epoch": 1.5589231017249718,
      "grad_norm": 32.5,
      "learning_rate": 8.970358814352573e-05,
      "loss": 5.3641,
      "step": 9670
    },
    {
      "epoch": 1.5605352248911817,
      "grad_norm": 5.53125,
      "learning_rate": 8.969282909247404e-05,
      "loss": 3.3539,
      "step": 9680
    },
    {
      "epoch": 1.5621473480573917,
      "grad_norm": 36.0,
      "learning_rate": 8.968207004142236e-05,
      "loss": 5.0164,
      "step": 9690
    },
    {
      "epoch": 1.5637594712236016,
      "grad_norm": 19.5,
      "learning_rate": 8.967131099037065e-05,
      "loss": 5.2977,
      "step": 9700
    },
    {
      "epoch": 1.5653715943898114,
      "grad_norm": 24.0,
      "learning_rate": 8.966055193931896e-05,
      "loss": 3.1524,
      "step": 9710
    },
    {
      "epoch": 1.5669837175560213,
      "grad_norm": 27.125,
      "learning_rate": 8.964979288826726e-05,
      "loss": 4.2469,
      "step": 9720
    },
    {
      "epoch": 1.5685958407222311,
      "grad_norm": 22.375,
      "learning_rate": 8.963903383721555e-05,
      "loss": 6.0945,
      "step": 9730
    },
    {
      "epoch": 1.570207963888441,
      "grad_norm": 179.0,
      "learning_rate": 8.962827478616386e-05,
      "loss": 4.475,
      "step": 9740
    },
    {
      "epoch": 1.5718200870546508,
      "grad_norm": 28.125,
      "learning_rate": 8.961751573511216e-05,
      "loss": 5.2125,
      "step": 9750
    },
    {
      "epoch": 1.5734322102208609,
      "grad_norm": 12.1875,
      "learning_rate": 8.960675668406047e-05,
      "loss": 3.2684,
      "step": 9760
    },
    {
      "epoch": 1.5750443333870707,
      "grad_norm": 43.25,
      "learning_rate": 8.959599763300878e-05,
      "loss": 3.7664,
      "step": 9770
    },
    {
      "epoch": 1.5766564565532808,
      "grad_norm": 19.875,
      "learning_rate": 8.958523858195708e-05,
      "loss": 3.9117,
      "step": 9780
    },
    {
      "epoch": 1.5782685797194906,
      "grad_norm": 7.84375,
      "learning_rate": 8.957447953090537e-05,
      "loss": 3.6148,
      "step": 9790
    },
    {
      "epoch": 1.5798807028857005,
      "grad_norm": 65.0,
      "learning_rate": 8.956372047985368e-05,
      "loss": 3.5656,
      "step": 9800
    },
    {
      "epoch": 1.5814928260519103,
      "grad_norm": 19.25,
      "learning_rate": 8.955296142880198e-05,
      "loss": 3.9789,
      "step": 9810
    },
    {
      "epoch": 1.5831049492181202,
      "grad_norm": 8.6875,
      "learning_rate": 8.954220237775028e-05,
      "loss": 3.9367,
      "step": 9820
    },
    {
      "epoch": 1.58471707238433,
      "grad_norm": 11.25,
      "learning_rate": 8.953144332669859e-05,
      "loss": 3.8805,
      "step": 9830
    },
    {
      "epoch": 1.58632919555054,
      "grad_norm": 8.375,
      "learning_rate": 8.95206842756469e-05,
      "loss": 3.8797,
      "step": 9840
    },
    {
      "epoch": 1.58794131871675,
      "grad_norm": 11.4375,
      "learning_rate": 8.95099252245952e-05,
      "loss": 5.1109,
      "step": 9850
    },
    {
      "epoch": 1.58955344188296,
      "grad_norm": 24.25,
      "learning_rate": 8.94991661735435e-05,
      "loss": 4.0023,
      "step": 9860
    },
    {
      "epoch": 1.5911655650491698,
      "grad_norm": 9.6875,
      "learning_rate": 8.94884071224918e-05,
      "loss": 4.8852,
      "step": 9870
    },
    {
      "epoch": 1.5927776882153797,
      "grad_norm": 12.6875,
      "learning_rate": 8.94776480714401e-05,
      "loss": 2.9398,
      "step": 9880
    },
    {
      "epoch": 1.5943898113815895,
      "grad_norm": 29.5,
      "learning_rate": 8.946688902038841e-05,
      "loss": 3.6945,
      "step": 9890
    },
    {
      "epoch": 1.5960019345477994,
      "grad_norm": 26.875,
      "learning_rate": 8.945612996933672e-05,
      "loss": 5.1703,
      "step": 9900
    },
    {
      "epoch": 1.5976140577140092,
      "grad_norm": 9.1875,
      "learning_rate": 8.944537091828501e-05,
      "loss": 3.5359,
      "step": 9910
    },
    {
      "epoch": 1.5992261808802193,
      "grad_norm": 6.21875,
      "learning_rate": 8.943461186723331e-05,
      "loss": 2.6949,
      "step": 9920
    },
    {
      "epoch": 1.6008383040464291,
      "grad_norm": 7.625,
      "learning_rate": 8.942385281618162e-05,
      "loss": 3.4727,
      "step": 9930
    },
    {
      "epoch": 1.6024504272126392,
      "grad_norm": 11.8125,
      "learning_rate": 8.941309376512992e-05,
      "loss": 4.2672,
      "step": 9940
    },
    {
      "epoch": 1.604062550378849,
      "grad_norm": 8.8125,
      "learning_rate": 8.940233471407821e-05,
      "loss": 2.8633,
      "step": 9950
    },
    {
      "epoch": 1.6056746735450589,
      "grad_norm": 18.625,
      "learning_rate": 8.939157566302652e-05,
      "loss": 4.1281,
      "step": 9960
    },
    {
      "epoch": 1.6072867967112687,
      "grad_norm": 11.8125,
      "learning_rate": 8.938081661197483e-05,
      "loss": 4.3539,
      "step": 9970
    },
    {
      "epoch": 1.6088989198774786,
      "grad_norm": 30.875,
      "learning_rate": 8.937005756092313e-05,
      "loss": 4.4344,
      "step": 9980
    },
    {
      "epoch": 1.6105110430436884,
      "grad_norm": 9.875,
      "learning_rate": 8.935929850987144e-05,
      "loss": 3.2422,
      "step": 9990
    },
    {
      "epoch": 1.6121231662098985,
      "grad_norm": 16.0,
      "learning_rate": 8.934853945881974e-05,
      "loss": 4.0227,
      "step": 10000
    },
    {
      "epoch": 1.6137352893761083,
      "grad_norm": 118.5,
      "learning_rate": 8.933778040776803e-05,
      "loss": 3.0836,
      "step": 10010
    },
    {
      "epoch": 1.6153474125423184,
      "grad_norm": 17.0,
      "learning_rate": 8.932702135671634e-05,
      "loss": 4.8656,
      "step": 10020
    },
    {
      "epoch": 1.6169595357085282,
      "grad_norm": 16.375,
      "learning_rate": 8.931626230566464e-05,
      "loss": 4.2902,
      "step": 10030
    },
    {
      "epoch": 1.618571658874738,
      "grad_norm": 14.6875,
      "learning_rate": 8.930550325461295e-05,
      "loss": 3.8047,
      "step": 10040
    },
    {
      "epoch": 1.620183782040948,
      "grad_norm": 37.0,
      "learning_rate": 8.929474420356126e-05,
      "loss": 3.1875,
      "step": 10050
    },
    {
      "epoch": 1.6217959052071578,
      "grad_norm": 9.9375,
      "learning_rate": 8.928398515250956e-05,
      "loss": 3.4586,
      "step": 10060
    },
    {
      "epoch": 1.6234080283733676,
      "grad_norm": 10.375,
      "learning_rate": 8.927322610145785e-05,
      "loss": 3.3711,
      "step": 10070
    },
    {
      "epoch": 1.6250201515395775,
      "grad_norm": 7.375,
      "learning_rate": 8.926246705040616e-05,
      "loss": 3.2969,
      "step": 10080
    },
    {
      "epoch": 1.6266322747057875,
      "grad_norm": 8.4375,
      "learning_rate": 8.925170799935446e-05,
      "loss": 3.8797,
      "step": 10090
    },
    {
      "epoch": 1.6282443978719974,
      "grad_norm": 39.0,
      "learning_rate": 8.924094894830276e-05,
      "loss": 4.6855,
      "step": 10100
    },
    {
      "epoch": 1.6298565210382074,
      "grad_norm": 9.125,
      "learning_rate": 8.923018989725107e-05,
      "loss": 4.5984,
      "step": 10110
    },
    {
      "epoch": 1.6314686442044173,
      "grad_norm": 8.625,
      "learning_rate": 8.921943084619938e-05,
      "loss": 4.2266,
      "step": 10120
    },
    {
      "epoch": 1.6330807673706271,
      "grad_norm": 24.625,
      "learning_rate": 8.920867179514767e-05,
      "loss": 4.0125,
      "step": 10130
    },
    {
      "epoch": 1.634692890536837,
      "grad_norm": 10.875,
      "learning_rate": 8.919791274409598e-05,
      "loss": 5.375,
      "step": 10140
    },
    {
      "epoch": 1.6363050137030468,
      "grad_norm": 118.5,
      "learning_rate": 8.918715369304428e-05,
      "loss": 4.2875,
      "step": 10150
    },
    {
      "epoch": 1.6379171368692567,
      "grad_norm": 12.25,
      "learning_rate": 8.917639464199258e-05,
      "loss": 2.9766,
      "step": 10160
    },
    {
      "epoch": 1.6395292600354667,
      "grad_norm": 39.75,
      "learning_rate": 8.916563559094089e-05,
      "loss": 5.0125,
      "step": 10170
    },
    {
      "epoch": 1.6411413832016766,
      "grad_norm": 10.4375,
      "learning_rate": 8.915487653988918e-05,
      "loss": 3.5063,
      "step": 10180
    },
    {
      "epoch": 1.6427535063678866,
      "grad_norm": 15.5625,
      "learning_rate": 8.914411748883749e-05,
      "loss": 2.9605,
      "step": 10190
    },
    {
      "epoch": 1.6443656295340965,
      "grad_norm": 19.625,
      "learning_rate": 8.91333584377858e-05,
      "loss": 3.3086,
      "step": 10200
    },
    {
      "epoch": 1.6459777527003063,
      "grad_norm": 21.0,
      "learning_rate": 8.91225993867341e-05,
      "loss": 5.0483,
      "step": 10210
    },
    {
      "epoch": 1.6475898758665162,
      "grad_norm": 6.6875,
      "learning_rate": 8.91118403356824e-05,
      "loss": 2.6055,
      "step": 10220
    },
    {
      "epoch": 1.649201999032726,
      "grad_norm": 21.0,
      "learning_rate": 8.91010812846307e-05,
      "loss": 2.8297,
      "step": 10230
    },
    {
      "epoch": 1.6508141221989359,
      "grad_norm": 12.75,
      "learning_rate": 8.9090322233579e-05,
      "loss": 3.4609,
      "step": 10240
    },
    {
      "epoch": 1.652426245365146,
      "grad_norm": 17.5,
      "learning_rate": 8.90795631825273e-05,
      "loss": 4.6543,
      "step": 10250
    },
    {
      "epoch": 1.6540383685313558,
      "grad_norm": 9.75,
      "learning_rate": 8.906880413147561e-05,
      "loss": 4.6164,
      "step": 10260
    },
    {
      "epoch": 1.6556504916975658,
      "grad_norm": 20.125,
      "learning_rate": 8.905804508042392e-05,
      "loss": 2.9352,
      "step": 10270
    },
    {
      "epoch": 1.6572626148637757,
      "grad_norm": 31.625,
      "learning_rate": 8.904728602937221e-05,
      "loss": 3.1422,
      "step": 10280
    },
    {
      "epoch": 1.6588747380299855,
      "grad_norm": 93.5,
      "learning_rate": 8.903652697832051e-05,
      "loss": 5.4227,
      "step": 10290
    },
    {
      "epoch": 1.6604868611961954,
      "grad_norm": 34.0,
      "learning_rate": 8.902576792726882e-05,
      "loss": 4.4234,
      "step": 10300
    },
    {
      "epoch": 1.6620989843624052,
      "grad_norm": 9.6875,
      "learning_rate": 8.901500887621712e-05,
      "loss": 5.4008,
      "step": 10310
    },
    {
      "epoch": 1.663711107528615,
      "grad_norm": 17.875,
      "learning_rate": 8.900424982516541e-05,
      "loss": 4.2453,
      "step": 10320
    },
    {
      "epoch": 1.6653232306948251,
      "grad_norm": 25.5,
      "learning_rate": 8.899349077411372e-05,
      "loss": 6.3527,
      "step": 10330
    },
    {
      "epoch": 1.666935353861035,
      "grad_norm": 7.8125,
      "learning_rate": 8.898273172306203e-05,
      "loss": 4.25,
      "step": 10340
    },
    {
      "epoch": 1.668547477027245,
      "grad_norm": 37.25,
      "learning_rate": 8.897197267201033e-05,
      "loss": 5.2367,
      "step": 10350
    },
    {
      "epoch": 1.6701596001934549,
      "grad_norm": 19.0,
      "learning_rate": 8.896121362095864e-05,
      "loss": 4.5102,
      "step": 10360
    },
    {
      "epoch": 1.6717717233596647,
      "grad_norm": 10.0625,
      "learning_rate": 8.895045456990694e-05,
      "loss": 3.5395,
      "step": 10370
    },
    {
      "epoch": 1.6733838465258746,
      "grad_norm": 30.875,
      "learning_rate": 8.893969551885523e-05,
      "loss": 2.8324,
      "step": 10380
    },
    {
      "epoch": 1.6749959696920844,
      "grad_norm": 8.3125,
      "learning_rate": 8.892893646780354e-05,
      "loss": 3.6035,
      "step": 10390
    },
    {
      "epoch": 1.6766080928582943,
      "grad_norm": 13.6875,
      "learning_rate": 8.891817741675184e-05,
      "loss": 3.732,
      "step": 10400
    },
    {
      "epoch": 1.678220216024504,
      "grad_norm": 34.5,
      "learning_rate": 8.890741836570015e-05,
      "loss": 4.2617,
      "step": 10410
    },
    {
      "epoch": 1.6798323391907142,
      "grad_norm": 18.0,
      "learning_rate": 8.889665931464846e-05,
      "loss": 4.3117,
      "step": 10420
    },
    {
      "epoch": 1.681444462356924,
      "grad_norm": 21.0,
      "learning_rate": 8.888590026359676e-05,
      "loss": 4.4359,
      "step": 10430
    },
    {
      "epoch": 1.683056585523134,
      "grad_norm": 8.5625,
      "learning_rate": 8.887514121254505e-05,
      "loss": 3.5387,
      "step": 10440
    },
    {
      "epoch": 1.684668708689344,
      "grad_norm": 43.0,
      "learning_rate": 8.886438216149336e-05,
      "loss": 3.6883,
      "step": 10450
    },
    {
      "epoch": 1.6862808318555538,
      "grad_norm": 104.5,
      "learning_rate": 8.885362311044166e-05,
      "loss": 3.6051,
      "step": 10460
    },
    {
      "epoch": 1.6878929550217636,
      "grad_norm": 34.75,
      "learning_rate": 8.884286405938996e-05,
      "loss": 2.557,
      "step": 10470
    },
    {
      "epoch": 1.6895050781879735,
      "grad_norm": 6.625,
      "learning_rate": 8.883210500833827e-05,
      "loss": 2.0695,
      "step": 10480
    },
    {
      "epoch": 1.6911172013541833,
      "grad_norm": 17.125,
      "learning_rate": 8.882134595728658e-05,
      "loss": 5.5703,
      "step": 10490
    },
    {
      "epoch": 1.6927293245203934,
      "grad_norm": 10.1875,
      "learning_rate": 8.881058690623487e-05,
      "loss": 4.8352,
      "step": 10500
    },
    {
      "epoch": 1.6943414476866032,
      "grad_norm": 20.875,
      "learning_rate": 8.879982785518318e-05,
      "loss": 3.8438,
      "step": 10510
    },
    {
      "epoch": 1.6959535708528133,
      "grad_norm": 13.0625,
      "learning_rate": 8.878906880413148e-05,
      "loss": 3.9531,
      "step": 10520
    },
    {
      "epoch": 1.6975656940190231,
      "grad_norm": 16.75,
      "learning_rate": 8.877830975307978e-05,
      "loss": 3.9197,
      "step": 10530
    },
    {
      "epoch": 1.699177817185233,
      "grad_norm": 21.75,
      "learning_rate": 8.876755070202809e-05,
      "loss": 4.9703,
      "step": 10540
    },
    {
      "epoch": 1.7007899403514428,
      "grad_norm": 8.0,
      "learning_rate": 8.87567916509764e-05,
      "loss": 3.4787,
      "step": 10550
    },
    {
      "epoch": 1.7024020635176527,
      "grad_norm": 40.25,
      "learning_rate": 8.874603259992469e-05,
      "loss": 3.7273,
      "step": 10560
    },
    {
      "epoch": 1.7040141866838625,
      "grad_norm": 15.5,
      "learning_rate": 8.8735273548873e-05,
      "loss": 3.6695,
      "step": 10570
    },
    {
      "epoch": 1.7056263098500726,
      "grad_norm": 7.46875,
      "learning_rate": 8.87245144978213e-05,
      "loss": 3.0367,
      "step": 10580
    },
    {
      "epoch": 1.7072384330162824,
      "grad_norm": 0.0,
      "learning_rate": 8.87137554467696e-05,
      "loss": 3.676,
      "step": 10590
    },
    {
      "epoch": 1.7088505561824925,
      "grad_norm": 13.75,
      "learning_rate": 8.870299639571789e-05,
      "loss": 3.2671,
      "step": 10600
    },
    {
      "epoch": 1.7104626793487023,
      "grad_norm": 58.5,
      "learning_rate": 8.86922373446662e-05,
      "loss": 3.3594,
      "step": 10610
    },
    {
      "epoch": 1.7120748025149122,
      "grad_norm": 9.25,
      "learning_rate": 8.868147829361451e-05,
      "loss": 4.1336,
      "step": 10620
    },
    {
      "epoch": 1.713686925681122,
      "grad_norm": 5.78125,
      "learning_rate": 8.867071924256281e-05,
      "loss": 3.757,
      "step": 10630
    },
    {
      "epoch": 1.7152990488473319,
      "grad_norm": 19.5,
      "learning_rate": 8.865996019151112e-05,
      "loss": 3.3188,
      "step": 10640
    },
    {
      "epoch": 1.7169111720135417,
      "grad_norm": 7.90625,
      "learning_rate": 8.864920114045942e-05,
      "loss": 5.4844,
      "step": 10650
    },
    {
      "epoch": 1.7185232951797518,
      "grad_norm": 35.25,
      "learning_rate": 8.863844208940771e-05,
      "loss": 3.875,
      "step": 10660
    },
    {
      "epoch": 1.7201354183459616,
      "grad_norm": 38.25,
      "learning_rate": 8.862768303835602e-05,
      "loss": 4.3141,
      "step": 10670
    },
    {
      "epoch": 1.7217475415121717,
      "grad_norm": 24.5,
      "learning_rate": 8.861692398730432e-05,
      "loss": 5.5883,
      "step": 10680
    },
    {
      "epoch": 1.7233596646783815,
      "grad_norm": 15.125,
      "learning_rate": 8.860616493625263e-05,
      "loss": 3.1694,
      "step": 10690
    },
    {
      "epoch": 1.7249717878445914,
      "grad_norm": 25.625,
      "learning_rate": 8.859540588520094e-05,
      "loss": 3.5082,
      "step": 10700
    },
    {
      "epoch": 1.7265839110108012,
      "grad_norm": 11.75,
      "learning_rate": 8.858464683414924e-05,
      "loss": 2.4031,
      "step": 10710
    },
    {
      "epoch": 1.728196034177011,
      "grad_norm": 15.875,
      "learning_rate": 8.857388778309753e-05,
      "loss": 3.1504,
      "step": 10720
    },
    {
      "epoch": 1.729808157343221,
      "grad_norm": 10.75,
      "learning_rate": 8.856312873204584e-05,
      "loss": 5.2398,
      "step": 10730
    },
    {
      "epoch": 1.731420280509431,
      "grad_norm": 16.875,
      "learning_rate": 8.855236968099414e-05,
      "loss": 4.3992,
      "step": 10740
    },
    {
      "epoch": 1.7330324036756408,
      "grad_norm": 15.125,
      "learning_rate": 8.854161062994243e-05,
      "loss": 3.8672,
      "step": 10750
    },
    {
      "epoch": 1.7346445268418507,
      "grad_norm": 16.125,
      "learning_rate": 8.853085157889074e-05,
      "loss": 4.2453,
      "step": 10760
    },
    {
      "epoch": 1.7362566500080607,
      "grad_norm": 9.3125,
      "learning_rate": 8.852009252783905e-05,
      "loss": 3.8227,
      "step": 10770
    },
    {
      "epoch": 1.7378687731742706,
      "grad_norm": 17.0,
      "learning_rate": 8.850933347678735e-05,
      "loss": 3.4531,
      "step": 10780
    },
    {
      "epoch": 1.7394808963404804,
      "grad_norm": 11.625,
      "learning_rate": 8.849857442573566e-05,
      "loss": 4.6422,
      "step": 10790
    },
    {
      "epoch": 1.7410930195066903,
      "grad_norm": 33.25,
      "learning_rate": 8.848781537468396e-05,
      "loss": 2.5874,
      "step": 10800
    },
    {
      "epoch": 1.7427051426729,
      "grad_norm": 15.5625,
      "learning_rate": 8.847705632363225e-05,
      "loss": 4.1156,
      "step": 10810
    },
    {
      "epoch": 1.74431726583911,
      "grad_norm": 9.125,
      "learning_rate": 8.846629727258056e-05,
      "loss": 3.8992,
      "step": 10820
    },
    {
      "epoch": 1.74592938900532,
      "grad_norm": 11.5625,
      "learning_rate": 8.845553822152886e-05,
      "loss": 2.3352,
      "step": 10830
    },
    {
      "epoch": 1.7475415121715299,
      "grad_norm": 7.0,
      "learning_rate": 8.844477917047717e-05,
      "loss": 3.5344,
      "step": 10840
    },
    {
      "epoch": 1.74915363533774,
      "grad_norm": 8.8125,
      "learning_rate": 8.843402011942548e-05,
      "loss": 2.9758,
      "step": 10850
    },
    {
      "epoch": 1.7507657585039498,
      "grad_norm": 17.75,
      "learning_rate": 8.842326106837378e-05,
      "loss": 4.007,
      "step": 10860
    },
    {
      "epoch": 1.7523778816701596,
      "grad_norm": 8.4375,
      "learning_rate": 8.841250201732207e-05,
      "loss": 4.6852,
      "step": 10870
    },
    {
      "epoch": 1.7539900048363695,
      "grad_norm": 10.8125,
      "learning_rate": 8.840174296627038e-05,
      "loss": 5.2629,
      "step": 10880
    },
    {
      "epoch": 1.7556021280025793,
      "grad_norm": 14.0625,
      "learning_rate": 8.839098391521868e-05,
      "loss": 3.9266,
      "step": 10890
    },
    {
      "epoch": 1.7572142511687892,
      "grad_norm": 17.25,
      "learning_rate": 8.838022486416698e-05,
      "loss": 2.1895,
      "step": 10900
    },
    {
      "epoch": 1.7588263743349992,
      "grad_norm": 18.625,
      "learning_rate": 8.836946581311529e-05,
      "loss": 4.5178,
      "step": 10910
    },
    {
      "epoch": 1.760438497501209,
      "grad_norm": 10.6875,
      "learning_rate": 8.83587067620636e-05,
      "loss": 4.1078,
      "step": 10920
    },
    {
      "epoch": 1.7620506206674191,
      "grad_norm": 9.0,
      "learning_rate": 8.83479477110119e-05,
      "loss": 3.4375,
      "step": 10930
    },
    {
      "epoch": 1.763662743833629,
      "grad_norm": 24.75,
      "learning_rate": 8.833718865996019e-05,
      "loss": 5.3133,
      "step": 10940
    },
    {
      "epoch": 1.7652748669998388,
      "grad_norm": 22.25,
      "learning_rate": 8.83264296089085e-05,
      "loss": 3.3859,
      "step": 10950
    },
    {
      "epoch": 1.7668869901660487,
      "grad_norm": 14.75,
      "learning_rate": 8.83156705578568e-05,
      "loss": 3.6703,
      "step": 10960
    },
    {
      "epoch": 1.7684991133322585,
      "grad_norm": 16.125,
      "learning_rate": 8.83049115068051e-05,
      "loss": 5.5969,
      "step": 10970
    },
    {
      "epoch": 1.7701112364984684,
      "grad_norm": 11.8125,
      "learning_rate": 8.82941524557534e-05,
      "loss": 3.3469,
      "step": 10980
    },
    {
      "epoch": 1.7717233596646784,
      "grad_norm": 13.0625,
      "learning_rate": 8.828339340470171e-05,
      "loss": 4.4203,
      "step": 10990
    },
    {
      "epoch": 1.7733354828308883,
      "grad_norm": 17.0,
      "learning_rate": 8.827263435365001e-05,
      "loss": 5.0844,
      "step": 11000
    },
    {
      "epoch": 1.7749476059970983,
      "grad_norm": 9.0625,
      "learning_rate": 8.826187530259832e-05,
      "loss": 4.9336,
      "step": 11010
    },
    {
      "epoch": 1.7765597291633082,
      "grad_norm": 9.3125,
      "learning_rate": 8.825111625154662e-05,
      "loss": 2.9797,
      "step": 11020
    },
    {
      "epoch": 1.778171852329518,
      "grad_norm": 10.0,
      "learning_rate": 8.824035720049491e-05,
      "loss": 4.2517,
      "step": 11030
    },
    {
      "epoch": 1.7797839754957279,
      "grad_norm": 11.625,
      "learning_rate": 8.822959814944322e-05,
      "loss": 3.5324,
      "step": 11040
    },
    {
      "epoch": 1.7813960986619377,
      "grad_norm": 13.8125,
      "learning_rate": 8.821883909839152e-05,
      "loss": 3.4305,
      "step": 11050
    },
    {
      "epoch": 1.7830082218281476,
      "grad_norm": 20.625,
      "learning_rate": 8.820808004733983e-05,
      "loss": 3.7891,
      "step": 11060
    },
    {
      "epoch": 1.7846203449943576,
      "grad_norm": 27.625,
      "learning_rate": 8.819732099628814e-05,
      "loss": 5.2227,
      "step": 11070
    },
    {
      "epoch": 1.7862324681605675,
      "grad_norm": 14.8125,
      "learning_rate": 8.818656194523644e-05,
      "loss": 4.1055,
      "step": 11080
    },
    {
      "epoch": 1.7878445913267773,
      "grad_norm": 16.375,
      "learning_rate": 8.817580289418473e-05,
      "loss": 3.9729,
      "step": 11090
    },
    {
      "epoch": 1.7894567144929874,
      "grad_norm": 75.0,
      "learning_rate": 8.816504384313304e-05,
      "loss": 3.6672,
      "step": 11100
    },
    {
      "epoch": 1.7910688376591972,
      "grad_norm": 24.75,
      "learning_rate": 8.815428479208134e-05,
      "loss": 5.7618,
      "step": 11110
    },
    {
      "epoch": 1.792680960825407,
      "grad_norm": 12.5625,
      "learning_rate": 8.814352574102964e-05,
      "loss": 5.3336,
      "step": 11120
    },
    {
      "epoch": 1.794293083991617,
      "grad_norm": 10.375,
      "learning_rate": 8.813276668997795e-05,
      "loss": 4.9682,
      "step": 11130
    },
    {
      "epoch": 1.7959052071578268,
      "grad_norm": 25.125,
      "learning_rate": 8.812200763892626e-05,
      "loss": 4.4805,
      "step": 11140
    },
    {
      "epoch": 1.7975173303240366,
      "grad_norm": 7.1875,
      "learning_rate": 8.811124858787455e-05,
      "loss": 3.7391,
      "step": 11150
    },
    {
      "epoch": 1.7991294534902467,
      "grad_norm": 21.75,
      "learning_rate": 8.810048953682286e-05,
      "loss": 5.6492,
      "step": 11160
    },
    {
      "epoch": 1.8007415766564565,
      "grad_norm": 8.6875,
      "learning_rate": 8.808973048577116e-05,
      "loss": 3.5391,
      "step": 11170
    },
    {
      "epoch": 1.8023536998226666,
      "grad_norm": 57.75,
      "learning_rate": 8.807897143471946e-05,
      "loss": 3.506,
      "step": 11180
    },
    {
      "epoch": 1.8039658229888764,
      "grad_norm": 13.5625,
      "learning_rate": 8.806821238366777e-05,
      "loss": 2.6063,
      "step": 11190
    },
    {
      "epoch": 1.8055779461550863,
      "grad_norm": 10.875,
      "learning_rate": 8.805745333261606e-05,
      "loss": 2.9691,
      "step": 11200
    },
    {
      "epoch": 1.807190069321296,
      "grad_norm": 13.375,
      "learning_rate": 8.804669428156437e-05,
      "loss": 4.8867,
      "step": 11210
    },
    {
      "epoch": 1.808802192487506,
      "grad_norm": 22.625,
      "learning_rate": 8.803593523051268e-05,
      "loss": 4.8602,
      "step": 11220
    },
    {
      "epoch": 1.8104143156537158,
      "grad_norm": 22.0,
      "learning_rate": 8.802517617946098e-05,
      "loss": 4.5891,
      "step": 11230
    },
    {
      "epoch": 1.8120264388199259,
      "grad_norm": 0.0,
      "learning_rate": 8.801441712840928e-05,
      "loss": 2.5618,
      "step": 11240
    },
    {
      "epoch": 1.8136385619861357,
      "grad_norm": 17.5,
      "learning_rate": 8.800365807735757e-05,
      "loss": 3.5492,
      "step": 11250
    },
    {
      "epoch": 1.8152506851523458,
      "grad_norm": 28.0,
      "learning_rate": 8.799289902630588e-05,
      "loss": 4.4328,
      "step": 11260
    },
    {
      "epoch": 1.8168628083185556,
      "grad_norm": 69.0,
      "learning_rate": 8.798213997525419e-05,
      "loss": 4.0078,
      "step": 11270
    },
    {
      "epoch": 1.8184749314847655,
      "grad_norm": 16.375,
      "learning_rate": 8.797138092420249e-05,
      "loss": 4.6449,
      "step": 11280
    },
    {
      "epoch": 1.8200870546509753,
      "grad_norm": 22.875,
      "learning_rate": 8.79606218731508e-05,
      "loss": 3.3297,
      "step": 11290
    },
    {
      "epoch": 1.8216991778171852,
      "grad_norm": 12.875,
      "learning_rate": 8.79498628220991e-05,
      "loss": 3.3336,
      "step": 11300
    },
    {
      "epoch": 1.823311300983395,
      "grad_norm": 10.875,
      "learning_rate": 8.793910377104739e-05,
      "loss": 4.1102,
      "step": 11310
    },
    {
      "epoch": 1.824923424149605,
      "grad_norm": 6.5,
      "learning_rate": 8.79283447199957e-05,
      "loss": 2.9779,
      "step": 11320
    },
    {
      "epoch": 1.826535547315815,
      "grad_norm": 0.0,
      "learning_rate": 8.7917585668944e-05,
      "loss": 2.64,
      "step": 11330
    },
    {
      "epoch": 1.828147670482025,
      "grad_norm": 17.125,
      "learning_rate": 8.790682661789231e-05,
      "loss": 3.4608,
      "step": 11340
    },
    {
      "epoch": 1.8297597936482348,
      "grad_norm": 30.625,
      "learning_rate": 8.789606756684062e-05,
      "loss": 6.0859,
      "step": 11350
    },
    {
      "epoch": 1.8313719168144447,
      "grad_norm": 15.5,
      "learning_rate": 8.788530851578891e-05,
      "loss": 4.4402,
      "step": 11360
    },
    {
      "epoch": 1.8329840399806545,
      "grad_norm": 9.125,
      "learning_rate": 8.787454946473721e-05,
      "loss": 2.8992,
      "step": 11370
    },
    {
      "epoch": 1.8345961631468644,
      "grad_norm": 15.25,
      "learning_rate": 8.786379041368552e-05,
      "loss": 2.5293,
      "step": 11380
    },
    {
      "epoch": 1.8362082863130742,
      "grad_norm": 23.25,
      "learning_rate": 8.785303136263382e-05,
      "loss": 3.7117,
      "step": 11390
    },
    {
      "epoch": 1.8378204094792843,
      "grad_norm": 15.6875,
      "learning_rate": 8.784227231158211e-05,
      "loss": 3.5984,
      "step": 11400
    },
    {
      "epoch": 1.8394325326454941,
      "grad_norm": 35.5,
      "learning_rate": 8.783151326053042e-05,
      "loss": 5.6238,
      "step": 11410
    },
    {
      "epoch": 1.8410446558117042,
      "grad_norm": 28.125,
      "learning_rate": 8.782075420947873e-05,
      "loss": 4.3203,
      "step": 11420
    },
    {
      "epoch": 1.842656778977914,
      "grad_norm": 11.0,
      "learning_rate": 8.780999515842703e-05,
      "loss": 4.9277,
      "step": 11430
    },
    {
      "epoch": 1.8442689021441239,
      "grad_norm": 41.25,
      "learning_rate": 8.779923610737534e-05,
      "loss": 3.0406,
      "step": 11440
    },
    {
      "epoch": 1.8458810253103337,
      "grad_norm": 12.3125,
      "learning_rate": 8.778847705632364e-05,
      "loss": 2.5645,
      "step": 11450
    },
    {
      "epoch": 1.8474931484765436,
      "grad_norm": 29.875,
      "learning_rate": 8.777771800527193e-05,
      "loss": 4.7906,
      "step": 11460
    },
    {
      "epoch": 1.8491052716427534,
      "grad_norm": 19.0,
      "learning_rate": 8.776695895422024e-05,
      "loss": 4.2883,
      "step": 11470
    },
    {
      "epoch": 1.8507173948089632,
      "grad_norm": 17.625,
      "learning_rate": 8.775619990316854e-05,
      "loss": 3.657,
      "step": 11480
    },
    {
      "epoch": 1.8523295179751733,
      "grad_norm": 49.25,
      "learning_rate": 8.774544085211685e-05,
      "loss": 4.8142,
      "step": 11490
    },
    {
      "epoch": 1.8539416411413832,
      "grad_norm": 9.75,
      "learning_rate": 8.773468180106516e-05,
      "loss": 3.5328,
      "step": 11500
    },
    {
      "epoch": 1.8555537643075932,
      "grad_norm": 10.1875,
      "learning_rate": 8.772392275001346e-05,
      "loss": 4.9164,
      "step": 11510
    },
    {
      "epoch": 1.857165887473803,
      "grad_norm": 16.375,
      "learning_rate": 8.771316369896175e-05,
      "loss": 3.2805,
      "step": 11520
    },
    {
      "epoch": 1.858778010640013,
      "grad_norm": 29.375,
      "learning_rate": 8.770240464791006e-05,
      "loss": 5.9387,
      "step": 11530
    },
    {
      "epoch": 1.8603901338062228,
      "grad_norm": 12.0625,
      "learning_rate": 8.769164559685836e-05,
      "loss": 2.7781,
      "step": 11540
    },
    {
      "epoch": 1.8620022569724326,
      "grad_norm": 15.8125,
      "learning_rate": 8.768088654580666e-05,
      "loss": 4.1484,
      "step": 11550
    },
    {
      "epoch": 1.8636143801386424,
      "grad_norm": 19.125,
      "learning_rate": 8.767012749475497e-05,
      "loss": 4.9008,
      "step": 11560
    },
    {
      "epoch": 1.8652265033048525,
      "grad_norm": 10.625,
      "learning_rate": 8.765936844370328e-05,
      "loss": 3.6153,
      "step": 11570
    },
    {
      "epoch": 1.8668386264710624,
      "grad_norm": 47.25,
      "learning_rate": 8.764860939265157e-05,
      "loss": 3.3579,
      "step": 11580
    },
    {
      "epoch": 1.8684507496372724,
      "grad_norm": 25.125,
      "learning_rate": 8.763785034159987e-05,
      "loss": 3.1195,
      "step": 11590
    },
    {
      "epoch": 1.8700628728034823,
      "grad_norm": 18.5,
      "learning_rate": 8.762709129054818e-05,
      "loss": 5.1188,
      "step": 11600
    },
    {
      "epoch": 1.8716749959696921,
      "grad_norm": 27.125,
      "learning_rate": 8.761633223949648e-05,
      "loss": 3.7133,
      "step": 11610
    },
    {
      "epoch": 1.873287119135902,
      "grad_norm": 23.375,
      "learning_rate": 8.760557318844477e-05,
      "loss": 4.0375,
      "step": 11620
    },
    {
      "epoch": 1.8748992423021118,
      "grad_norm": 20.125,
      "learning_rate": 8.759481413739308e-05,
      "loss": 4.4375,
      "step": 11630
    },
    {
      "epoch": 1.8765113654683216,
      "grad_norm": 56.5,
      "learning_rate": 8.758405508634139e-05,
      "loss": 3.4277,
      "step": 11640
    },
    {
      "epoch": 1.8781234886345317,
      "grad_norm": 11.375,
      "learning_rate": 8.757329603528969e-05,
      "loss": 3.3148,
      "step": 11650
    },
    {
      "epoch": 1.8797356118007416,
      "grad_norm": 12.5,
      "learning_rate": 8.7562536984238e-05,
      "loss": 3.7398,
      "step": 11660
    },
    {
      "epoch": 1.8813477349669516,
      "grad_norm": 9.0625,
      "learning_rate": 8.75517779331863e-05,
      "loss": 3.8094,
      "step": 11670
    },
    {
      "epoch": 1.8829598581331615,
      "grad_norm": 14.9375,
      "learning_rate": 8.754101888213459e-05,
      "loss": 4.1523,
      "step": 11680
    },
    {
      "epoch": 1.8845719812993713,
      "grad_norm": 93.0,
      "learning_rate": 8.75302598310829e-05,
      "loss": 3.2027,
      "step": 11690
    },
    {
      "epoch": 1.8861841044655812,
      "grad_norm": 34.5,
      "learning_rate": 8.75195007800312e-05,
      "loss": 3.3195,
      "step": 11700
    },
    {
      "epoch": 1.887796227631791,
      "grad_norm": 27.5,
      "learning_rate": 8.750874172897951e-05,
      "loss": 4.7797,
      "step": 11710
    },
    {
      "epoch": 1.8894083507980008,
      "grad_norm": 7.625,
      "learning_rate": 8.749798267792782e-05,
      "loss": 3.4188,
      "step": 11720
    },
    {
      "epoch": 1.891020473964211,
      "grad_norm": 37.0,
      "learning_rate": 8.748722362687612e-05,
      "loss": 2.2727,
      "step": 11730
    },
    {
      "epoch": 1.8926325971304208,
      "grad_norm": 54.0,
      "learning_rate": 8.747646457582441e-05,
      "loss": 3.4023,
      "step": 11740
    },
    {
      "epoch": 1.8942447202966308,
      "grad_norm": 27.125,
      "learning_rate": 8.746570552477272e-05,
      "loss": 4.1047,
      "step": 11750
    },
    {
      "epoch": 1.8958568434628407,
      "grad_norm": 16.25,
      "learning_rate": 8.745494647372102e-05,
      "loss": 5.8008,
      "step": 11760
    },
    {
      "epoch": 1.8974689666290505,
      "grad_norm": 22.75,
      "learning_rate": 8.744418742266932e-05,
      "loss": 4.8359,
      "step": 11770
    },
    {
      "epoch": 1.8990810897952604,
      "grad_norm": 16.625,
      "learning_rate": 8.743342837161763e-05,
      "loss": 3.3789,
      "step": 11780
    },
    {
      "epoch": 1.9006932129614702,
      "grad_norm": 20.5,
      "learning_rate": 8.742266932056594e-05,
      "loss": 4.7984,
      "step": 11790
    },
    {
      "epoch": 1.90230533612768,
      "grad_norm": 9.4375,
      "learning_rate": 8.741191026951423e-05,
      "loss": 3.1953,
      "step": 11800
    },
    {
      "epoch": 1.90391745929389,
      "grad_norm": 7.3125,
      "learning_rate": 8.740115121846254e-05,
      "loss": 4.8891,
      "step": 11810
    },
    {
      "epoch": 1.9055295824601,
      "grad_norm": 45.75,
      "learning_rate": 8.739039216741084e-05,
      "loss": 3.2164,
      "step": 11820
    },
    {
      "epoch": 1.9071417056263098,
      "grad_norm": 61.25,
      "learning_rate": 8.737963311635913e-05,
      "loss": 2.8687,
      "step": 11830
    },
    {
      "epoch": 1.9087538287925199,
      "grad_norm": 17.375,
      "learning_rate": 8.736887406530744e-05,
      "loss": 4.1199,
      "step": 11840
    },
    {
      "epoch": 1.9103659519587297,
      "grad_norm": 42.0,
      "learning_rate": 8.735811501425574e-05,
      "loss": 3.648,
      "step": 11850
    },
    {
      "epoch": 1.9119780751249396,
      "grad_norm": 20.375,
      "learning_rate": 8.734735596320405e-05,
      "loss": 4.309,
      "step": 11860
    },
    {
      "epoch": 1.9135901982911494,
      "grad_norm": 33.5,
      "learning_rate": 8.733659691215236e-05,
      "loss": 4.0051,
      "step": 11870
    },
    {
      "epoch": 1.9152023214573592,
      "grad_norm": 80.5,
      "learning_rate": 8.732583786110066e-05,
      "loss": 3.6828,
      "step": 11880
    },
    {
      "epoch": 1.916814444623569,
      "grad_norm": 48.5,
      "learning_rate": 8.731507881004895e-05,
      "loss": 3.9062,
      "step": 11890
    },
    {
      "epoch": 1.9184265677897792,
      "grad_norm": 7.21875,
      "learning_rate": 8.730431975899726e-05,
      "loss": 3.8395,
      "step": 11900
    },
    {
      "epoch": 1.920038690955989,
      "grad_norm": 13.1875,
      "learning_rate": 8.729356070794556e-05,
      "loss": 3.1437,
      "step": 11910
    },
    {
      "epoch": 1.921650814122199,
      "grad_norm": 15.375,
      "learning_rate": 8.728280165689387e-05,
      "loss": 2.4648,
      "step": 11920
    },
    {
      "epoch": 1.923262937288409,
      "grad_norm": 11.625,
      "learning_rate": 8.727204260584217e-05,
      "loss": 2.7914,
      "step": 11930
    },
    {
      "epoch": 1.9248750604546188,
      "grad_norm": 18.75,
      "learning_rate": 8.726128355479048e-05,
      "loss": 5.4594,
      "step": 11940
    },
    {
      "epoch": 1.9264871836208286,
      "grad_norm": 30.75,
      "learning_rate": 8.725052450373877e-05,
      "loss": 3.0113,
      "step": 11950
    },
    {
      "epoch": 1.9280993067870384,
      "grad_norm": 13.5,
      "learning_rate": 8.723976545268707e-05,
      "loss": 4.0414,
      "step": 11960
    },
    {
      "epoch": 1.9297114299532483,
      "grad_norm": 15.8125,
      "learning_rate": 8.722900640163538e-05,
      "loss": 4.5586,
      "step": 11970
    },
    {
      "epoch": 1.9313235531194584,
      "grad_norm": 75.0,
      "learning_rate": 8.721824735058368e-05,
      "loss": 4.0031,
      "step": 11980
    },
    {
      "epoch": 1.9329356762856682,
      "grad_norm": 9.0,
      "learning_rate": 8.720748829953199e-05,
      "loss": 2.8344,
      "step": 11990
    },
    {
      "epoch": 1.9345477994518783,
      "grad_norm": 24.375,
      "learning_rate": 8.71967292484803e-05,
      "loss": 4.2258,
      "step": 12000
    },
    {
      "epoch": 1.9361599226180881,
      "grad_norm": 16.875,
      "learning_rate": 8.71859701974286e-05,
      "loss": 2.9633,
      "step": 12010
    },
    {
      "epoch": 1.937772045784298,
      "grad_norm": 17.375,
      "learning_rate": 8.717521114637689e-05,
      "loss": 2.8813,
      "step": 12020
    },
    {
      "epoch": 1.9393841689505078,
      "grad_norm": 0.0,
      "learning_rate": 8.71644520953252e-05,
      "loss": 4.0313,
      "step": 12030
    },
    {
      "epoch": 1.9409962921167176,
      "grad_norm": 8.0,
      "learning_rate": 8.71536930442735e-05,
      "loss": 2.8906,
      "step": 12040
    },
    {
      "epoch": 1.9426084152829275,
      "grad_norm": 37.75,
      "learning_rate": 8.71429339932218e-05,
      "loss": 4.1914,
      "step": 12050
    },
    {
      "epoch": 1.9442205384491376,
      "grad_norm": 23.25,
      "learning_rate": 8.71321749421701e-05,
      "loss": 2.5323,
      "step": 12060
    },
    {
      "epoch": 1.9458326616153474,
      "grad_norm": 11.0,
      "learning_rate": 8.712141589111841e-05,
      "loss": 5.8922,
      "step": 12070
    },
    {
      "epoch": 1.9474447847815575,
      "grad_norm": 13.375,
      "learning_rate": 8.711065684006671e-05,
      "loss": 4.3484,
      "step": 12080
    },
    {
      "epoch": 1.9490569079477673,
      "grad_norm": 19.875,
      "learning_rate": 8.709989778901502e-05,
      "loss": 2.7016,
      "step": 12090
    },
    {
      "epoch": 1.9506690311139772,
      "grad_norm": 20.0,
      "learning_rate": 8.708913873796332e-05,
      "loss": 6.025,
      "step": 12100
    },
    {
      "epoch": 1.952281154280187,
      "grad_norm": 82.5,
      "learning_rate": 8.707837968691161e-05,
      "loss": 3.622,
      "step": 12110
    },
    {
      "epoch": 1.9538932774463968,
      "grad_norm": 5.875,
      "learning_rate": 8.706762063585992e-05,
      "loss": 3.7469,
      "step": 12120
    },
    {
      "epoch": 1.9555054006126067,
      "grad_norm": 11.8125,
      "learning_rate": 8.705686158480822e-05,
      "loss": 3.2687,
      "step": 12130
    },
    {
      "epoch": 1.9571175237788165,
      "grad_norm": 17.875,
      "learning_rate": 8.704610253375653e-05,
      "loss": 2.6812,
      "step": 12140
    },
    {
      "epoch": 1.9587296469450266,
      "grad_norm": 11.875,
      "learning_rate": 8.703534348270484e-05,
      "loss": 5.1188,
      "step": 12150
    },
    {
      "epoch": 1.9603417701112364,
      "grad_norm": 35.0,
      "learning_rate": 8.702458443165314e-05,
      "loss": 3.3312,
      "step": 12160
    },
    {
      "epoch": 1.9619538932774465,
      "grad_norm": 47.25,
      "learning_rate": 8.701382538060143e-05,
      "loss": 3.0438,
      "step": 12170
    },
    {
      "epoch": 1.9635660164436564,
      "grad_norm": 11.125,
      "learning_rate": 8.700306632954974e-05,
      "loss": 4.4516,
      "step": 12180
    },
    {
      "epoch": 1.9651781396098662,
      "grad_norm": 19.75,
      "learning_rate": 8.699230727849804e-05,
      "loss": 4.3246,
      "step": 12190
    },
    {
      "epoch": 1.966790262776076,
      "grad_norm": 29.0,
      "learning_rate": 8.698154822744634e-05,
      "loss": 4.1928,
      "step": 12200
    },
    {
      "epoch": 1.968402385942286,
      "grad_norm": 19.5,
      "learning_rate": 8.697078917639465e-05,
      "loss": 3.9391,
      "step": 12210
    },
    {
      "epoch": 1.9700145091084957,
      "grad_norm": 34.25,
      "learning_rate": 8.696003012534296e-05,
      "loss": 4.182,
      "step": 12220
    },
    {
      "epoch": 1.9716266322747058,
      "grad_norm": 33.0,
      "learning_rate": 8.694927107429125e-05,
      "loss": 2.9248,
      "step": 12230
    },
    {
      "epoch": 1.9732387554409156,
      "grad_norm": 15.125,
      "learning_rate": 8.693851202323955e-05,
      "loss": 5.2016,
      "step": 12240
    },
    {
      "epoch": 1.9748508786071257,
      "grad_norm": 29.5,
      "learning_rate": 8.692775297218786e-05,
      "loss": 3.532,
      "step": 12250
    },
    {
      "epoch": 1.9764630017733356,
      "grad_norm": 23.75,
      "learning_rate": 8.691699392113616e-05,
      "loss": 4.1449,
      "step": 12260
    },
    {
      "epoch": 1.9780751249395454,
      "grad_norm": 16.5,
      "learning_rate": 8.690623487008445e-05,
      "loss": 4.3602,
      "step": 12270
    },
    {
      "epoch": 1.9796872481057552,
      "grad_norm": 28.25,
      "learning_rate": 8.689547581903276e-05,
      "loss": 3.5859,
      "step": 12280
    },
    {
      "epoch": 1.981299371271965,
      "grad_norm": 41.5,
      "learning_rate": 8.688471676798107e-05,
      "loss": 4.0652,
      "step": 12290
    },
    {
      "epoch": 1.982911494438175,
      "grad_norm": 54.0,
      "learning_rate": 8.687395771692937e-05,
      "loss": 5.0289,
      "step": 12300
    },
    {
      "epoch": 1.984523617604385,
      "grad_norm": 35.0,
      "learning_rate": 8.686319866587768e-05,
      "loss": 3.2383,
      "step": 12310
    },
    {
      "epoch": 1.9861357407705948,
      "grad_norm": 15.6875,
      "learning_rate": 8.685243961482598e-05,
      "loss": 2.1672,
      "step": 12320
    },
    {
      "epoch": 1.987747863936805,
      "grad_norm": 15.125,
      "learning_rate": 8.684168056377427e-05,
      "loss": 5.2859,
      "step": 12330
    },
    {
      "epoch": 1.9893599871030148,
      "grad_norm": 26.625,
      "learning_rate": 8.683092151272258e-05,
      "loss": 3.1906,
      "step": 12340
    },
    {
      "epoch": 1.9909721102692246,
      "grad_norm": 20.875,
      "learning_rate": 8.682016246167088e-05,
      "loss": 2.8172,
      "step": 12350
    },
    {
      "epoch": 1.9925842334354344,
      "grad_norm": 16.0,
      "learning_rate": 8.680940341061919e-05,
      "loss": 4.2221,
      "step": 12360
    },
    {
      "epoch": 1.9941963566016443,
      "grad_norm": 9.0,
      "learning_rate": 8.67986443595675e-05,
      "loss": 3.5586,
      "step": 12370
    },
    {
      "epoch": 1.9958084797678541,
      "grad_norm": 14.3125,
      "learning_rate": 8.67878853085158e-05,
      "loss": 3.2516,
      "step": 12380
    },
    {
      "epoch": 1.9974206029340642,
      "grad_norm": 40.0,
      "learning_rate": 8.677712625746409e-05,
      "loss": 3.9879,
      "step": 12390
    },
    {
      "epoch": 1.999032726100274,
      "grad_norm": 14.875,
      "learning_rate": 8.67663672064124e-05,
      "loss": 2.2164,
      "step": 12400
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.7485547065734863,
      "eval_runtime": 23.9771,
      "eval_samples_per_second": 4.171,
      "eval_steps_per_second": 4.171,
      "step": 12406
    },
    {
      "epoch": 2.000644849266484,
      "grad_norm": 11.8125,
      "learning_rate": 8.67556081553607e-05,
      "loss": 3.4156,
      "step": 12410
    },
    {
      "epoch": 2.002256972432694,
      "grad_norm": 38.25,
      "learning_rate": 8.6744849104309e-05,
      "loss": 3.2891,
      "step": 12420
    },
    {
      "epoch": 2.003869095598904,
      "grad_norm": 13.5,
      "learning_rate": 8.67340900532573e-05,
      "loss": 4.7686,
      "step": 12430
    },
    {
      "epoch": 2.0054812187651136,
      "grad_norm": 16.375,
      "learning_rate": 8.672333100220561e-05,
      "loss": 3.1133,
      "step": 12440
    },
    {
      "epoch": 2.0070933419313235,
      "grad_norm": 40.75,
      "learning_rate": 8.671257195115391e-05,
      "loss": 4.9844,
      "step": 12450
    },
    {
      "epoch": 2.0087054650975333,
      "grad_norm": 8.1875,
      "learning_rate": 8.670181290010222e-05,
      "loss": 2.7086,
      "step": 12460
    },
    {
      "epoch": 2.010317588263743,
      "grad_norm": 54.75,
      "learning_rate": 8.669105384905052e-05,
      "loss": 2.8641,
      "step": 12470
    },
    {
      "epoch": 2.0119297114299535,
      "grad_norm": 15.375,
      "learning_rate": 8.668029479799881e-05,
      "loss": 2.7721,
      "step": 12480
    },
    {
      "epoch": 2.0135418345961633,
      "grad_norm": 64.5,
      "learning_rate": 8.666953574694712e-05,
      "loss": 4.7172,
      "step": 12490
    },
    {
      "epoch": 2.015153957762373,
      "grad_norm": 7.3125,
      "learning_rate": 8.665877669589542e-05,
      "loss": 3.666,
      "step": 12500
    },
    {
      "epoch": 2.016766080928583,
      "grad_norm": 10.5625,
      "learning_rate": 8.664801764484373e-05,
      "loss": 4.2352,
      "step": 12510
    },
    {
      "epoch": 2.018378204094793,
      "grad_norm": 25.625,
      "learning_rate": 8.663725859379204e-05,
      "loss": 3.1234,
      "step": 12520
    },
    {
      "epoch": 2.0199903272610027,
      "grad_norm": 13.625,
      "learning_rate": 8.662649954274034e-05,
      "loss": 2.1836,
      "step": 12530
    },
    {
      "epoch": 2.0216024504272125,
      "grad_norm": 9.375,
      "learning_rate": 8.661574049168863e-05,
      "loss": 2.0508,
      "step": 12540
    },
    {
      "epoch": 2.0232145735934224,
      "grad_norm": 12.375,
      "learning_rate": 8.660498144063694e-05,
      "loss": 2.0234,
      "step": 12550
    },
    {
      "epoch": 2.0248266967596322,
      "grad_norm": 27.25,
      "learning_rate": 8.659422238958524e-05,
      "loss": 1.9762,
      "step": 12560
    },
    {
      "epoch": 2.0264388199258425,
      "grad_norm": 10.75,
      "learning_rate": 8.658346333853354e-05,
      "loss": 4.273,
      "step": 12570
    },
    {
      "epoch": 2.0280509430920524,
      "grad_norm": 20.875,
      "learning_rate": 8.657270428748185e-05,
      "loss": 2.0248,
      "step": 12580
    },
    {
      "epoch": 2.029663066258262,
      "grad_norm": 42.0,
      "learning_rate": 8.656194523643016e-05,
      "loss": 4.5227,
      "step": 12590
    },
    {
      "epoch": 2.031275189424472,
      "grad_norm": 16.125,
      "learning_rate": 8.655118618537845e-05,
      "loss": 2.9008,
      "step": 12600
    },
    {
      "epoch": 2.032887312590682,
      "grad_norm": 13.1875,
      "learning_rate": 8.654042713432675e-05,
      "loss": 2.3551,
      "step": 12610
    },
    {
      "epoch": 2.0344994357568917,
      "grad_norm": 12.0,
      "learning_rate": 8.652966808327506e-05,
      "loss": 3.4641,
      "step": 12620
    },
    {
      "epoch": 2.0361115589231016,
      "grad_norm": 49.25,
      "learning_rate": 8.651890903222336e-05,
      "loss": 3.3831,
      "step": 12630
    },
    {
      "epoch": 2.0377236820893114,
      "grad_norm": 19.25,
      "learning_rate": 8.650814998117167e-05,
      "loss": 4.1402,
      "step": 12640
    },
    {
      "epoch": 2.0393358052555217,
      "grad_norm": 61.25,
      "learning_rate": 8.649739093011998e-05,
      "loss": 4.0711,
      "step": 12650
    },
    {
      "epoch": 2.0409479284217316,
      "grad_norm": 32.25,
      "learning_rate": 8.648663187906827e-05,
      "loss": 3.3816,
      "step": 12660
    },
    {
      "epoch": 2.0425600515879414,
      "grad_norm": 0.0,
      "learning_rate": 8.647587282801657e-05,
      "loss": 3.2093,
      "step": 12670
    },
    {
      "epoch": 2.0441721747541512,
      "grad_norm": 23.875,
      "learning_rate": 8.646511377696488e-05,
      "loss": 4.2078,
      "step": 12680
    },
    {
      "epoch": 2.045784297920361,
      "grad_norm": 16.25,
      "learning_rate": 8.645435472591318e-05,
      "loss": 2.6104,
      "step": 12690
    },
    {
      "epoch": 2.047396421086571,
      "grad_norm": 204.0,
      "learning_rate": 8.644359567486147e-05,
      "loss": 3.3191,
      "step": 12700
    },
    {
      "epoch": 2.049008544252781,
      "grad_norm": 13.0,
      "learning_rate": 8.643283662380978e-05,
      "loss": 2.4488,
      "step": 12710
    },
    {
      "epoch": 2.0506206674189906,
      "grad_norm": 34.75,
      "learning_rate": 8.642207757275809e-05,
      "loss": 3.6992,
      "step": 12720
    },
    {
      "epoch": 2.052232790585201,
      "grad_norm": 13.25,
      "learning_rate": 8.641131852170639e-05,
      "loss": 4.5086,
      "step": 12730
    },
    {
      "epoch": 2.0538449137514108,
      "grad_norm": 15.4375,
      "learning_rate": 8.64005594706547e-05,
      "loss": 2.9328,
      "step": 12740
    },
    {
      "epoch": 2.0554570369176206,
      "grad_norm": 61.25,
      "learning_rate": 8.6389800419603e-05,
      "loss": 2.9648,
      "step": 12750
    },
    {
      "epoch": 2.0570691600838305,
      "grad_norm": 16.125,
      "learning_rate": 8.637904136855129e-05,
      "loss": 3.1234,
      "step": 12760
    },
    {
      "epoch": 2.0586812832500403,
      "grad_norm": 0.0,
      "learning_rate": 8.63682823174996e-05,
      "loss": 2.4372,
      "step": 12770
    },
    {
      "epoch": 2.06029340641625,
      "grad_norm": 12.9375,
      "learning_rate": 8.63575232664479e-05,
      "loss": 4.375,
      "step": 12780
    },
    {
      "epoch": 2.06190552958246,
      "grad_norm": 18.5,
      "learning_rate": 8.634676421539621e-05,
      "loss": 3.3188,
      "step": 12790
    },
    {
      "epoch": 2.06351765274867,
      "grad_norm": 109.5,
      "learning_rate": 8.633600516434452e-05,
      "loss": 3.2934,
      "step": 12800
    },
    {
      "epoch": 2.0651297759148797,
      "grad_norm": 43.75,
      "learning_rate": 8.632524611329282e-05,
      "loss": 2.7973,
      "step": 12810
    },
    {
      "epoch": 2.06674189908109,
      "grad_norm": 0.0,
      "learning_rate": 8.631448706224111e-05,
      "loss": 3.5612,
      "step": 12820
    },
    {
      "epoch": 2.0683540222473,
      "grad_norm": 16.5,
      "learning_rate": 8.630372801118942e-05,
      "loss": 2.8805,
      "step": 12830
    },
    {
      "epoch": 2.0699661454135097,
      "grad_norm": 12.125,
      "learning_rate": 8.629296896013772e-05,
      "loss": 2.7992,
      "step": 12840
    },
    {
      "epoch": 2.0715782685797195,
      "grad_norm": 15.375,
      "learning_rate": 8.628220990908602e-05,
      "loss": 3.6539,
      "step": 12850
    },
    {
      "epoch": 2.0731903917459293,
      "grad_norm": 23.5,
      "learning_rate": 8.627145085803433e-05,
      "loss": 2.5836,
      "step": 12860
    },
    {
      "epoch": 2.074802514912139,
      "grad_norm": 68.0,
      "learning_rate": 8.626069180698264e-05,
      "loss": 3.3504,
      "step": 12870
    },
    {
      "epoch": 2.076414638078349,
      "grad_norm": 34.0,
      "learning_rate": 8.624993275593093e-05,
      "loss": 3.1852,
      "step": 12880
    },
    {
      "epoch": 2.078026761244559,
      "grad_norm": 29.25,
      "learning_rate": 8.623917370487924e-05,
      "loss": 2.6629,
      "step": 12890
    },
    {
      "epoch": 2.079638884410769,
      "grad_norm": 35.0,
      "learning_rate": 8.622841465382754e-05,
      "loss": 5.2711,
      "step": 12900
    },
    {
      "epoch": 2.081251007576979,
      "grad_norm": 60.0,
      "learning_rate": 8.621765560277583e-05,
      "loss": 5.1102,
      "step": 12910
    },
    {
      "epoch": 2.082863130743189,
      "grad_norm": 8.375,
      "learning_rate": 8.620689655172413e-05,
      "loss": 3.9191,
      "step": 12920
    },
    {
      "epoch": 2.0844752539093987,
      "grad_norm": 53.5,
      "learning_rate": 8.619613750067244e-05,
      "loss": 2.9648,
      "step": 12930
    },
    {
      "epoch": 2.0860873770756085,
      "grad_norm": 13.375,
      "learning_rate": 8.618537844962075e-05,
      "loss": 2.2055,
      "step": 12940
    },
    {
      "epoch": 2.0876995002418184,
      "grad_norm": 22.875,
      "learning_rate": 8.617461939856905e-05,
      "loss": 3.8578,
      "step": 12950
    },
    {
      "epoch": 2.0893116234080282,
      "grad_norm": 16.375,
      "learning_rate": 8.616386034751736e-05,
      "loss": 2.098,
      "step": 12960
    },
    {
      "epoch": 2.090923746574238,
      "grad_norm": 43.5,
      "learning_rate": 8.615310129646565e-05,
      "loss": 3.0844,
      "step": 12970
    },
    {
      "epoch": 2.0925358697404484,
      "grad_norm": 10.25,
      "learning_rate": 8.614234224541395e-05,
      "loss": 2.2259,
      "step": 12980
    },
    {
      "epoch": 2.094147992906658,
      "grad_norm": 14.5,
      "learning_rate": 8.613158319436226e-05,
      "loss": 2.8773,
      "step": 12990
    },
    {
      "epoch": 2.095760116072868,
      "grad_norm": 65.0,
      "learning_rate": 8.612082414331056e-05,
      "loss": 3.9531,
      "step": 13000
    },
    {
      "epoch": 2.097372239239078,
      "grad_norm": 27.125,
      "learning_rate": 8.611006509225887e-05,
      "loss": 3.7242,
      "step": 13010
    },
    {
      "epoch": 2.0989843624052877,
      "grad_norm": 59.75,
      "learning_rate": 8.609930604120718e-05,
      "loss": 3.2812,
      "step": 13020
    },
    {
      "epoch": 2.1005964855714976,
      "grad_norm": 13.125,
      "learning_rate": 8.608854699015547e-05,
      "loss": 4.1934,
      "step": 13030
    },
    {
      "epoch": 2.1022086087377074,
      "grad_norm": 66.0,
      "learning_rate": 8.607778793910377e-05,
      "loss": 3.7852,
      "step": 13040
    },
    {
      "epoch": 2.1038207319039173,
      "grad_norm": 37.75,
      "learning_rate": 8.606702888805208e-05,
      "loss": 3.4349,
      "step": 13050
    },
    {
      "epoch": 2.1054328550701276,
      "grad_norm": 12.4375,
      "learning_rate": 8.605626983700038e-05,
      "loss": 2.5285,
      "step": 13060
    },
    {
      "epoch": 2.1070449782363374,
      "grad_norm": 0.0,
      "learning_rate": 8.604551078594867e-05,
      "loss": 3.5301,
      "step": 13070
    },
    {
      "epoch": 2.1086571014025473,
      "grad_norm": 19.75,
      "learning_rate": 8.603475173489698e-05,
      "loss": 3.682,
      "step": 13080
    },
    {
      "epoch": 2.110269224568757,
      "grad_norm": 23.5,
      "learning_rate": 8.60239926838453e-05,
      "loss": 3.7578,
      "step": 13090
    },
    {
      "epoch": 2.111881347734967,
      "grad_norm": 28.0,
      "learning_rate": 8.601323363279359e-05,
      "loss": 3.693,
      "step": 13100
    },
    {
      "epoch": 2.113493470901177,
      "grad_norm": 14.1875,
      "learning_rate": 8.60024745817419e-05,
      "loss": 3.8076,
      "step": 13110
    },
    {
      "epoch": 2.1151055940673866,
      "grad_norm": 125.5,
      "learning_rate": 8.59917155306902e-05,
      "loss": 2.3827,
      "step": 13120
    },
    {
      "epoch": 2.1167177172335965,
      "grad_norm": 11.0,
      "learning_rate": 8.59809564796385e-05,
      "loss": 3.7039,
      "step": 13130
    },
    {
      "epoch": 2.1183298403998068,
      "grad_norm": 50.0,
      "learning_rate": 8.59701974285868e-05,
      "loss": 3.2875,
      "step": 13140
    },
    {
      "epoch": 2.1199419635660166,
      "grad_norm": 14.375,
      "learning_rate": 8.59594383775351e-05,
      "loss": 3.0219,
      "step": 13150
    },
    {
      "epoch": 2.1215540867322265,
      "grad_norm": 27.125,
      "learning_rate": 8.594867932648341e-05,
      "loss": 2.7812,
      "step": 13160
    },
    {
      "epoch": 2.1231662098984363,
      "grad_norm": 10.3125,
      "learning_rate": 8.593792027543172e-05,
      "loss": 2.7948,
      "step": 13170
    },
    {
      "epoch": 2.124778333064646,
      "grad_norm": 16.625,
      "learning_rate": 8.592716122438002e-05,
      "loss": 2.4442,
      "step": 13180
    },
    {
      "epoch": 2.126390456230856,
      "grad_norm": 18.25,
      "learning_rate": 8.591640217332831e-05,
      "loss": 4.5328,
      "step": 13190
    },
    {
      "epoch": 2.128002579397066,
      "grad_norm": 27.375,
      "learning_rate": 8.590564312227662e-05,
      "loss": 2.7676,
      "step": 13200
    },
    {
      "epoch": 2.1296147025632757,
      "grad_norm": 59.5,
      "learning_rate": 8.589488407122492e-05,
      "loss": 3.3617,
      "step": 13210
    },
    {
      "epoch": 2.1312268257294855,
      "grad_norm": 57.75,
      "learning_rate": 8.588412502017322e-05,
      "loss": 3.2059,
      "step": 13220
    },
    {
      "epoch": 2.132838948895696,
      "grad_norm": 21.375,
      "learning_rate": 8.587336596912154e-05,
      "loss": 2.857,
      "step": 13230
    },
    {
      "epoch": 2.1344510720619057,
      "grad_norm": 10.125,
      "learning_rate": 8.586260691806984e-05,
      "loss": 1.9539,
      "step": 13240
    },
    {
      "epoch": 2.1360631952281155,
      "grad_norm": 129.0,
      "learning_rate": 8.585184786701813e-05,
      "loss": 3.7766,
      "step": 13250
    },
    {
      "epoch": 2.1376753183943253,
      "grad_norm": 93.0,
      "learning_rate": 8.584108881596643e-05,
      "loss": 3.7656,
      "step": 13260
    },
    {
      "epoch": 2.139287441560535,
      "grad_norm": 35.0,
      "learning_rate": 8.583032976491474e-05,
      "loss": 5.2461,
      "step": 13270
    },
    {
      "epoch": 2.140899564726745,
      "grad_norm": 17.25,
      "learning_rate": 8.581957071386304e-05,
      "loss": 3.1658,
      "step": 13280
    },
    {
      "epoch": 2.142511687892955,
      "grad_norm": 17.75,
      "learning_rate": 8.580881166281135e-05,
      "loss": 3.8929,
      "step": 13290
    },
    {
      "epoch": 2.1441238110591647,
      "grad_norm": 38.5,
      "learning_rate": 8.579805261175966e-05,
      "loss": 3.377,
      "step": 13300
    },
    {
      "epoch": 2.145735934225375,
      "grad_norm": 9.5625,
      "learning_rate": 8.578729356070795e-05,
      "loss": 3.0207,
      "step": 13310
    },
    {
      "epoch": 2.147348057391585,
      "grad_norm": 33.75,
      "learning_rate": 8.577653450965625e-05,
      "loss": 4.9266,
      "step": 13320
    },
    {
      "epoch": 2.1489601805577947,
      "grad_norm": 8.0625,
      "learning_rate": 8.576577545860456e-05,
      "loss": 3.0086,
      "step": 13330
    },
    {
      "epoch": 2.1505723037240045,
      "grad_norm": 47.75,
      "learning_rate": 8.575501640755286e-05,
      "loss": 3.3195,
      "step": 13340
    },
    {
      "epoch": 2.1521844268902144,
      "grad_norm": 12.1875,
      "learning_rate": 8.574425735650115e-05,
      "loss": 2.6465,
      "step": 13350
    },
    {
      "epoch": 2.1537965500564242,
      "grad_norm": 25.5,
      "learning_rate": 8.573349830544946e-05,
      "loss": 4.0678,
      "step": 13360
    },
    {
      "epoch": 2.155408673222634,
      "grad_norm": 33.75,
      "learning_rate": 8.572273925439777e-05,
      "loss": 3.2875,
      "step": 13370
    },
    {
      "epoch": 2.157020796388844,
      "grad_norm": 34.5,
      "learning_rate": 8.571198020334607e-05,
      "loss": 2.3801,
      "step": 13380
    },
    {
      "epoch": 2.158632919555054,
      "grad_norm": 46.5,
      "learning_rate": 8.570122115229438e-05,
      "loss": 4.4836,
      "step": 13390
    },
    {
      "epoch": 2.160245042721264,
      "grad_norm": 15.0,
      "learning_rate": 8.569046210124268e-05,
      "loss": 2.5437,
      "step": 13400
    },
    {
      "epoch": 2.161857165887474,
      "grad_norm": 9.3125,
      "learning_rate": 8.567970305019097e-05,
      "loss": 2.6422,
      "step": 13410
    },
    {
      "epoch": 2.1634692890536837,
      "grad_norm": 12.375,
      "learning_rate": 8.566894399913928e-05,
      "loss": 4.0164,
      "step": 13420
    },
    {
      "epoch": 2.1650814122198936,
      "grad_norm": 31.125,
      "learning_rate": 8.565818494808758e-05,
      "loss": 3.782,
      "step": 13430
    },
    {
      "epoch": 2.1666935353861034,
      "grad_norm": 21.25,
      "learning_rate": 8.564742589703589e-05,
      "loss": 3.0727,
      "step": 13440
    },
    {
      "epoch": 2.1683056585523133,
      "grad_norm": 24.5,
      "learning_rate": 8.56366668459842e-05,
      "loss": 2.3524,
      "step": 13450
    },
    {
      "epoch": 2.169917781718523,
      "grad_norm": 28.0,
      "learning_rate": 8.56259077949325e-05,
      "loss": 3.4277,
      "step": 13460
    },
    {
      "epoch": 2.171529904884733,
      "grad_norm": 35.5,
      "learning_rate": 8.561514874388079e-05,
      "loss": 2.423,
      "step": 13470
    },
    {
      "epoch": 2.1731420280509433,
      "grad_norm": 49.75,
      "learning_rate": 8.56043896928291e-05,
      "loss": 3.5148,
      "step": 13480
    },
    {
      "epoch": 2.174754151217153,
      "grad_norm": 19.0,
      "learning_rate": 8.55936306417774e-05,
      "loss": 3.3523,
      "step": 13490
    },
    {
      "epoch": 2.176366274383363,
      "grad_norm": 35.25,
      "learning_rate": 8.55828715907257e-05,
      "loss": 2.7262,
      "step": 13500
    },
    {
      "epoch": 2.177978397549573,
      "grad_norm": 25.875,
      "learning_rate": 8.5572112539674e-05,
      "loss": 2.7437,
      "step": 13510
    },
    {
      "epoch": 2.1795905207157826,
      "grad_norm": 22.875,
      "learning_rate": 8.556135348862231e-05,
      "loss": 2.4184,
      "step": 13520
    },
    {
      "epoch": 2.1812026438819925,
      "grad_norm": 13.75,
      "learning_rate": 8.555059443757061e-05,
      "loss": 3.1906,
      "step": 13530
    },
    {
      "epoch": 2.1828147670482023,
      "grad_norm": 20.25,
      "learning_rate": 8.553983538651892e-05,
      "loss": 3.507,
      "step": 13540
    },
    {
      "epoch": 2.1844268902144126,
      "grad_norm": 22.125,
      "learning_rate": 8.552907633546722e-05,
      "loss": 3.3148,
      "step": 13550
    },
    {
      "epoch": 2.1860390133806225,
      "grad_norm": 15.4375,
      "learning_rate": 8.551831728441551e-05,
      "loss": 3.5789,
      "step": 13560
    },
    {
      "epoch": 2.1876511365468323,
      "grad_norm": 19.5,
      "learning_rate": 8.550755823336381e-05,
      "loss": 4.3977,
      "step": 13570
    },
    {
      "epoch": 2.189263259713042,
      "grad_norm": 33.75,
      "learning_rate": 8.549679918231212e-05,
      "loss": 2.7648,
      "step": 13580
    },
    {
      "epoch": 2.190875382879252,
      "grad_norm": 14.8125,
      "learning_rate": 8.548604013126043e-05,
      "loss": 2.6336,
      "step": 13590
    },
    {
      "epoch": 2.192487506045462,
      "grad_norm": 70.0,
      "learning_rate": 8.547528108020873e-05,
      "loss": 3.0063,
      "step": 13600
    },
    {
      "epoch": 2.1940996292116717,
      "grad_norm": 18.75,
      "learning_rate": 8.546452202915704e-05,
      "loss": 2.0898,
      "step": 13610
    },
    {
      "epoch": 2.1957117523778815,
      "grad_norm": 76.5,
      "learning_rate": 8.545376297810533e-05,
      "loss": 2.7883,
      "step": 13620
    },
    {
      "epoch": 2.1973238755440914,
      "grad_norm": 61.25,
      "learning_rate": 8.544300392705363e-05,
      "loss": 3.7492,
      "step": 13630
    },
    {
      "epoch": 2.1989359987103017,
      "grad_norm": 18.625,
      "learning_rate": 8.543224487600194e-05,
      "loss": 3.2322,
      "step": 13640
    },
    {
      "epoch": 2.2005481218765115,
      "grad_norm": 13.8125,
      "learning_rate": 8.542148582495024e-05,
      "loss": 3.0004,
      "step": 13650
    },
    {
      "epoch": 2.2021602450427213,
      "grad_norm": 0.0,
      "learning_rate": 8.541072677389855e-05,
      "loss": 4.1388,
      "step": 13660
    },
    {
      "epoch": 2.203772368208931,
      "grad_norm": 27.0,
      "learning_rate": 8.539996772284686e-05,
      "loss": 2.7645,
      "step": 13670
    },
    {
      "epoch": 2.205384491375141,
      "grad_norm": 47.0,
      "learning_rate": 8.538920867179515e-05,
      "loss": 3.8781,
      "step": 13680
    },
    {
      "epoch": 2.206996614541351,
      "grad_norm": 23.0,
      "learning_rate": 8.537844962074345e-05,
      "loss": 2.066,
      "step": 13690
    },
    {
      "epoch": 2.2086087377075607,
      "grad_norm": 23.25,
      "learning_rate": 8.536769056969176e-05,
      "loss": 3.3203,
      "step": 13700
    },
    {
      "epoch": 2.2102208608737706,
      "grad_norm": 42.75,
      "learning_rate": 8.535693151864006e-05,
      "loss": 3.0125,
      "step": 13710
    },
    {
      "epoch": 2.211832984039981,
      "grad_norm": 23.5,
      "learning_rate": 8.534617246758835e-05,
      "loss": 2.2059,
      "step": 13720
    },
    {
      "epoch": 2.2134451072061907,
      "grad_norm": 10.4375,
      "learning_rate": 8.533541341653666e-05,
      "loss": 3.3984,
      "step": 13730
    },
    {
      "epoch": 2.2150572303724005,
      "grad_norm": 12.4375,
      "learning_rate": 8.532465436548497e-05,
      "loss": 2.7523,
      "step": 13740
    },
    {
      "epoch": 2.2166693535386104,
      "grad_norm": 16.0,
      "learning_rate": 8.531389531443327e-05,
      "loss": 3.2137,
      "step": 13750
    },
    {
      "epoch": 2.2182814767048202,
      "grad_norm": 14.0625,
      "learning_rate": 8.530313626338158e-05,
      "loss": 3.9945,
      "step": 13760
    },
    {
      "epoch": 2.21989359987103,
      "grad_norm": 12.5625,
      "learning_rate": 8.529237721232988e-05,
      "loss": 3.0061,
      "step": 13770
    },
    {
      "epoch": 2.22150572303724,
      "grad_norm": 14.375,
      "learning_rate": 8.528161816127817e-05,
      "loss": 3.1648,
      "step": 13780
    },
    {
      "epoch": 2.2231178462034498,
      "grad_norm": 21.375,
      "learning_rate": 8.527085911022648e-05,
      "loss": 4.0035,
      "step": 13790
    },
    {
      "epoch": 2.22472996936966,
      "grad_norm": 10.0625,
      "learning_rate": 8.526010005917478e-05,
      "loss": 2.4066,
      "step": 13800
    },
    {
      "epoch": 2.22634209253587,
      "grad_norm": 74.0,
      "learning_rate": 8.524934100812309e-05,
      "loss": 3.9688,
      "step": 13810
    },
    {
      "epoch": 2.2279542157020797,
      "grad_norm": 24.5,
      "learning_rate": 8.52385819570714e-05,
      "loss": 3.2477,
      "step": 13820
    },
    {
      "epoch": 2.2295663388682896,
      "grad_norm": 15.4375,
      "learning_rate": 8.52278229060197e-05,
      "loss": 3.7133,
      "step": 13830
    },
    {
      "epoch": 2.2311784620344994,
      "grad_norm": 18.125,
      "learning_rate": 8.521706385496799e-05,
      "loss": 4.7289,
      "step": 13840
    },
    {
      "epoch": 2.2327905852007093,
      "grad_norm": 21.5,
      "learning_rate": 8.52063048039163e-05,
      "loss": 2.8785,
      "step": 13850
    },
    {
      "epoch": 2.234402708366919,
      "grad_norm": 39.75,
      "learning_rate": 8.51955457528646e-05,
      "loss": 2.6171,
      "step": 13860
    },
    {
      "epoch": 2.236014831533129,
      "grad_norm": 0.0,
      "learning_rate": 8.51847867018129e-05,
      "loss": 3.8827,
      "step": 13870
    },
    {
      "epoch": 2.237626954699339,
      "grad_norm": 17.625,
      "learning_rate": 8.517402765076122e-05,
      "loss": 4.0367,
      "step": 13880
    },
    {
      "epoch": 2.239239077865549,
      "grad_norm": 16.125,
      "learning_rate": 8.516326859970952e-05,
      "loss": 2.6537,
      "step": 13890
    },
    {
      "epoch": 2.240851201031759,
      "grad_norm": 17.75,
      "learning_rate": 8.515250954865781e-05,
      "loss": 4.168,
      "step": 13900
    },
    {
      "epoch": 2.242463324197969,
      "grad_norm": 29.75,
      "learning_rate": 8.514175049760611e-05,
      "loss": 4.4992,
      "step": 13910
    },
    {
      "epoch": 2.2440754473641786,
      "grad_norm": 44.0,
      "learning_rate": 8.513099144655442e-05,
      "loss": 3.3523,
      "step": 13920
    },
    {
      "epoch": 2.2456875705303885,
      "grad_norm": 13.9375,
      "learning_rate": 8.512023239550272e-05,
      "loss": 3.8,
      "step": 13930
    },
    {
      "epoch": 2.2472996936965983,
      "grad_norm": 43.25,
      "learning_rate": 8.510947334445103e-05,
      "loss": 3.8664,
      "step": 13940
    },
    {
      "epoch": 2.248911816862808,
      "grad_norm": 36.25,
      "learning_rate": 8.509871429339934e-05,
      "loss": 3.0432,
      "step": 13950
    },
    {
      "epoch": 2.2505239400290185,
      "grad_norm": 0.0,
      "learning_rate": 8.508795524234763e-05,
      "loss": 4.1409,
      "step": 13960
    },
    {
      "epoch": 2.2521360631952283,
      "grad_norm": 21.25,
      "learning_rate": 8.507719619129593e-05,
      "loss": 1.5822,
      "step": 13970
    },
    {
      "epoch": 2.253748186361438,
      "grad_norm": 59.75,
      "learning_rate": 8.506643714024424e-05,
      "loss": 3.9758,
      "step": 13980
    },
    {
      "epoch": 2.255360309527648,
      "grad_norm": 50.5,
      "learning_rate": 8.505567808919253e-05,
      "loss": 3.209,
      "step": 13990
    },
    {
      "epoch": 2.256972432693858,
      "grad_norm": 57.25,
      "learning_rate": 8.504491903814083e-05,
      "loss": 3.9363,
      "step": 14000
    },
    {
      "epoch": 2.2585845558600677,
      "grad_norm": 31.25,
      "learning_rate": 8.503415998708914e-05,
      "loss": 2.6453,
      "step": 14010
    },
    {
      "epoch": 2.2601966790262775,
      "grad_norm": 87.5,
      "learning_rate": 8.502340093603745e-05,
      "loss": 3.149,
      "step": 14020
    },
    {
      "epoch": 2.2618088021924874,
      "grad_norm": 14.8125,
      "learning_rate": 8.501264188498575e-05,
      "loss": 1.8318,
      "step": 14030
    },
    {
      "epoch": 2.263420925358697,
      "grad_norm": 16.5,
      "learning_rate": 8.500188283393406e-05,
      "loss": 3.5086,
      "step": 14040
    },
    {
      "epoch": 2.2650330485249075,
      "grad_norm": 42.25,
      "learning_rate": 8.499112378288235e-05,
      "loss": 3.9633,
      "step": 14050
    },
    {
      "epoch": 2.2666451716911173,
      "grad_norm": 19.875,
      "learning_rate": 8.498036473183065e-05,
      "loss": 2.6531,
      "step": 14060
    },
    {
      "epoch": 2.268257294857327,
      "grad_norm": 40.5,
      "learning_rate": 8.496960568077896e-05,
      "loss": 4.2383,
      "step": 14070
    },
    {
      "epoch": 2.269869418023537,
      "grad_norm": 5.9375,
      "learning_rate": 8.495884662972726e-05,
      "loss": 3.2052,
      "step": 14080
    },
    {
      "epoch": 2.271481541189747,
      "grad_norm": 110.5,
      "learning_rate": 8.494808757867557e-05,
      "loss": 3.3164,
      "step": 14090
    },
    {
      "epoch": 2.2730936643559567,
      "grad_norm": 0.0,
      "learning_rate": 8.493732852762388e-05,
      "loss": 3.1611,
      "step": 14100
    },
    {
      "epoch": 2.2747057875221666,
      "grad_norm": 23.75,
      "learning_rate": 8.492656947657217e-05,
      "loss": 3.4223,
      "step": 14110
    },
    {
      "epoch": 2.2763179106883764,
      "grad_norm": 16.5,
      "learning_rate": 8.491581042552047e-05,
      "loss": 3.8841,
      "step": 14120
    },
    {
      "epoch": 2.2779300338545863,
      "grad_norm": 32.5,
      "learning_rate": 8.490505137446878e-05,
      "loss": 2.0719,
      "step": 14130
    },
    {
      "epoch": 2.2795421570207965,
      "grad_norm": 11.1875,
      "learning_rate": 8.489429232341708e-05,
      "loss": 4.0461,
      "step": 14140
    },
    {
      "epoch": 2.2811542801870064,
      "grad_norm": 17.875,
      "learning_rate": 8.488353327236537e-05,
      "loss": 2.5203,
      "step": 14150
    },
    {
      "epoch": 2.2827664033532162,
      "grad_norm": 128.0,
      "learning_rate": 8.487277422131368e-05,
      "loss": 3.1785,
      "step": 14160
    },
    {
      "epoch": 2.284378526519426,
      "grad_norm": 16.625,
      "learning_rate": 8.4862015170262e-05,
      "loss": 3.357,
      "step": 14170
    },
    {
      "epoch": 2.285990649685636,
      "grad_norm": 41.5,
      "learning_rate": 8.485125611921029e-05,
      "loss": 3.8582,
      "step": 14180
    },
    {
      "epoch": 2.2876027728518458,
      "grad_norm": 98.0,
      "learning_rate": 8.48404970681586e-05,
      "loss": 2.9246,
      "step": 14190
    },
    {
      "epoch": 2.2892148960180556,
      "grad_norm": 14.1875,
      "learning_rate": 8.48297380171069e-05,
      "loss": 2.9451,
      "step": 14200
    },
    {
      "epoch": 2.290827019184266,
      "grad_norm": 137.0,
      "learning_rate": 8.48189789660552e-05,
      "loss": 4.082,
      "step": 14210
    },
    {
      "epoch": 2.2924391423504757,
      "grad_norm": 37.5,
      "learning_rate": 8.48082199150035e-05,
      "loss": 2.0087,
      "step": 14220
    },
    {
      "epoch": 2.2940512655166856,
      "grad_norm": 22.75,
      "learning_rate": 8.47974608639518e-05,
      "loss": 2.4805,
      "step": 14230
    },
    {
      "epoch": 2.2956633886828954,
      "grad_norm": 66.0,
      "learning_rate": 8.478670181290011e-05,
      "loss": 3.9594,
      "step": 14240
    },
    {
      "epoch": 2.2972755118491053,
      "grad_norm": 21.375,
      "learning_rate": 8.47759427618484e-05,
      "loss": 4.0156,
      "step": 14250
    },
    {
      "epoch": 2.298887635015315,
      "grad_norm": 27.875,
      "learning_rate": 8.476518371079672e-05,
      "loss": 4.6797,
      "step": 14260
    },
    {
      "epoch": 2.300499758181525,
      "grad_norm": 20.25,
      "learning_rate": 8.475442465974501e-05,
      "loss": 1.7489,
      "step": 14270
    },
    {
      "epoch": 2.302111881347735,
      "grad_norm": 286.0,
      "learning_rate": 8.474366560869331e-05,
      "loss": 2.6246,
      "step": 14280
    },
    {
      "epoch": 2.3037240045139447,
      "grad_norm": 17.625,
      "learning_rate": 8.473290655764162e-05,
      "loss": 1.9515,
      "step": 14290
    },
    {
      "epoch": 2.305336127680155,
      "grad_norm": 10.0625,
      "learning_rate": 8.472214750658992e-05,
      "loss": 2.5687,
      "step": 14300
    },
    {
      "epoch": 2.306948250846365,
      "grad_norm": 73.5,
      "learning_rate": 8.471138845553823e-05,
      "loss": 3.0148,
      "step": 14310
    },
    {
      "epoch": 2.3085603740125746,
      "grad_norm": 46.75,
      "learning_rate": 8.470062940448654e-05,
      "loss": 3.9018,
      "step": 14320
    },
    {
      "epoch": 2.3101724971787845,
      "grad_norm": 69.5,
      "learning_rate": 8.468987035343483e-05,
      "loss": 2.8082,
      "step": 14330
    },
    {
      "epoch": 2.3117846203449943,
      "grad_norm": 20.75,
      "learning_rate": 8.467911130238313e-05,
      "loss": 3.4723,
      "step": 14340
    },
    {
      "epoch": 2.313396743511204,
      "grad_norm": 9.1875,
      "learning_rate": 8.466835225133144e-05,
      "loss": 2.3401,
      "step": 14350
    },
    {
      "epoch": 2.315008866677414,
      "grad_norm": 14.1875,
      "learning_rate": 8.465759320027974e-05,
      "loss": 3.5801,
      "step": 14360
    },
    {
      "epoch": 2.3166209898436243,
      "grad_norm": 26.0,
      "learning_rate": 8.464683414922803e-05,
      "loss": 1.5279,
      "step": 14370
    },
    {
      "epoch": 2.318233113009834,
      "grad_norm": 16.125,
      "learning_rate": 8.463607509817634e-05,
      "loss": 3.3951,
      "step": 14380
    },
    {
      "epoch": 2.319845236176044,
      "grad_norm": 36.5,
      "learning_rate": 8.462531604712465e-05,
      "loss": 4.0381,
      "step": 14390
    },
    {
      "epoch": 2.321457359342254,
      "grad_norm": 16.875,
      "learning_rate": 8.461455699607295e-05,
      "loss": 3.4502,
      "step": 14400
    },
    {
      "epoch": 2.3230694825084637,
      "grad_norm": 152.0,
      "learning_rate": 8.460379794502126e-05,
      "loss": 3.9309,
      "step": 14410
    },
    {
      "epoch": 2.3246816056746735,
      "grad_norm": 31.125,
      "learning_rate": 8.459303889396956e-05,
      "loss": 3.1824,
      "step": 14420
    },
    {
      "epoch": 2.3262937288408834,
      "grad_norm": 43.75,
      "learning_rate": 8.458227984291785e-05,
      "loss": 2.2455,
      "step": 14430
    },
    {
      "epoch": 2.327905852007093,
      "grad_norm": 11.125,
      "learning_rate": 8.457152079186616e-05,
      "loss": 2.3725,
      "step": 14440
    },
    {
      "epoch": 2.329517975173303,
      "grad_norm": 18.5,
      "learning_rate": 8.456076174081446e-05,
      "loss": 2.2215,
      "step": 14450
    },
    {
      "epoch": 2.3311300983395133,
      "grad_norm": 37.25,
      "learning_rate": 8.455000268976277e-05,
      "loss": 3.4262,
      "step": 14460
    },
    {
      "epoch": 2.332742221505723,
      "grad_norm": 15.6875,
      "learning_rate": 8.453924363871108e-05,
      "loss": 2.441,
      "step": 14470
    },
    {
      "epoch": 2.334354344671933,
      "grad_norm": 74.0,
      "learning_rate": 8.452848458765938e-05,
      "loss": 1.7282,
      "step": 14480
    },
    {
      "epoch": 2.335966467838143,
      "grad_norm": 43.75,
      "learning_rate": 8.451772553660767e-05,
      "loss": 2.6724,
      "step": 14490
    },
    {
      "epoch": 2.3375785910043527,
      "grad_norm": 13.875,
      "learning_rate": 8.450696648555598e-05,
      "loss": 2.3,
      "step": 14500
    },
    {
      "epoch": 2.3391907141705626,
      "grad_norm": 81.0,
      "learning_rate": 8.449620743450428e-05,
      "loss": 3.4215,
      "step": 14510
    },
    {
      "epoch": 2.3408028373367724,
      "grad_norm": 22.625,
      "learning_rate": 8.448544838345257e-05,
      "loss": 3.3648,
      "step": 14520
    },
    {
      "epoch": 2.3424149605029823,
      "grad_norm": 20.375,
      "learning_rate": 8.447468933240088e-05,
      "loss": 2.2637,
      "step": 14530
    },
    {
      "epoch": 2.344027083669192,
      "grad_norm": 43.25,
      "learning_rate": 8.44639302813492e-05,
      "loss": 3.3754,
      "step": 14540
    },
    {
      "epoch": 2.3456392068354024,
      "grad_norm": 156.0,
      "learning_rate": 8.445317123029749e-05,
      "loss": 3.5011,
      "step": 14550
    },
    {
      "epoch": 2.3472513300016122,
      "grad_norm": 20.375,
      "learning_rate": 8.44424121792458e-05,
      "loss": 3.9004,
      "step": 14560
    },
    {
      "epoch": 2.348863453167822,
      "grad_norm": 15.25,
      "learning_rate": 8.44316531281941e-05,
      "loss": 3.7641,
      "step": 14570
    },
    {
      "epoch": 2.350475576334032,
      "grad_norm": 30.75,
      "learning_rate": 8.44208940771424e-05,
      "loss": 2.1656,
      "step": 14580
    },
    {
      "epoch": 2.3520876995002418,
      "grad_norm": 67.0,
      "learning_rate": 8.441013502609069e-05,
      "loss": 2.8141,
      "step": 14590
    },
    {
      "epoch": 2.3536998226664516,
      "grad_norm": 66.0,
      "learning_rate": 8.439937597503901e-05,
      "loss": 3.4922,
      "step": 14600
    },
    {
      "epoch": 2.3553119458326615,
      "grad_norm": 80.0,
      "learning_rate": 8.438861692398731e-05,
      "loss": 2.7,
      "step": 14610
    },
    {
      "epoch": 2.3569240689988717,
      "grad_norm": 170.0,
      "learning_rate": 8.437785787293561e-05,
      "loss": 3.5266,
      "step": 14620
    },
    {
      "epoch": 2.3585361921650816,
      "grad_norm": 7.8125,
      "learning_rate": 8.436709882188392e-05,
      "loss": 1.5871,
      "step": 14630
    },
    {
      "epoch": 2.3601483153312914,
      "grad_norm": 79.5,
      "learning_rate": 8.435633977083221e-05,
      "loss": 2.9422,
      "step": 14640
    },
    {
      "epoch": 2.3617604384975013,
      "grad_norm": 13.75,
      "learning_rate": 8.434558071978051e-05,
      "loss": 2.8629,
      "step": 14650
    },
    {
      "epoch": 2.363372561663711,
      "grad_norm": 11.75,
      "learning_rate": 8.433482166872882e-05,
      "loss": 1.8389,
      "step": 14660
    },
    {
      "epoch": 2.364984684829921,
      "grad_norm": 8.25,
      "learning_rate": 8.432406261767713e-05,
      "loss": 3.9719,
      "step": 14670
    },
    {
      "epoch": 2.366596807996131,
      "grad_norm": 94.5,
      "learning_rate": 8.431330356662543e-05,
      "loss": 2.4918,
      "step": 14680
    },
    {
      "epoch": 2.3682089311623407,
      "grad_norm": 11.25,
      "learning_rate": 8.430254451557374e-05,
      "loss": 2.5111,
      "step": 14690
    },
    {
      "epoch": 2.3698210543285505,
      "grad_norm": 73.0,
      "learning_rate": 8.429178546452203e-05,
      "loss": 2.316,
      "step": 14700
    },
    {
      "epoch": 2.371433177494761,
      "grad_norm": 25.5,
      "learning_rate": 8.428102641347033e-05,
      "loss": 3.3242,
      "step": 14710
    },
    {
      "epoch": 2.3730453006609706,
      "grad_norm": 16.25,
      "learning_rate": 8.427026736241864e-05,
      "loss": 3.2406,
      "step": 14720
    },
    {
      "epoch": 2.3746574238271805,
      "grad_norm": 26.5,
      "learning_rate": 8.425950831136694e-05,
      "loss": 3.4762,
      "step": 14730
    },
    {
      "epoch": 2.3762695469933903,
      "grad_norm": 215.0,
      "learning_rate": 8.424874926031525e-05,
      "loss": 2.9395,
      "step": 14740
    },
    {
      "epoch": 2.3778816701596,
      "grad_norm": 28.5,
      "learning_rate": 8.423799020926356e-05,
      "loss": 3.9891,
      "step": 14750
    },
    {
      "epoch": 2.37949379332581,
      "grad_norm": 19.75,
      "learning_rate": 8.422723115821185e-05,
      "loss": 4.3488,
      "step": 14760
    },
    {
      "epoch": 2.38110591649202,
      "grad_norm": 50.25,
      "learning_rate": 8.421647210716015e-05,
      "loss": 4.2183,
      "step": 14770
    },
    {
      "epoch": 2.3827180396582297,
      "grad_norm": 39.5,
      "learning_rate": 8.420571305610846e-05,
      "loss": 4.1375,
      "step": 14780
    },
    {
      "epoch": 2.3843301628244395,
      "grad_norm": 24.875,
      "learning_rate": 8.419495400505676e-05,
      "loss": 4.9832,
      "step": 14790
    },
    {
      "epoch": 2.38594228599065,
      "grad_norm": 35.0,
      "learning_rate": 8.418419495400505e-05,
      "loss": 2.3538,
      "step": 14800
    },
    {
      "epoch": 2.3875544091568597,
      "grad_norm": 24.875,
      "learning_rate": 8.417343590295336e-05,
      "loss": 2.6286,
      "step": 14810
    },
    {
      "epoch": 2.3891665323230695,
      "grad_norm": 45.25,
      "learning_rate": 8.416267685190167e-05,
      "loss": 2.6648,
      "step": 14820
    },
    {
      "epoch": 2.3907786554892794,
      "grad_norm": 20.375,
      "learning_rate": 8.415191780084997e-05,
      "loss": 2.3324,
      "step": 14830
    },
    {
      "epoch": 2.392390778655489,
      "grad_norm": 72.0,
      "learning_rate": 8.414115874979828e-05,
      "loss": 2.9614,
      "step": 14840
    },
    {
      "epoch": 2.394002901821699,
      "grad_norm": 25.0,
      "learning_rate": 8.413039969874658e-05,
      "loss": 2.0114,
      "step": 14850
    },
    {
      "epoch": 2.395615024987909,
      "grad_norm": 78.5,
      "learning_rate": 8.411964064769487e-05,
      "loss": 3.8402,
      "step": 14860
    },
    {
      "epoch": 2.397227148154119,
      "grad_norm": 29.875,
      "learning_rate": 8.410888159664318e-05,
      "loss": 1.8714,
      "step": 14870
    },
    {
      "epoch": 2.398839271320329,
      "grad_norm": 182.0,
      "learning_rate": 8.409812254559148e-05,
      "loss": 4.6427,
      "step": 14880
    },
    {
      "epoch": 2.400451394486539,
      "grad_norm": 27.0,
      "learning_rate": 8.408736349453979e-05,
      "loss": 1.8111,
      "step": 14890
    },
    {
      "epoch": 2.4020635176527487,
      "grad_norm": 22.875,
      "learning_rate": 8.407660444348809e-05,
      "loss": 1.3087,
      "step": 14900
    },
    {
      "epoch": 2.4036756408189586,
      "grad_norm": 16.0,
      "learning_rate": 8.40658453924364e-05,
      "loss": 2.8066,
      "step": 14910
    },
    {
      "epoch": 2.4052877639851684,
      "grad_norm": 0.0,
      "learning_rate": 8.405508634138469e-05,
      "loss": 3.306,
      "step": 14920
    },
    {
      "epoch": 2.4068998871513783,
      "grad_norm": 11.1875,
      "learning_rate": 8.404432729033299e-05,
      "loss": 2.9152,
      "step": 14930
    },
    {
      "epoch": 2.408512010317588,
      "grad_norm": 24.375,
      "learning_rate": 8.40335682392813e-05,
      "loss": 2.6654,
      "step": 14940
    },
    {
      "epoch": 2.410124133483798,
      "grad_norm": 18.5,
      "learning_rate": 8.40228091882296e-05,
      "loss": 2.543,
      "step": 14950
    },
    {
      "epoch": 2.4117362566500082,
      "grad_norm": 30.875,
      "learning_rate": 8.40120501371779e-05,
      "loss": 3.3039,
      "step": 14960
    },
    {
      "epoch": 2.413348379816218,
      "grad_norm": 103.0,
      "learning_rate": 8.400129108612622e-05,
      "loss": 4.5742,
      "step": 14970
    },
    {
      "epoch": 2.414960502982428,
      "grad_norm": 37.25,
      "learning_rate": 8.399053203507451e-05,
      "loss": 3.2078,
      "step": 14980
    },
    {
      "epoch": 2.4165726261486378,
      "grad_norm": 23.375,
      "learning_rate": 8.397977298402281e-05,
      "loss": 2.0193,
      "step": 14990
    },
    {
      "epoch": 2.4181847493148476,
      "grad_norm": 0.0,
      "learning_rate": 8.396901393297112e-05,
      "loss": 2.2868,
      "step": 15000
    },
    {
      "epoch": 2.4197968724810575,
      "grad_norm": 39.0,
      "learning_rate": 8.395825488191942e-05,
      "loss": 4.3012,
      "step": 15010
    },
    {
      "epoch": 2.4214089956472673,
      "grad_norm": 4.96875,
      "learning_rate": 8.394749583086771e-05,
      "loss": 2.6135,
      "step": 15020
    },
    {
      "epoch": 2.4230211188134776,
      "grad_norm": 39.75,
      "learning_rate": 8.393673677981602e-05,
      "loss": 3.599,
      "step": 15030
    },
    {
      "epoch": 2.4246332419796874,
      "grad_norm": 33.0,
      "learning_rate": 8.392597772876433e-05,
      "loss": 3.0155,
      "step": 15040
    },
    {
      "epoch": 2.4262453651458973,
      "grad_norm": 34.5,
      "learning_rate": 8.391521867771263e-05,
      "loss": 2.6125,
      "step": 15050
    },
    {
      "epoch": 2.427857488312107,
      "grad_norm": 54.0,
      "learning_rate": 8.390445962666094e-05,
      "loss": 2.8211,
      "step": 15060
    },
    {
      "epoch": 2.429469611478317,
      "grad_norm": 15.875,
      "learning_rate": 8.389370057560923e-05,
      "loss": 3.0041,
      "step": 15070
    },
    {
      "epoch": 2.431081734644527,
      "grad_norm": 19.5,
      "learning_rate": 8.388294152455753e-05,
      "loss": 2.0059,
      "step": 15080
    },
    {
      "epoch": 2.4326938578107367,
      "grad_norm": 32.5,
      "learning_rate": 8.387218247350584e-05,
      "loss": 3.4398,
      "step": 15090
    },
    {
      "epoch": 2.4343059809769465,
      "grad_norm": 53.5,
      "learning_rate": 8.386142342245414e-05,
      "loss": 2.7646,
      "step": 15100
    },
    {
      "epoch": 2.4359181041431563,
      "grad_norm": 38.0,
      "learning_rate": 8.385066437140245e-05,
      "loss": 3.3426,
      "step": 15110
    },
    {
      "epoch": 2.4375302273093666,
      "grad_norm": 28.0,
      "learning_rate": 8.383990532035076e-05,
      "loss": 2.7512,
      "step": 15120
    },
    {
      "epoch": 2.4391423504755765,
      "grad_norm": 18.375,
      "learning_rate": 8.382914626929905e-05,
      "loss": 3.2402,
      "step": 15130
    },
    {
      "epoch": 2.4407544736417863,
      "grad_norm": 36.75,
      "learning_rate": 8.381838721824735e-05,
      "loss": 3.8924,
      "step": 15140
    },
    {
      "epoch": 2.442366596807996,
      "grad_norm": 63.0,
      "learning_rate": 8.380762816719566e-05,
      "loss": 2.8066,
      "step": 15150
    },
    {
      "epoch": 2.443978719974206,
      "grad_norm": 44.0,
      "learning_rate": 8.379686911614396e-05,
      "loss": 4.55,
      "step": 15160
    },
    {
      "epoch": 2.445590843140416,
      "grad_norm": 19.5,
      "learning_rate": 8.378611006509225e-05,
      "loss": 2.4025,
      "step": 15170
    },
    {
      "epoch": 2.4472029663066257,
      "grad_norm": 32.5,
      "learning_rate": 8.377535101404056e-05,
      "loss": 2.1289,
      "step": 15180
    },
    {
      "epoch": 2.4488150894728355,
      "grad_norm": 14.9375,
      "learning_rate": 8.376459196298887e-05,
      "loss": 5.8063,
      "step": 15190
    },
    {
      "epoch": 2.4504272126390454,
      "grad_norm": 20.375,
      "learning_rate": 8.375383291193717e-05,
      "loss": 1.8647,
      "step": 15200
    },
    {
      "epoch": 2.4520393358052557,
      "grad_norm": 104.5,
      "learning_rate": 8.374307386088548e-05,
      "loss": 2.8016,
      "step": 15210
    },
    {
      "epoch": 2.4536514589714655,
      "grad_norm": 32.75,
      "learning_rate": 8.373231480983378e-05,
      "loss": 4.3484,
      "step": 15220
    },
    {
      "epoch": 2.4552635821376754,
      "grad_norm": 7.59375,
      "learning_rate": 8.372155575878207e-05,
      "loss": 2.5105,
      "step": 15230
    },
    {
      "epoch": 2.456875705303885,
      "grad_norm": 0.0,
      "learning_rate": 8.371079670773037e-05,
      "loss": 2.5738,
      "step": 15240
    },
    {
      "epoch": 2.458487828470095,
      "grad_norm": 10.3125,
      "learning_rate": 8.37000376566787e-05,
      "loss": 2.574,
      "step": 15250
    },
    {
      "epoch": 2.460099951636305,
      "grad_norm": 14.6875,
      "learning_rate": 8.368927860562699e-05,
      "loss": 1.7582,
      "step": 15260
    },
    {
      "epoch": 2.4617120748025147,
      "grad_norm": 66.0,
      "learning_rate": 8.367851955457529e-05,
      "loss": 3.8527,
      "step": 15270
    },
    {
      "epoch": 2.463324197968725,
      "grad_norm": 3.84375,
      "learning_rate": 8.36677605035236e-05,
      "loss": 2.5126,
      "step": 15280
    },
    {
      "epoch": 2.464936321134935,
      "grad_norm": 19.375,
      "learning_rate": 8.36570014524719e-05,
      "loss": 3.1296,
      "step": 15290
    },
    {
      "epoch": 2.4665484443011447,
      "grad_norm": 30.5,
      "learning_rate": 8.364624240142019e-05,
      "loss": 2.8549,
      "step": 15300
    },
    {
      "epoch": 2.4681605674673546,
      "grad_norm": 24.125,
      "learning_rate": 8.36354833503685e-05,
      "loss": 3.0377,
      "step": 15310
    },
    {
      "epoch": 2.4697726906335644,
      "grad_norm": 55.5,
      "learning_rate": 8.362472429931681e-05,
      "loss": 4.1727,
      "step": 15320
    },
    {
      "epoch": 2.4713848137997743,
      "grad_norm": 20.125,
      "learning_rate": 8.36139652482651e-05,
      "loss": 2.3226,
      "step": 15330
    },
    {
      "epoch": 2.472996936965984,
      "grad_norm": 20.625,
      "learning_rate": 8.360320619721342e-05,
      "loss": 3.7669,
      "step": 15340
    },
    {
      "epoch": 2.474609060132194,
      "grad_norm": 26.875,
      "learning_rate": 8.359244714616171e-05,
      "loss": 3.8247,
      "step": 15350
    },
    {
      "epoch": 2.476221183298404,
      "grad_norm": 34.0,
      "learning_rate": 8.358168809511001e-05,
      "loss": 1.7251,
      "step": 15360
    },
    {
      "epoch": 2.477833306464614,
      "grad_norm": 44.5,
      "learning_rate": 8.357092904405832e-05,
      "loss": 4.8109,
      "step": 15370
    },
    {
      "epoch": 2.479445429630824,
      "grad_norm": 109.0,
      "learning_rate": 8.356016999300662e-05,
      "loss": 4.6469,
      "step": 15380
    },
    {
      "epoch": 2.4810575527970338,
      "grad_norm": 57.75,
      "learning_rate": 8.354941094195493e-05,
      "loss": 3.2832,
      "step": 15390
    },
    {
      "epoch": 2.4826696759632436,
      "grad_norm": 14.3125,
      "learning_rate": 8.353865189090324e-05,
      "loss": 2.1008,
      "step": 15400
    },
    {
      "epoch": 2.4842817991294535,
      "grad_norm": 32.5,
      "learning_rate": 8.352789283985153e-05,
      "loss": 4.9062,
      "step": 15410
    },
    {
      "epoch": 2.4858939222956633,
      "grad_norm": 37.25,
      "learning_rate": 8.351713378879983e-05,
      "loss": 3.4328,
      "step": 15420
    },
    {
      "epoch": 2.487506045461873,
      "grad_norm": 37.0,
      "learning_rate": 8.350637473774814e-05,
      "loss": 2.9021,
      "step": 15430
    },
    {
      "epoch": 2.489118168628083,
      "grad_norm": 19.125,
      "learning_rate": 8.349561568669644e-05,
      "loss": 3.4115,
      "step": 15440
    },
    {
      "epoch": 2.490730291794293,
      "grad_norm": 41.5,
      "learning_rate": 8.348485663564473e-05,
      "loss": 3.2875,
      "step": 15450
    },
    {
      "epoch": 2.492342414960503,
      "grad_norm": 7.53125,
      "learning_rate": 8.347409758459304e-05,
      "loss": 1.4469,
      "step": 15460
    },
    {
      "epoch": 2.493954538126713,
      "grad_norm": 57.25,
      "learning_rate": 8.346333853354135e-05,
      "loss": 2.5332,
      "step": 15470
    },
    {
      "epoch": 2.495566661292923,
      "grad_norm": 12.875,
      "learning_rate": 8.345257948248965e-05,
      "loss": 1.6292,
      "step": 15480
    },
    {
      "epoch": 2.4971787844591327,
      "grad_norm": 28.0,
      "learning_rate": 8.344182043143796e-05,
      "loss": 1.8484,
      "step": 15490
    },
    {
      "epoch": 2.4987909076253425,
      "grad_norm": 144.0,
      "learning_rate": 8.343106138038626e-05,
      "loss": 2.697,
      "step": 15500
    },
    {
      "epoch": 2.5004030307915524,
      "grad_norm": 57.25,
      "learning_rate": 8.342030232933455e-05,
      "loss": 5.4375,
      "step": 15510
    },
    {
      "epoch": 2.502015153957762,
      "grad_norm": 22.75,
      "learning_rate": 8.340954327828286e-05,
      "loss": 2.0688,
      "step": 15520
    },
    {
      "epoch": 2.5036272771239725,
      "grad_norm": 35.75,
      "learning_rate": 8.339878422723116e-05,
      "loss": 2.723,
      "step": 15530
    },
    {
      "epoch": 2.5052394002901823,
      "grad_norm": 35.25,
      "learning_rate": 8.338802517617947e-05,
      "loss": 3.4551,
      "step": 15540
    },
    {
      "epoch": 2.506851523456392,
      "grad_norm": 16.0,
      "learning_rate": 8.337726612512778e-05,
      "loss": 2.4045,
      "step": 15550
    },
    {
      "epoch": 2.508463646622602,
      "grad_norm": 20.5,
      "learning_rate": 8.336650707407608e-05,
      "loss": 2.5541,
      "step": 15560
    },
    {
      "epoch": 2.510075769788812,
      "grad_norm": 51.25,
      "learning_rate": 8.335574802302437e-05,
      "loss": 3.7037,
      "step": 15570
    },
    {
      "epoch": 2.5116878929550217,
      "grad_norm": 79.0,
      "learning_rate": 8.334498897197267e-05,
      "loss": 2.8715,
      "step": 15580
    },
    {
      "epoch": 2.5133000161212316,
      "grad_norm": 11.5625,
      "learning_rate": 8.333422992092098e-05,
      "loss": 2.6375,
      "step": 15590
    },
    {
      "epoch": 2.5149121392874414,
      "grad_norm": 43.0,
      "learning_rate": 8.332347086986927e-05,
      "loss": 2.9483,
      "step": 15600
    },
    {
      "epoch": 2.5165242624536512,
      "grad_norm": 23.5,
      "learning_rate": 8.331271181881758e-05,
      "loss": 2.429,
      "step": 15610
    },
    {
      "epoch": 2.5181363856198615,
      "grad_norm": 20.5,
      "learning_rate": 8.33019527677659e-05,
      "loss": 3.3355,
      "step": 15620
    },
    {
      "epoch": 2.5197485087860714,
      "grad_norm": 72.5,
      "learning_rate": 8.329119371671419e-05,
      "loss": 5.7693,
      "step": 15630
    },
    {
      "epoch": 2.521360631952281,
      "grad_norm": 17.625,
      "learning_rate": 8.328043466566249e-05,
      "loss": 4.1961,
      "step": 15640
    },
    {
      "epoch": 2.522972755118491,
      "grad_norm": 20.375,
      "learning_rate": 8.32696756146108e-05,
      "loss": 1.2367,
      "step": 15650
    },
    {
      "epoch": 2.524584878284701,
      "grad_norm": 45.25,
      "learning_rate": 8.32589165635591e-05,
      "loss": 5.9281,
      "step": 15660
    },
    {
      "epoch": 2.5261970014509108,
      "grad_norm": 40.0,
      "learning_rate": 8.324815751250739e-05,
      "loss": 3.4008,
      "step": 15670
    },
    {
      "epoch": 2.5278091246171206,
      "grad_norm": 47.5,
      "learning_rate": 8.32373984614557e-05,
      "loss": 3.9593,
      "step": 15680
    },
    {
      "epoch": 2.529421247783331,
      "grad_norm": 16.25,
      "learning_rate": 8.322663941040401e-05,
      "loss": 4.3006,
      "step": 15690
    },
    {
      "epoch": 2.5310333709495403,
      "grad_norm": 14.5625,
      "learning_rate": 8.321588035935231e-05,
      "loss": 3.1336,
      "step": 15700
    },
    {
      "epoch": 2.5326454941157506,
      "grad_norm": 25.5,
      "learning_rate": 8.320512130830062e-05,
      "loss": 3.0318,
      "step": 15710
    },
    {
      "epoch": 2.5342576172819604,
      "grad_norm": 30.625,
      "learning_rate": 8.319436225724891e-05,
      "loss": 3.4586,
      "step": 15720
    },
    {
      "epoch": 2.5358697404481703,
      "grad_norm": 133.0,
      "learning_rate": 8.318360320619721e-05,
      "loss": 2.7357,
      "step": 15730
    },
    {
      "epoch": 2.53748186361438,
      "grad_norm": 17.5,
      "learning_rate": 8.317284415514552e-05,
      "loss": 4.2621,
      "step": 15740
    },
    {
      "epoch": 2.53909398678059,
      "grad_norm": 47.0,
      "learning_rate": 8.316208510409382e-05,
      "loss": 3.8523,
      "step": 15750
    },
    {
      "epoch": 2.5407061099468,
      "grad_norm": 39.5,
      "learning_rate": 8.315132605304213e-05,
      "loss": 1.9655,
      "step": 15760
    },
    {
      "epoch": 2.5423182331130096,
      "grad_norm": 40.5,
      "learning_rate": 8.314056700199044e-05,
      "loss": 3.327,
      "step": 15770
    },
    {
      "epoch": 2.54393035627922,
      "grad_norm": 56.75,
      "learning_rate": 8.312980795093873e-05,
      "loss": 4.035,
      "step": 15780
    },
    {
      "epoch": 2.5455424794454298,
      "grad_norm": 87.0,
      "learning_rate": 8.311904889988703e-05,
      "loss": 4.7622,
      "step": 15790
    },
    {
      "epoch": 2.5471546026116396,
      "grad_norm": 39.5,
      "learning_rate": 8.310828984883534e-05,
      "loss": 2.5774,
      "step": 15800
    },
    {
      "epoch": 2.5487667257778495,
      "grad_norm": 38.25,
      "learning_rate": 8.309753079778364e-05,
      "loss": 2.9246,
      "step": 15810
    },
    {
      "epoch": 2.5503788489440593,
      "grad_norm": 156.0,
      "learning_rate": 8.308677174673193e-05,
      "loss": 3.1242,
      "step": 15820
    },
    {
      "epoch": 2.551990972110269,
      "grad_norm": 41.75,
      "learning_rate": 8.307601269568024e-05,
      "loss": 3.9363,
      "step": 15830
    },
    {
      "epoch": 2.553603095276479,
      "grad_norm": 29.625,
      "learning_rate": 8.306525364462855e-05,
      "loss": 2.9854,
      "step": 15840
    },
    {
      "epoch": 2.5552152184426893,
      "grad_norm": 27.0,
      "learning_rate": 8.305449459357685e-05,
      "loss": 2.675,
      "step": 15850
    },
    {
      "epoch": 2.5568273416088987,
      "grad_norm": 22.625,
      "learning_rate": 8.304373554252516e-05,
      "loss": 4.8127,
      "step": 15860
    },
    {
      "epoch": 2.558439464775109,
      "grad_norm": 198.0,
      "learning_rate": 8.303297649147346e-05,
      "loss": 2.668,
      "step": 15870
    },
    {
      "epoch": 2.560051587941319,
      "grad_norm": 68.5,
      "learning_rate": 8.302221744042175e-05,
      "loss": 3.6244,
      "step": 15880
    },
    {
      "epoch": 2.5616637111075287,
      "grad_norm": 40.75,
      "learning_rate": 8.301145838937006e-05,
      "loss": 3.1094,
      "step": 15890
    },
    {
      "epoch": 2.5632758342737385,
      "grad_norm": 22.875,
      "learning_rate": 8.300069933831836e-05,
      "loss": 3.1952,
      "step": 15900
    },
    {
      "epoch": 2.5648879574399484,
      "grad_norm": 78.5,
      "learning_rate": 8.298994028726667e-05,
      "loss": 3.8676,
      "step": 15910
    },
    {
      "epoch": 2.566500080606158,
      "grad_norm": 44.75,
      "learning_rate": 8.297918123621497e-05,
      "loss": 3.6969,
      "step": 15920
    },
    {
      "epoch": 2.568112203772368,
      "grad_norm": 51.25,
      "learning_rate": 8.296842218516328e-05,
      "loss": 2.8374,
      "step": 15930
    },
    {
      "epoch": 2.5697243269385783,
      "grad_norm": 29.625,
      "learning_rate": 8.295766313411157e-05,
      "loss": 4.4486,
      "step": 15940
    },
    {
      "epoch": 2.5713364501047877,
      "grad_norm": 55.25,
      "learning_rate": 8.294690408305987e-05,
      "loss": 3.7072,
      "step": 15950
    },
    {
      "epoch": 2.572948573270998,
      "grad_norm": 21.125,
      "learning_rate": 8.293614503200818e-05,
      "loss": 2.8595,
      "step": 15960
    },
    {
      "epoch": 2.574560696437208,
      "grad_norm": 22.0,
      "learning_rate": 8.292538598095649e-05,
      "loss": 3.288,
      "step": 15970
    },
    {
      "epoch": 2.5761728196034177,
      "grad_norm": 14.0,
      "learning_rate": 8.291462692990479e-05,
      "loss": 3.4803,
      "step": 15980
    },
    {
      "epoch": 2.5777849427696276,
      "grad_norm": 49.25,
      "learning_rate": 8.29038678788531e-05,
      "loss": 3.8273,
      "step": 15990
    },
    {
      "epoch": 2.5793970659358374,
      "grad_norm": 32.75,
      "learning_rate": 8.289310882780139e-05,
      "loss": 3.6877,
      "step": 16000
    },
    {
      "epoch": 2.5810091891020472,
      "grad_norm": 18.125,
      "learning_rate": 8.288234977674969e-05,
      "loss": 3.129,
      "step": 16010
    },
    {
      "epoch": 2.582621312268257,
      "grad_norm": 27.5,
      "learning_rate": 8.2871590725698e-05,
      "loss": 3.4965,
      "step": 16020
    },
    {
      "epoch": 2.5842334354344674,
      "grad_norm": 40.75,
      "learning_rate": 8.28608316746463e-05,
      "loss": 2.0451,
      "step": 16030
    },
    {
      "epoch": 2.585845558600677,
      "grad_norm": 21.5,
      "learning_rate": 8.28500726235946e-05,
      "loss": 3.0912,
      "step": 16040
    },
    {
      "epoch": 2.587457681766887,
      "grad_norm": 22.375,
      "learning_rate": 8.283931357254292e-05,
      "loss": 2.0115,
      "step": 16050
    },
    {
      "epoch": 2.589069804933097,
      "grad_norm": 27.5,
      "learning_rate": 8.282855452149121e-05,
      "loss": 2.3576,
      "step": 16060
    },
    {
      "epoch": 2.5906819280993068,
      "grad_norm": 93.0,
      "learning_rate": 8.281779547043951e-05,
      "loss": 1.6692,
      "step": 16070
    },
    {
      "epoch": 2.5922940512655166,
      "grad_norm": 13.1875,
      "learning_rate": 8.280703641938782e-05,
      "loss": 3.7942,
      "step": 16080
    },
    {
      "epoch": 2.5939061744317264,
      "grad_norm": 23.375,
      "learning_rate": 8.279627736833612e-05,
      "loss": 2.427,
      "step": 16090
    },
    {
      "epoch": 2.5955182975979367,
      "grad_norm": 136.0,
      "learning_rate": 8.278551831728441e-05,
      "loss": 2.7535,
      "step": 16100
    },
    {
      "epoch": 2.597130420764146,
      "grad_norm": 61.0,
      "learning_rate": 8.277475926623272e-05,
      "loss": 3.2932,
      "step": 16110
    },
    {
      "epoch": 2.5987425439303564,
      "grad_norm": 38.0,
      "learning_rate": 8.276400021518103e-05,
      "loss": 2.2891,
      "step": 16120
    },
    {
      "epoch": 2.6003546670965663,
      "grad_norm": 22.625,
      "learning_rate": 8.275324116412933e-05,
      "loss": 4.1156,
      "step": 16130
    },
    {
      "epoch": 2.601966790262776,
      "grad_norm": 18.625,
      "learning_rate": 8.274248211307764e-05,
      "loss": 4.3828,
      "step": 16140
    },
    {
      "epoch": 2.603578913428986,
      "grad_norm": 18.125,
      "learning_rate": 8.273172306202593e-05,
      "loss": 2.4971,
      "step": 16150
    },
    {
      "epoch": 2.605191036595196,
      "grad_norm": 61.75,
      "learning_rate": 8.272096401097423e-05,
      "loss": 3.3113,
      "step": 16160
    },
    {
      "epoch": 2.6068031597614056,
      "grad_norm": 22.0,
      "learning_rate": 8.271020495992254e-05,
      "loss": 2.0348,
      "step": 16170
    },
    {
      "epoch": 2.6084152829276155,
      "grad_norm": 112.0,
      "learning_rate": 8.269944590887084e-05,
      "loss": 2.3912,
      "step": 16180
    },
    {
      "epoch": 2.6100274060938258,
      "grad_norm": 87.0,
      "learning_rate": 8.268868685781915e-05,
      "loss": 2.967,
      "step": 16190
    },
    {
      "epoch": 2.6116395292600356,
      "grad_norm": 53.5,
      "learning_rate": 8.267792780676746e-05,
      "loss": 2.0156,
      "step": 16200
    },
    {
      "epoch": 2.6132516524262455,
      "grad_norm": 22.25,
      "learning_rate": 8.266716875571575e-05,
      "loss": 3.6136,
      "step": 16210
    },
    {
      "epoch": 2.6148637755924553,
      "grad_norm": 23.75,
      "learning_rate": 8.265640970466405e-05,
      "loss": 2.8301,
      "step": 16220
    },
    {
      "epoch": 2.616475898758665,
      "grad_norm": 59.5,
      "learning_rate": 8.264565065361235e-05,
      "loss": 2.6316,
      "step": 16230
    },
    {
      "epoch": 2.618088021924875,
      "grad_norm": 33.0,
      "learning_rate": 8.263489160256066e-05,
      "loss": 4.4766,
      "step": 16240
    },
    {
      "epoch": 2.619700145091085,
      "grad_norm": 81.0,
      "learning_rate": 8.262413255150895e-05,
      "loss": 3.5152,
      "step": 16250
    },
    {
      "epoch": 2.621312268257295,
      "grad_norm": 32.75,
      "learning_rate": 8.261337350045726e-05,
      "loss": 3.9268,
      "step": 16260
    },
    {
      "epoch": 2.6229243914235045,
      "grad_norm": 20.75,
      "learning_rate": 8.260261444940557e-05,
      "loss": 3.0043,
      "step": 16270
    },
    {
      "epoch": 2.624536514589715,
      "grad_norm": 93.5,
      "learning_rate": 8.259185539835387e-05,
      "loss": 2.9766,
      "step": 16280
    },
    {
      "epoch": 2.6261486377559247,
      "grad_norm": 20.0,
      "learning_rate": 8.258109634730217e-05,
      "loss": 2.5377,
      "step": 16290
    },
    {
      "epoch": 2.6277607609221345,
      "grad_norm": 93.0,
      "learning_rate": 8.257033729625048e-05,
      "loss": 2.8078,
      "step": 16300
    },
    {
      "epoch": 2.6293728840883444,
      "grad_norm": 10.0,
      "learning_rate": 8.255957824519877e-05,
      "loss": 2.3516,
      "step": 16310
    },
    {
      "epoch": 2.630985007254554,
      "grad_norm": 44.25,
      "learning_rate": 8.254881919414707e-05,
      "loss": 4.0236,
      "step": 16320
    },
    {
      "epoch": 2.632597130420764,
      "grad_norm": 30.375,
      "learning_rate": 8.253806014309538e-05,
      "loss": 1.8607,
      "step": 16330
    },
    {
      "epoch": 2.634209253586974,
      "grad_norm": 70.5,
      "learning_rate": 8.252730109204369e-05,
      "loss": 3.037,
      "step": 16340
    },
    {
      "epoch": 2.635821376753184,
      "grad_norm": 27.25,
      "learning_rate": 8.251654204099199e-05,
      "loss": 2.2425,
      "step": 16350
    },
    {
      "epoch": 2.6374334999193936,
      "grad_norm": 13.0625,
      "learning_rate": 8.25057829899403e-05,
      "loss": 1.4946,
      "step": 16360
    },
    {
      "epoch": 2.639045623085604,
      "grad_norm": 20.625,
      "learning_rate": 8.24950239388886e-05,
      "loss": 1.7685,
      "step": 16370
    },
    {
      "epoch": 2.6406577462518137,
      "grad_norm": 27.125,
      "learning_rate": 8.248426488783689e-05,
      "loss": 1.7324,
      "step": 16380
    },
    {
      "epoch": 2.6422698694180236,
      "grad_norm": 14.75,
      "learning_rate": 8.24735058367852e-05,
      "loss": 1.528,
      "step": 16390
    },
    {
      "epoch": 2.6438819925842334,
      "grad_norm": 38.5,
      "learning_rate": 8.24627467857335e-05,
      "loss": 2.8263,
      "step": 16400
    },
    {
      "epoch": 2.6454941157504432,
      "grad_norm": 64.0,
      "learning_rate": 8.24519877346818e-05,
      "loss": 2.1492,
      "step": 16410
    },
    {
      "epoch": 2.647106238916653,
      "grad_norm": 30.25,
      "learning_rate": 8.244122868363012e-05,
      "loss": 2.7338,
      "step": 16420
    },
    {
      "epoch": 2.648718362082863,
      "grad_norm": 24.25,
      "learning_rate": 8.243046963257841e-05,
      "loss": 2.4792,
      "step": 16430
    },
    {
      "epoch": 2.650330485249073,
      "grad_norm": 48.75,
      "learning_rate": 8.241971058152671e-05,
      "loss": 5.3187,
      "step": 16440
    },
    {
      "epoch": 2.651942608415283,
      "grad_norm": 8.8125,
      "learning_rate": 8.240895153047502e-05,
      "loss": 1.1661,
      "step": 16450
    },
    {
      "epoch": 2.653554731581493,
      "grad_norm": 75.5,
      "learning_rate": 8.239819247942332e-05,
      "loss": 4.3403,
      "step": 16460
    },
    {
      "epoch": 2.6551668547477028,
      "grad_norm": 26.0,
      "learning_rate": 8.238743342837161e-05,
      "loss": 3.8988,
      "step": 16470
    },
    {
      "epoch": 2.6567789779139126,
      "grad_norm": 14.5,
      "learning_rate": 8.237667437731992e-05,
      "loss": 1.747,
      "step": 16480
    },
    {
      "epoch": 2.6583911010801224,
      "grad_norm": 47.5,
      "learning_rate": 8.236591532626823e-05,
      "loss": 3.3623,
      "step": 16490
    },
    {
      "epoch": 2.6600032242463323,
      "grad_norm": 21.75,
      "learning_rate": 8.235515627521653e-05,
      "loss": 3.1453,
      "step": 16500
    },
    {
      "epoch": 2.6616153474125426,
      "grad_norm": 22.125,
      "learning_rate": 8.234439722416484e-05,
      "loss": 2.0321,
      "step": 16510
    },
    {
      "epoch": 2.663227470578752,
      "grad_norm": 33.5,
      "learning_rate": 8.233363817311314e-05,
      "loss": 2.4973,
      "step": 16520
    },
    {
      "epoch": 2.6648395937449623,
      "grad_norm": 20.5,
      "learning_rate": 8.232287912206143e-05,
      "loss": 3.7771,
      "step": 16530
    },
    {
      "epoch": 2.666451716911172,
      "grad_norm": 31.375,
      "learning_rate": 8.231212007100974e-05,
      "loss": 5.0883,
      "step": 16540
    },
    {
      "epoch": 2.668063840077382,
      "grad_norm": 0.103515625,
      "learning_rate": 8.230136101995804e-05,
      "loss": 2.6618,
      "step": 16550
    },
    {
      "epoch": 2.669675963243592,
      "grad_norm": 0.56640625,
      "learning_rate": 8.229060196890635e-05,
      "loss": 3.5405,
      "step": 16560
    },
    {
      "epoch": 2.6712880864098016,
      "grad_norm": 426.0,
      "learning_rate": 8.227984291785465e-05,
      "loss": 1.9645,
      "step": 16570
    },
    {
      "epoch": 2.6729002095760115,
      "grad_norm": 43.0,
      "learning_rate": 8.226908386680296e-05,
      "loss": 3.0666,
      "step": 16580
    },
    {
      "epoch": 2.6745123327422213,
      "grad_norm": 26.25,
      "learning_rate": 8.225832481575125e-05,
      "loss": 4.6617,
      "step": 16590
    },
    {
      "epoch": 2.6761244559084316,
      "grad_norm": 27.375,
      "learning_rate": 8.224756576469955e-05,
      "loss": 3.0949,
      "step": 16600
    },
    {
      "epoch": 2.6777365790746415,
      "grad_norm": 34.0,
      "learning_rate": 8.223680671364786e-05,
      "loss": 2.3138,
      "step": 16610
    },
    {
      "epoch": 2.6793487022408513,
      "grad_norm": 21.625,
      "learning_rate": 8.222604766259617e-05,
      "loss": 4.9801,
      "step": 16620
    },
    {
      "epoch": 2.680960825407061,
      "grad_norm": 26.375,
      "learning_rate": 8.221528861154446e-05,
      "loss": 2.1453,
      "step": 16630
    },
    {
      "epoch": 2.682572948573271,
      "grad_norm": 15.875,
      "learning_rate": 8.220452956049278e-05,
      "loss": 3.6357,
      "step": 16640
    },
    {
      "epoch": 2.684185071739481,
      "grad_norm": 34.25,
      "learning_rate": 8.219377050944107e-05,
      "loss": 3.6172,
      "step": 16650
    },
    {
      "epoch": 2.6857971949056907,
      "grad_norm": 38.25,
      "learning_rate": 8.218301145838937e-05,
      "loss": 2.4131,
      "step": 16660
    },
    {
      "epoch": 2.6874093180719005,
      "grad_norm": 65.0,
      "learning_rate": 8.217225240733768e-05,
      "loss": 2.5565,
      "step": 16670
    },
    {
      "epoch": 2.6890214412381104,
      "grad_norm": 31.75,
      "learning_rate": 8.216149335628597e-05,
      "loss": 2.7438,
      "step": 16680
    },
    {
      "epoch": 2.6906335644043207,
      "grad_norm": 36.25,
      "learning_rate": 8.215073430523428e-05,
      "loss": 2.1904,
      "step": 16690
    },
    {
      "epoch": 2.6922456875705305,
      "grad_norm": 134.0,
      "learning_rate": 8.21399752541826e-05,
      "loss": 2.5844,
      "step": 16700
    },
    {
      "epoch": 2.6938578107367404,
      "grad_norm": 41.75,
      "learning_rate": 8.212921620313089e-05,
      "loss": 2.2519,
      "step": 16710
    },
    {
      "epoch": 2.69546993390295,
      "grad_norm": 28.125,
      "learning_rate": 8.211845715207919e-05,
      "loss": 2.3941,
      "step": 16720
    },
    {
      "epoch": 2.69708205706916,
      "grad_norm": 27.0,
      "learning_rate": 8.21076981010275e-05,
      "loss": 3.2803,
      "step": 16730
    },
    {
      "epoch": 2.69869418023537,
      "grad_norm": 36.75,
      "learning_rate": 8.20969390499758e-05,
      "loss": 4.3704,
      "step": 16740
    },
    {
      "epoch": 2.7003063034015797,
      "grad_norm": 16.125,
      "learning_rate": 8.208617999892409e-05,
      "loss": 2.8619,
      "step": 16750
    },
    {
      "epoch": 2.70191842656779,
      "grad_norm": 57.0,
      "learning_rate": 8.20754209478724e-05,
      "loss": 2.6357,
      "step": 16760
    },
    {
      "epoch": 2.7035305497339994,
      "grad_norm": 4.90625,
      "learning_rate": 8.206466189682071e-05,
      "loss": 4.0531,
      "step": 16770
    },
    {
      "epoch": 2.7051426729002097,
      "grad_norm": 21.125,
      "learning_rate": 8.205390284576901e-05,
      "loss": 3.0736,
      "step": 16780
    },
    {
      "epoch": 2.7067547960664196,
      "grad_norm": 19.0,
      "learning_rate": 8.204314379471732e-05,
      "loss": 2.184,
      "step": 16790
    },
    {
      "epoch": 2.7083669192326294,
      "grad_norm": 15.5,
      "learning_rate": 8.203238474366561e-05,
      "loss": 1.1948,
      "step": 16800
    },
    {
      "epoch": 2.7099790423988392,
      "grad_norm": 19.75,
      "learning_rate": 8.202162569261391e-05,
      "loss": 2.95,
      "step": 16810
    },
    {
      "epoch": 2.711591165565049,
      "grad_norm": 36.75,
      "learning_rate": 8.201086664156222e-05,
      "loss": 1.7973,
      "step": 16820
    },
    {
      "epoch": 2.713203288731259,
      "grad_norm": 60.0,
      "learning_rate": 8.200010759051052e-05,
      "loss": 2.9055,
      "step": 16830
    },
    {
      "epoch": 2.7148154118974688,
      "grad_norm": 38.0,
      "learning_rate": 8.198934853945883e-05,
      "loss": 2.3446,
      "step": 16840
    },
    {
      "epoch": 2.716427535063679,
      "grad_norm": 18.0,
      "learning_rate": 8.197858948840714e-05,
      "loss": 1.8519,
      "step": 16850
    },
    {
      "epoch": 2.718039658229889,
      "grad_norm": 284.0,
      "learning_rate": 8.196783043735543e-05,
      "loss": 2.9555,
      "step": 16860
    },
    {
      "epoch": 2.7196517813960988,
      "grad_norm": 40.5,
      "learning_rate": 8.195707138630373e-05,
      "loss": 3.6633,
      "step": 16870
    },
    {
      "epoch": 2.7212639045623086,
      "grad_norm": 45.0,
      "learning_rate": 8.194631233525204e-05,
      "loss": 3.0726,
      "step": 16880
    },
    {
      "epoch": 2.7228760277285184,
      "grad_norm": 28.375,
      "learning_rate": 8.193555328420034e-05,
      "loss": 3.741,
      "step": 16890
    },
    {
      "epoch": 2.7244881508947283,
      "grad_norm": 52.75,
      "learning_rate": 8.192479423314863e-05,
      "loss": 2.5293,
      "step": 16900
    },
    {
      "epoch": 2.726100274060938,
      "grad_norm": 64.0,
      "learning_rate": 8.191403518209694e-05,
      "loss": 3.08,
      "step": 16910
    },
    {
      "epoch": 2.7277123972271484,
      "grad_norm": 102.0,
      "learning_rate": 8.190327613104525e-05,
      "loss": 3.2148,
      "step": 16920
    },
    {
      "epoch": 2.729324520393358,
      "grad_norm": 11.375,
      "learning_rate": 8.189251707999355e-05,
      "loss": 3.5828,
      "step": 16930
    },
    {
      "epoch": 2.730936643559568,
      "grad_norm": 31.125,
      "learning_rate": 8.188175802894185e-05,
      "loss": 4.5111,
      "step": 16940
    },
    {
      "epoch": 2.732548766725778,
      "grad_norm": 58.0,
      "learning_rate": 8.187099897789016e-05,
      "loss": 4.0645,
      "step": 16950
    },
    {
      "epoch": 2.734160889891988,
      "grad_norm": 43.75,
      "learning_rate": 8.186023992683845e-05,
      "loss": 3.7159,
      "step": 16960
    },
    {
      "epoch": 2.7357730130581976,
      "grad_norm": 36.0,
      "learning_rate": 8.184948087578675e-05,
      "loss": 2.3312,
      "step": 16970
    },
    {
      "epoch": 2.7373851362244075,
      "grad_norm": 21.25,
      "learning_rate": 8.183872182473506e-05,
      "loss": 1.5193,
      "step": 16980
    },
    {
      "epoch": 2.7389972593906173,
      "grad_norm": 16.75,
      "learning_rate": 8.182796277368337e-05,
      "loss": 3.9801,
      "step": 16990
    },
    {
      "epoch": 2.740609382556827,
      "grad_norm": 18.375,
      "learning_rate": 8.181720372263167e-05,
      "loss": 3.3295,
      "step": 17000
    },
    {
      "epoch": 2.7422215057230375,
      "grad_norm": 36.5,
      "learning_rate": 8.180644467157998e-05,
      "loss": 2.0314,
      "step": 17010
    },
    {
      "epoch": 2.743833628889247,
      "grad_norm": 40.5,
      "learning_rate": 8.179568562052827e-05,
      "loss": 4.1715,
      "step": 17020
    },
    {
      "epoch": 2.745445752055457,
      "grad_norm": 62.75,
      "learning_rate": 8.178492656947657e-05,
      "loss": 2.7089,
      "step": 17030
    },
    {
      "epoch": 2.747057875221667,
      "grad_norm": 69.5,
      "learning_rate": 8.177416751842488e-05,
      "loss": 3.8797,
      "step": 17040
    },
    {
      "epoch": 2.748669998387877,
      "grad_norm": 20.125,
      "learning_rate": 8.176340846737318e-05,
      "loss": 5.2508,
      "step": 17050
    },
    {
      "epoch": 2.7502821215540867,
      "grad_norm": 43.5,
      "learning_rate": 8.175264941632149e-05,
      "loss": 2.7449,
      "step": 17060
    },
    {
      "epoch": 2.7518942447202965,
      "grad_norm": 74.5,
      "learning_rate": 8.17418903652698e-05,
      "loss": 2.9238,
      "step": 17070
    },
    {
      "epoch": 2.7535063678865064,
      "grad_norm": 38.75,
      "learning_rate": 8.173113131421809e-05,
      "loss": 2.9961,
      "step": 17080
    },
    {
      "epoch": 2.7551184910527162,
      "grad_norm": 118.0,
      "learning_rate": 8.172037226316639e-05,
      "loss": 3.6875,
      "step": 17090
    },
    {
      "epoch": 2.7567306142189265,
      "grad_norm": 64.0,
      "learning_rate": 8.17096132121147e-05,
      "loss": 2.0133,
      "step": 17100
    },
    {
      "epoch": 2.7583427373851364,
      "grad_norm": 29.625,
      "learning_rate": 8.1698854161063e-05,
      "loss": 2.3029,
      "step": 17110
    },
    {
      "epoch": 2.759954860551346,
      "grad_norm": 18.875,
      "learning_rate": 8.168809511001129e-05,
      "loss": 3.3793,
      "step": 17120
    },
    {
      "epoch": 2.761566983717556,
      "grad_norm": 18.5,
      "learning_rate": 8.16773360589596e-05,
      "loss": 3.4162,
      "step": 17130
    },
    {
      "epoch": 2.763179106883766,
      "grad_norm": 33.0,
      "learning_rate": 8.166657700790791e-05,
      "loss": 4.1713,
      "step": 17140
    },
    {
      "epoch": 2.7647912300499757,
      "grad_norm": 17.875,
      "learning_rate": 8.165581795685621e-05,
      "loss": 3.2405,
      "step": 17150
    },
    {
      "epoch": 2.7664033532161856,
      "grad_norm": 36.5,
      "learning_rate": 8.164505890580452e-05,
      "loss": 2.9978,
      "step": 17160
    },
    {
      "epoch": 2.768015476382396,
      "grad_norm": 35.75,
      "learning_rate": 8.163429985475281e-05,
      "loss": 2.7922,
      "step": 17170
    },
    {
      "epoch": 2.7696275995486053,
      "grad_norm": 30.5,
      "learning_rate": 8.162354080370111e-05,
      "loss": 2.9703,
      "step": 17180
    },
    {
      "epoch": 2.7712397227148156,
      "grad_norm": 17.875,
      "learning_rate": 8.161278175264942e-05,
      "loss": 3.5887,
      "step": 17190
    },
    {
      "epoch": 2.7728518458810254,
      "grad_norm": 27.5,
      "learning_rate": 8.160202270159772e-05,
      "loss": 2.9646,
      "step": 17200
    },
    {
      "epoch": 2.7744639690472352,
      "grad_norm": 36.25,
      "learning_rate": 8.159126365054603e-05,
      "loss": 2.7605,
      "step": 17210
    },
    {
      "epoch": 2.776076092213445,
      "grad_norm": 25.375,
      "learning_rate": 8.158050459949434e-05,
      "loss": 2.3061,
      "step": 17220
    },
    {
      "epoch": 2.777688215379655,
      "grad_norm": 50.0,
      "learning_rate": 8.156974554844263e-05,
      "loss": 2.9283,
      "step": 17230
    },
    {
      "epoch": 2.779300338545865,
      "grad_norm": 63.25,
      "learning_rate": 8.155898649739093e-05,
      "loss": 3.4439,
      "step": 17240
    },
    {
      "epoch": 2.7809124617120746,
      "grad_norm": 32.0,
      "learning_rate": 8.154822744633923e-05,
      "loss": 2.6823,
      "step": 17250
    },
    {
      "epoch": 2.782524584878285,
      "grad_norm": 49.5,
      "learning_rate": 8.153746839528754e-05,
      "loss": 4.2188,
      "step": 17260
    },
    {
      "epoch": 2.7841367080444948,
      "grad_norm": 212.0,
      "learning_rate": 8.152670934423583e-05,
      "loss": 3.075,
      "step": 17270
    },
    {
      "epoch": 2.7857488312107046,
      "grad_norm": 13.625,
      "learning_rate": 8.151595029318414e-05,
      "loss": 2.3713,
      "step": 17280
    },
    {
      "epoch": 2.7873609543769144,
      "grad_norm": 13.5,
      "learning_rate": 8.150519124213245e-05,
      "loss": 3.5363,
      "step": 17290
    },
    {
      "epoch": 2.7889730775431243,
      "grad_norm": 42.75,
      "learning_rate": 8.149443219108075e-05,
      "loss": 4.6967,
      "step": 17300
    },
    {
      "epoch": 2.790585200709334,
      "grad_norm": 22.125,
      "learning_rate": 8.148367314002905e-05,
      "loss": 3.5566,
      "step": 17310
    },
    {
      "epoch": 2.792197323875544,
      "grad_norm": 7.4375,
      "learning_rate": 8.147291408897736e-05,
      "loss": 2.7062,
      "step": 17320
    },
    {
      "epoch": 2.793809447041754,
      "grad_norm": 15.125,
      "learning_rate": 8.146215503792565e-05,
      "loss": 3.0055,
      "step": 17330
    },
    {
      "epoch": 2.7954215702079637,
      "grad_norm": 28.25,
      "learning_rate": 8.145139598687396e-05,
      "loss": 3.7766,
      "step": 17340
    },
    {
      "epoch": 2.797033693374174,
      "grad_norm": 113.0,
      "learning_rate": 8.144063693582227e-05,
      "loss": 3.0504,
      "step": 17350
    },
    {
      "epoch": 2.798645816540384,
      "grad_norm": 25.25,
      "learning_rate": 8.142987788477057e-05,
      "loss": 2.0411,
      "step": 17360
    },
    {
      "epoch": 2.8002579397065936,
      "grad_norm": 53.5,
      "learning_rate": 8.141911883371887e-05,
      "loss": 3.4166,
      "step": 17370
    },
    {
      "epoch": 2.8018700628728035,
      "grad_norm": 77.5,
      "learning_rate": 8.140835978266718e-05,
      "loss": 3.6629,
      "step": 17380
    },
    {
      "epoch": 2.8034821860390133,
      "grad_norm": 39.5,
      "learning_rate": 8.139760073161547e-05,
      "loss": 2.8213,
      "step": 17390
    },
    {
      "epoch": 2.805094309205223,
      "grad_norm": 32.75,
      "learning_rate": 8.138684168056377e-05,
      "loss": 2.3582,
      "step": 17400
    },
    {
      "epoch": 2.806706432371433,
      "grad_norm": 23.875,
      "learning_rate": 8.137608262951208e-05,
      "loss": 1.9094,
      "step": 17410
    },
    {
      "epoch": 2.8083185555376433,
      "grad_norm": 178.0,
      "learning_rate": 8.136532357846039e-05,
      "loss": 2.7445,
      "step": 17420
    },
    {
      "epoch": 2.8099306787038527,
      "grad_norm": 17.25,
      "learning_rate": 8.135456452740869e-05,
      "loss": 1.9334,
      "step": 17430
    },
    {
      "epoch": 2.811542801870063,
      "grad_norm": 17.125,
      "learning_rate": 8.1343805476357e-05,
      "loss": 2.4237,
      "step": 17440
    },
    {
      "epoch": 2.813154925036273,
      "grad_norm": 41.5,
      "learning_rate": 8.13330464253053e-05,
      "loss": 3.4487,
      "step": 17450
    },
    {
      "epoch": 2.8147670482024827,
      "grad_norm": 13.1875,
      "learning_rate": 8.132228737425359e-05,
      "loss": 3.1195,
      "step": 17460
    },
    {
      "epoch": 2.8163791713686925,
      "grad_norm": 27.25,
      "learning_rate": 8.13115283232019e-05,
      "loss": 2.5039,
      "step": 17470
    },
    {
      "epoch": 2.8179912945349024,
      "grad_norm": 103.0,
      "learning_rate": 8.13007692721502e-05,
      "loss": 3.6488,
      "step": 17480
    },
    {
      "epoch": 2.8196034177011122,
      "grad_norm": 46.25,
      "learning_rate": 8.12900102210985e-05,
      "loss": 4.5543,
      "step": 17490
    },
    {
      "epoch": 2.821215540867322,
      "grad_norm": 37.25,
      "learning_rate": 8.127925117004682e-05,
      "loss": 3.9375,
      "step": 17500
    },
    {
      "epoch": 2.8228276640335324,
      "grad_norm": 34.0,
      "learning_rate": 8.126849211899511e-05,
      "loss": 3.1203,
      "step": 17510
    },
    {
      "epoch": 2.824439787199742,
      "grad_norm": 32.5,
      "learning_rate": 8.125773306794341e-05,
      "loss": 4.3293,
      "step": 17520
    },
    {
      "epoch": 2.826051910365952,
      "grad_norm": 22.375,
      "learning_rate": 8.124697401689172e-05,
      "loss": 3.6277,
      "step": 17530
    },
    {
      "epoch": 2.827664033532162,
      "grad_norm": 88.0,
      "learning_rate": 8.123621496584002e-05,
      "loss": 2.0905,
      "step": 17540
    },
    {
      "epoch": 2.8292761566983717,
      "grad_norm": 28.75,
      "learning_rate": 8.122545591478831e-05,
      "loss": 2.8172,
      "step": 17550
    },
    {
      "epoch": 2.8308882798645816,
      "grad_norm": 12.9375,
      "learning_rate": 8.121469686373662e-05,
      "loss": 1.9369,
      "step": 17560
    },
    {
      "epoch": 2.8325004030307914,
      "grad_norm": 24.0,
      "learning_rate": 8.120393781268493e-05,
      "loss": 1.3679,
      "step": 17570
    },
    {
      "epoch": 2.8341125261970017,
      "grad_norm": 117.5,
      "learning_rate": 8.119317876163323e-05,
      "loss": 2.5914,
      "step": 17580
    },
    {
      "epoch": 2.835724649363211,
      "grad_norm": 50.5,
      "learning_rate": 8.118241971058153e-05,
      "loss": 3.2113,
      "step": 17590
    },
    {
      "epoch": 2.8373367725294214,
      "grad_norm": 32.75,
      "learning_rate": 8.117166065952984e-05,
      "loss": 5.4271,
      "step": 17600
    },
    {
      "epoch": 2.8389488956956312,
      "grad_norm": 11.4375,
      "learning_rate": 8.116090160847813e-05,
      "loss": 3.6824,
      "step": 17610
    },
    {
      "epoch": 2.840561018861841,
      "grad_norm": 36.25,
      "learning_rate": 8.115014255742643e-05,
      "loss": 2.5672,
      "step": 17620
    },
    {
      "epoch": 2.842173142028051,
      "grad_norm": 15.125,
      "learning_rate": 8.113938350637474e-05,
      "loss": 2.8764,
      "step": 17630
    },
    {
      "epoch": 2.843785265194261,
      "grad_norm": 14.5625,
      "learning_rate": 8.112862445532305e-05,
      "loss": 1.803,
      "step": 17640
    },
    {
      "epoch": 2.8453973883604706,
      "grad_norm": 9.5,
      "learning_rate": 8.111786540427135e-05,
      "loss": 3.5852,
      "step": 17650
    },
    {
      "epoch": 2.8470095115266805,
      "grad_norm": 46.25,
      "learning_rate": 8.110710635321966e-05,
      "loss": 2.7592,
      "step": 17660
    },
    {
      "epoch": 2.8486216346928908,
      "grad_norm": 1.1484375,
      "learning_rate": 8.109634730216795e-05,
      "loss": 2.4202,
      "step": 17670
    },
    {
      "epoch": 2.8502337578591,
      "grad_norm": 36.25,
      "learning_rate": 8.108558825111625e-05,
      "loss": 2.8744,
      "step": 17680
    },
    {
      "epoch": 2.8518458810253104,
      "grad_norm": 22.875,
      "learning_rate": 8.107482920006456e-05,
      "loss": 2.7271,
      "step": 17690
    },
    {
      "epoch": 2.8534580041915203,
      "grad_norm": 31.75,
      "learning_rate": 8.106407014901285e-05,
      "loss": 2.6178,
      "step": 17700
    },
    {
      "epoch": 2.85507012735773,
      "grad_norm": 30.25,
      "learning_rate": 8.105331109796116e-05,
      "loss": 3.7191,
      "step": 17710
    },
    {
      "epoch": 2.85668225052394,
      "grad_norm": 11.9375,
      "learning_rate": 8.104255204690948e-05,
      "loss": 2.1561,
      "step": 17720
    },
    {
      "epoch": 2.85829437369015,
      "grad_norm": 21.25,
      "learning_rate": 8.103179299585777e-05,
      "loss": 1.484,
      "step": 17730
    },
    {
      "epoch": 2.8599064968563597,
      "grad_norm": 7.15625,
      "learning_rate": 8.102103394480607e-05,
      "loss": 1.7517,
      "step": 17740
    },
    {
      "epoch": 2.8615186200225695,
      "grad_norm": 38.75,
      "learning_rate": 8.101027489375438e-05,
      "loss": 2.3988,
      "step": 17750
    },
    {
      "epoch": 2.86313074318878,
      "grad_norm": 0.55078125,
      "learning_rate": 8.099951584270267e-05,
      "loss": 4.1082,
      "step": 17760
    },
    {
      "epoch": 2.8647428663549896,
      "grad_norm": 43.25,
      "learning_rate": 8.098875679165097e-05,
      "loss": 1.8887,
      "step": 17770
    },
    {
      "epoch": 2.8663549895211995,
      "grad_norm": 31.25,
      "learning_rate": 8.097799774059928e-05,
      "loss": 3.0797,
      "step": 17780
    },
    {
      "epoch": 2.8679671126874093,
      "grad_norm": 53.5,
      "learning_rate": 8.096723868954759e-05,
      "loss": 1.6987,
      "step": 17790
    },
    {
      "epoch": 2.869579235853619,
      "grad_norm": 34.5,
      "learning_rate": 8.095647963849589e-05,
      "loss": 3.0344,
      "step": 17800
    },
    {
      "epoch": 2.871191359019829,
      "grad_norm": 47.5,
      "learning_rate": 8.09457205874442e-05,
      "loss": 3.0324,
      "step": 17810
    },
    {
      "epoch": 2.872803482186039,
      "grad_norm": 22.625,
      "learning_rate": 8.09349615363925e-05,
      "loss": 1.065,
      "step": 17820
    },
    {
      "epoch": 2.874415605352249,
      "grad_norm": 98.0,
      "learning_rate": 8.092420248534079e-05,
      "loss": 2.0407,
      "step": 17830
    },
    {
      "epoch": 2.8760277285184586,
      "grad_norm": 58.5,
      "learning_rate": 8.09134434342891e-05,
      "loss": 2.5359,
      "step": 17840
    },
    {
      "epoch": 2.877639851684669,
      "grad_norm": 35.25,
      "learning_rate": 8.09026843832374e-05,
      "loss": 3.4584,
      "step": 17850
    },
    {
      "epoch": 2.8792519748508787,
      "grad_norm": 26.0,
      "learning_rate": 8.089192533218571e-05,
      "loss": 2.3762,
      "step": 17860
    },
    {
      "epoch": 2.8808640980170885,
      "grad_norm": 137.0,
      "learning_rate": 8.088116628113402e-05,
      "loss": 3.2195,
      "step": 17870
    },
    {
      "epoch": 2.8824762211832984,
      "grad_norm": 0.0,
      "learning_rate": 8.087040723008231e-05,
      "loss": 3.5118,
      "step": 17880
    },
    {
      "epoch": 2.8840883443495082,
      "grad_norm": 50.75,
      "learning_rate": 8.085964817903061e-05,
      "loss": 3.167,
      "step": 17890
    },
    {
      "epoch": 2.885700467515718,
      "grad_norm": 43.25,
      "learning_rate": 8.084888912797891e-05,
      "loss": 3.2375,
      "step": 17900
    },
    {
      "epoch": 2.887312590681928,
      "grad_norm": 44.75,
      "learning_rate": 8.083813007692722e-05,
      "loss": 2.7873,
      "step": 17910
    },
    {
      "epoch": 2.888924713848138,
      "grad_norm": 31.625,
      "learning_rate": 8.082737102587551e-05,
      "loss": 1.5108,
      "step": 17920
    },
    {
      "epoch": 2.890536837014348,
      "grad_norm": 38.0,
      "learning_rate": 8.081661197482382e-05,
      "loss": 3.0342,
      "step": 17930
    },
    {
      "epoch": 2.892148960180558,
      "grad_norm": 90.5,
      "learning_rate": 8.080585292377213e-05,
      "loss": 4.4551,
      "step": 17940
    },
    {
      "epoch": 2.8937610833467677,
      "grad_norm": 99.5,
      "learning_rate": 8.079509387272043e-05,
      "loss": 1.4506,
      "step": 17950
    },
    {
      "epoch": 2.8953732065129776,
      "grad_norm": 9.375,
      "learning_rate": 8.078433482166873e-05,
      "loss": 2.0431,
      "step": 17960
    },
    {
      "epoch": 2.8969853296791874,
      "grad_norm": 12.5,
      "learning_rate": 8.077357577061704e-05,
      "loss": 3.1307,
      "step": 17970
    },
    {
      "epoch": 2.8985974528453973,
      "grad_norm": 10.125,
      "learning_rate": 8.076281671956533e-05,
      "loss": 3.6744,
      "step": 17980
    },
    {
      "epoch": 2.9002095760116076,
      "grad_norm": 183.0,
      "learning_rate": 8.075205766851364e-05,
      "loss": 2.9089,
      "step": 17990
    },
    {
      "epoch": 2.901821699177817,
      "grad_norm": 19.125,
      "learning_rate": 8.074129861746195e-05,
      "loss": 3.13,
      "step": 18000
    },
    {
      "epoch": 2.9034338223440272,
      "grad_norm": 13.5625,
      "learning_rate": 8.073053956641025e-05,
      "loss": 1.7013,
      "step": 18010
    },
    {
      "epoch": 2.905045945510237,
      "grad_norm": 22.125,
      "learning_rate": 8.071978051535855e-05,
      "loss": 4.9648,
      "step": 18020
    },
    {
      "epoch": 2.906658068676447,
      "grad_norm": 262.0,
      "learning_rate": 8.070902146430686e-05,
      "loss": 2.3762,
      "step": 18030
    },
    {
      "epoch": 2.908270191842657,
      "grad_norm": 23.125,
      "learning_rate": 8.069826241325515e-05,
      "loss": 3.7634,
      "step": 18040
    },
    {
      "epoch": 2.9098823150088666,
      "grad_norm": 39.5,
      "learning_rate": 8.068750336220345e-05,
      "loss": 2.897,
      "step": 18050
    },
    {
      "epoch": 2.9114944381750765,
      "grad_norm": 23.125,
      "learning_rate": 8.067674431115176e-05,
      "loss": 1.9164,
      "step": 18060
    },
    {
      "epoch": 2.9131065613412863,
      "grad_norm": 31.125,
      "learning_rate": 8.066598526010007e-05,
      "loss": 4.3666,
      "step": 18070
    },
    {
      "epoch": 2.9147186845074966,
      "grad_norm": 38.25,
      "learning_rate": 8.065522620904837e-05,
      "loss": 3.5707,
      "step": 18080
    },
    {
      "epoch": 2.916330807673706,
      "grad_norm": 34.5,
      "learning_rate": 8.064446715799668e-05,
      "loss": 4.4662,
      "step": 18090
    },
    {
      "epoch": 2.9179429308399163,
      "grad_norm": 21.125,
      "learning_rate": 8.063370810694497e-05,
      "loss": 1.6874,
      "step": 18100
    },
    {
      "epoch": 2.919555054006126,
      "grad_norm": 9.125,
      "learning_rate": 8.062294905589327e-05,
      "loss": 2.5172,
      "step": 18110
    },
    {
      "epoch": 2.921167177172336,
      "grad_norm": 9.0,
      "learning_rate": 8.061219000484158e-05,
      "loss": 1.6799,
      "step": 18120
    },
    {
      "epoch": 2.922779300338546,
      "grad_norm": 31.125,
      "learning_rate": 8.060143095378988e-05,
      "loss": 2.3773,
      "step": 18130
    },
    {
      "epoch": 2.9243914235047557,
      "grad_norm": 15.5625,
      "learning_rate": 8.059067190273819e-05,
      "loss": 2.626,
      "step": 18140
    },
    {
      "epoch": 2.9260035466709655,
      "grad_norm": 157.0,
      "learning_rate": 8.05799128516865e-05,
      "loss": 2.0843,
      "step": 18150
    },
    {
      "epoch": 2.9276156698371754,
      "grad_norm": 27.0,
      "learning_rate": 8.056915380063479e-05,
      "loss": 2.0953,
      "step": 18160
    },
    {
      "epoch": 2.9292277930033856,
      "grad_norm": 48.5,
      "learning_rate": 8.055839474958309e-05,
      "loss": 2.8119,
      "step": 18170
    },
    {
      "epoch": 2.9308399161695955,
      "grad_norm": 20.25,
      "learning_rate": 8.05476356985314e-05,
      "loss": 5.007,
      "step": 18180
    },
    {
      "epoch": 2.9324520393358053,
      "grad_norm": 41.75,
      "learning_rate": 8.05368766474797e-05,
      "loss": 2.1664,
      "step": 18190
    },
    {
      "epoch": 2.934064162502015,
      "grad_norm": 58.5,
      "learning_rate": 8.052611759642799e-05,
      "loss": 2.2963,
      "step": 18200
    },
    {
      "epoch": 2.935676285668225,
      "grad_norm": 30.0,
      "learning_rate": 8.05153585453763e-05,
      "loss": 3.6154,
      "step": 18210
    },
    {
      "epoch": 2.937288408834435,
      "grad_norm": 12.0,
      "learning_rate": 8.050459949432461e-05,
      "loss": 3.249,
      "step": 18220
    },
    {
      "epoch": 2.9389005320006447,
      "grad_norm": 23.625,
      "learning_rate": 8.049384044327291e-05,
      "loss": 3.1103,
      "step": 18230
    },
    {
      "epoch": 2.940512655166855,
      "grad_norm": 135.0,
      "learning_rate": 8.04830813922212e-05,
      "loss": 2.8797,
      "step": 18240
    },
    {
      "epoch": 2.9421247783330644,
      "grad_norm": 0.0,
      "learning_rate": 8.047232234116951e-05,
      "loss": 2.1361,
      "step": 18250
    },
    {
      "epoch": 2.9437369014992747,
      "grad_norm": 34.5,
      "learning_rate": 8.046156329011781e-05,
      "loss": 2.5249,
      "step": 18260
    },
    {
      "epoch": 2.9453490246654845,
      "grad_norm": 46.75,
      "learning_rate": 8.045080423906611e-05,
      "loss": 1.9432,
      "step": 18270
    },
    {
      "epoch": 2.9469611478316944,
      "grad_norm": 209.0,
      "learning_rate": 8.044004518801442e-05,
      "loss": 2.2078,
      "step": 18280
    },
    {
      "epoch": 2.9485732709979042,
      "grad_norm": 52.5,
      "learning_rate": 8.042928613696273e-05,
      "loss": 5.054,
      "step": 18290
    },
    {
      "epoch": 2.950185394164114,
      "grad_norm": 24.0,
      "learning_rate": 8.041852708591102e-05,
      "loss": 2.519,
      "step": 18300
    },
    {
      "epoch": 2.951797517330324,
      "grad_norm": 1.8515625,
      "learning_rate": 8.040776803485933e-05,
      "loss": 3.2753,
      "step": 18310
    },
    {
      "epoch": 2.9534096404965338,
      "grad_norm": 33.25,
      "learning_rate": 8.039700898380763e-05,
      "loss": 4.7422,
      "step": 18320
    },
    {
      "epoch": 2.955021763662744,
      "grad_norm": 48.25,
      "learning_rate": 8.038624993275593e-05,
      "loss": 2.5469,
      "step": 18330
    },
    {
      "epoch": 2.956633886828954,
      "grad_norm": 28.25,
      "learning_rate": 8.037549088170424e-05,
      "loss": 2.0629,
      "step": 18340
    },
    {
      "epoch": 2.9582460099951637,
      "grad_norm": 38.0,
      "learning_rate": 8.036473183065253e-05,
      "loss": 3.16,
      "step": 18350
    },
    {
      "epoch": 2.9598581331613736,
      "grad_norm": 18.0,
      "learning_rate": 8.035397277960084e-05,
      "loss": 2.3532,
      "step": 18360
    },
    {
      "epoch": 2.9614702563275834,
      "grad_norm": 53.0,
      "learning_rate": 8.034321372854915e-05,
      "loss": 3.1508,
      "step": 18370
    },
    {
      "epoch": 2.9630823794937933,
      "grad_norm": 223.0,
      "learning_rate": 8.033245467749745e-05,
      "loss": 3.4734,
      "step": 18380
    },
    {
      "epoch": 2.964694502660003,
      "grad_norm": 182.0,
      "learning_rate": 8.032169562644575e-05,
      "loss": 3.2215,
      "step": 18390
    },
    {
      "epoch": 2.966306625826213,
      "grad_norm": 41.5,
      "learning_rate": 8.031093657539406e-05,
      "loss": 2.3414,
      "step": 18400
    },
    {
      "epoch": 2.967918748992423,
      "grad_norm": 37.25,
      "learning_rate": 8.030017752434235e-05,
      "loss": 3.5307,
      "step": 18410
    },
    {
      "epoch": 2.969530872158633,
      "grad_norm": 26.5,
      "learning_rate": 8.028941847329065e-05,
      "loss": 1.3276,
      "step": 18420
    },
    {
      "epoch": 2.971142995324843,
      "grad_norm": 10.5,
      "learning_rate": 8.027865942223896e-05,
      "loss": 2.0234,
      "step": 18430
    },
    {
      "epoch": 2.972755118491053,
      "grad_norm": 0.0,
      "learning_rate": 8.026790037118727e-05,
      "loss": 1.3044,
      "step": 18440
    },
    {
      "epoch": 2.9743672416572626,
      "grad_norm": 20.5,
      "learning_rate": 8.025714132013557e-05,
      "loss": 3.2266,
      "step": 18450
    },
    {
      "epoch": 2.9759793648234725,
      "grad_norm": 40.5,
      "learning_rate": 8.024638226908388e-05,
      "loss": 2.2589,
      "step": 18460
    },
    {
      "epoch": 2.9775914879896823,
      "grad_norm": 17.75,
      "learning_rate": 8.023562321803217e-05,
      "loss": 3.631,
      "step": 18470
    },
    {
      "epoch": 2.979203611155892,
      "grad_norm": 38.0,
      "learning_rate": 8.022486416698047e-05,
      "loss": 2.2475,
      "step": 18480
    },
    {
      "epoch": 2.9808157343221025,
      "grad_norm": 79.5,
      "learning_rate": 8.021410511592878e-05,
      "loss": 3.9871,
      "step": 18490
    },
    {
      "epoch": 2.982427857488312,
      "grad_norm": 32.5,
      "learning_rate": 8.020334606487708e-05,
      "loss": 2.1063,
      "step": 18500
    },
    {
      "epoch": 2.984039980654522,
      "grad_norm": 73.5,
      "learning_rate": 8.019258701382539e-05,
      "loss": 3.4245,
      "step": 18510
    },
    {
      "epoch": 2.985652103820732,
      "grad_norm": 25.0,
      "learning_rate": 8.01818279627737e-05,
      "loss": 3.4371,
      "step": 18520
    },
    {
      "epoch": 2.987264226986942,
      "grad_norm": 18.0,
      "learning_rate": 8.017106891172199e-05,
      "loss": 2.3785,
      "step": 18530
    },
    {
      "epoch": 2.9888763501531517,
      "grad_norm": 25.625,
      "learning_rate": 8.016030986067029e-05,
      "loss": 2.5141,
      "step": 18540
    },
    {
      "epoch": 2.9904884733193615,
      "grad_norm": 1.5078125,
      "learning_rate": 8.01495508096186e-05,
      "loss": 2.5139,
      "step": 18550
    },
    {
      "epoch": 2.9921005964855714,
      "grad_norm": 13.3125,
      "learning_rate": 8.01387917585669e-05,
      "loss": 2.1674,
      "step": 18560
    },
    {
      "epoch": 2.993712719651781,
      "grad_norm": 17.375,
      "learning_rate": 8.012803270751519e-05,
      "loss": 1.8372,
      "step": 18570
    },
    {
      "epoch": 2.9953248428179915,
      "grad_norm": 40.0,
      "learning_rate": 8.01172736564635e-05,
      "loss": 2.2277,
      "step": 18580
    },
    {
      "epoch": 2.9969369659842013,
      "grad_norm": 22.0,
      "learning_rate": 8.010651460541181e-05,
      "loss": 2.675,
      "step": 18590
    },
    {
      "epoch": 2.998549089150411,
      "grad_norm": 0.072265625,
      "learning_rate": 8.009575555436011e-05,
      "loss": 2.8899,
      "step": 18600
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.1202149391174316,
      "eval_runtime": 23.9801,
      "eval_samples_per_second": 4.17,
      "eval_steps_per_second": 4.17,
      "step": 18609
    },
    {
      "epoch": 3.000161212316621,
      "grad_norm": 1.4921875,
      "learning_rate": 8.00849965033084e-05,
      "loss": 2.1203,
      "step": 18610
    },
    {
      "epoch": 3.001773335482831,
      "grad_norm": 7.59375,
      "learning_rate": 8.007423745225672e-05,
      "loss": 2.0567,
      "step": 18620
    },
    {
      "epoch": 3.0033854586490407,
      "grad_norm": 181.0,
      "learning_rate": 8.006347840120501e-05,
      "loss": 1.0679,
      "step": 18630
    },
    {
      "epoch": 3.0049975818152506,
      "grad_norm": 19.625,
      "learning_rate": 8.005271935015331e-05,
      "loss": 2.6075,
      "step": 18640
    },
    {
      "epoch": 3.0066097049814604,
      "grad_norm": 25.125,
      "learning_rate": 8.004196029910163e-05,
      "loss": 3.8133,
      "step": 18650
    },
    {
      "epoch": 3.0082218281476707,
      "grad_norm": 23.125,
      "learning_rate": 8.003120124804993e-05,
      "loss": 3.3518,
      "step": 18660
    },
    {
      "epoch": 3.0098339513138805,
      "grad_norm": 28.125,
      "learning_rate": 8.002044219699823e-05,
      "loss": 2.4781,
      "step": 18670
    },
    {
      "epoch": 3.0114460744800904,
      "grad_norm": 21.0,
      "learning_rate": 8.000968314594654e-05,
      "loss": 2.3966,
      "step": 18680
    },
    {
      "epoch": 3.0130581976463002,
      "grad_norm": 7.8125,
      "learning_rate": 7.999892409489483e-05,
      "loss": 2.7607,
      "step": 18690
    },
    {
      "epoch": 3.01467032081251,
      "grad_norm": 21.25,
      "learning_rate": 7.998816504384313e-05,
      "loss": 1.8349,
      "step": 18700
    },
    {
      "epoch": 3.01628244397872,
      "grad_norm": 0.71484375,
      "learning_rate": 7.997740599279144e-05,
      "loss": 1.9138,
      "step": 18710
    },
    {
      "epoch": 3.0178945671449298,
      "grad_norm": 28.375,
      "learning_rate": 7.996664694173975e-05,
      "loss": 1.0578,
      "step": 18720
    },
    {
      "epoch": 3.0195066903111396,
      "grad_norm": 24.25,
      "learning_rate": 7.995588789068805e-05,
      "loss": 2.5493,
      "step": 18730
    },
    {
      "epoch": 3.0211188134773495,
      "grad_norm": 22.75,
      "learning_rate": 7.994512883963636e-05,
      "loss": 1.1395,
      "step": 18740
    },
    {
      "epoch": 3.0227309366435597,
      "grad_norm": 24.25,
      "learning_rate": 7.993436978858465e-05,
      "loss": 2.4814,
      "step": 18750
    },
    {
      "epoch": 3.0243430598097696,
      "grad_norm": 178.0,
      "learning_rate": 7.992361073753295e-05,
      "loss": 2.0344,
      "step": 18760
    },
    {
      "epoch": 3.0259551829759794,
      "grad_norm": 20.25,
      "learning_rate": 7.991285168648126e-05,
      "loss": 1.3295,
      "step": 18770
    },
    {
      "epoch": 3.0275673061421893,
      "grad_norm": 39.75,
      "learning_rate": 7.990209263542955e-05,
      "loss": 2.9132,
      "step": 18780
    },
    {
      "epoch": 3.029179429308399,
      "grad_norm": 9.5,
      "learning_rate": 7.989133358437786e-05,
      "loss": 2.1159,
      "step": 18790
    },
    {
      "epoch": 3.030791552474609,
      "grad_norm": 57.5,
      "learning_rate": 7.988057453332617e-05,
      "loss": 3.1035,
      "step": 18800
    },
    {
      "epoch": 3.032403675640819,
      "grad_norm": 25.125,
      "learning_rate": 7.986981548227447e-05,
      "loss": 2.2835,
      "step": 18810
    },
    {
      "epoch": 3.0340157988070287,
      "grad_norm": 15.3125,
      "learning_rate": 7.985905643122277e-05,
      "loss": 1.9242,
      "step": 18820
    },
    {
      "epoch": 3.035627921973239,
      "grad_norm": 36.5,
      "learning_rate": 7.984829738017108e-05,
      "loss": 1.5198,
      "step": 18830
    },
    {
      "epoch": 3.037240045139449,
      "grad_norm": 26.75,
      "learning_rate": 7.983753832911937e-05,
      "loss": 1.7424,
      "step": 18840
    },
    {
      "epoch": 3.0388521683056586,
      "grad_norm": 19.125,
      "learning_rate": 7.982677927806767e-05,
      "loss": 1.226,
      "step": 18850
    },
    {
      "epoch": 3.0404642914718685,
      "grad_norm": 43.0,
      "learning_rate": 7.981602022701598e-05,
      "loss": 3.1431,
      "step": 18860
    },
    {
      "epoch": 3.0420764146380783,
      "grad_norm": 12.0,
      "learning_rate": 7.980526117596429e-05,
      "loss": 2.031,
      "step": 18870
    },
    {
      "epoch": 3.043688537804288,
      "grad_norm": 30.5,
      "learning_rate": 7.979450212491259e-05,
      "loss": 2.2586,
      "step": 18880
    },
    {
      "epoch": 3.045300660970498,
      "grad_norm": 10.9375,
      "learning_rate": 7.978374307386088e-05,
      "loss": 1.9953,
      "step": 18890
    },
    {
      "epoch": 3.046912784136708,
      "grad_norm": 36.0,
      "learning_rate": 7.97729840228092e-05,
      "loss": 3.1212,
      "step": 18900
    },
    {
      "epoch": 3.048524907302918,
      "grad_norm": 42.75,
      "learning_rate": 7.976222497175749e-05,
      "loss": 1.8311,
      "step": 18910
    },
    {
      "epoch": 3.050137030469128,
      "grad_norm": 27.375,
      "learning_rate": 7.975146592070579e-05,
      "loss": 1.0144,
      "step": 18920
    },
    {
      "epoch": 3.051749153635338,
      "grad_norm": 12.3125,
      "learning_rate": 7.97407068696541e-05,
      "loss": 1.9352,
      "step": 18930
    },
    {
      "epoch": 3.0533612768015477,
      "grad_norm": 34.5,
      "learning_rate": 7.972994781860241e-05,
      "loss": 1.4801,
      "step": 18940
    },
    {
      "epoch": 3.0549733999677575,
      "grad_norm": 42.5,
      "learning_rate": 7.97191887675507e-05,
      "loss": 3.5992,
      "step": 18950
    },
    {
      "epoch": 3.0565855231339674,
      "grad_norm": 112.5,
      "learning_rate": 7.970842971649901e-05,
      "loss": 2.53,
      "step": 18960
    },
    {
      "epoch": 3.058197646300177,
      "grad_norm": 41.75,
      "learning_rate": 7.969767066544731e-05,
      "loss": 2.3496,
      "step": 18970
    },
    {
      "epoch": 3.059809769466387,
      "grad_norm": 13.9375,
      "learning_rate": 7.968691161439561e-05,
      "loss": 2.3193,
      "step": 18980
    },
    {
      "epoch": 3.0614218926325973,
      "grad_norm": 0.30078125,
      "learning_rate": 7.967615256334392e-05,
      "loss": 1.4733,
      "step": 18990
    },
    {
      "epoch": 3.063034015798807,
      "grad_norm": 17.5,
      "learning_rate": 7.966539351229221e-05,
      "loss": 1.3624,
      "step": 19000
    },
    {
      "epoch": 3.064646138965017,
      "grad_norm": 78.5,
      "learning_rate": 7.965463446124052e-05,
      "loss": 1.6848,
      "step": 19010
    },
    {
      "epoch": 3.066258262131227,
      "grad_norm": 16.0,
      "learning_rate": 7.964387541018883e-05,
      "loss": 2.4306,
      "step": 19020
    },
    {
      "epoch": 3.0678703852974367,
      "grad_norm": 28.75,
      "learning_rate": 7.963311635913713e-05,
      "loss": 2.0707,
      "step": 19030
    },
    {
      "epoch": 3.0694825084636466,
      "grad_norm": 38.0,
      "learning_rate": 7.962235730808543e-05,
      "loss": 2.2372,
      "step": 19040
    },
    {
      "epoch": 3.0710946316298564,
      "grad_norm": 111.5,
      "learning_rate": 7.961159825703374e-05,
      "loss": 2.9043,
      "step": 19050
    },
    {
      "epoch": 3.0727067547960663,
      "grad_norm": 35.75,
      "learning_rate": 7.960083920598203e-05,
      "loss": 1.5051,
      "step": 19060
    },
    {
      "epoch": 3.074318877962276,
      "grad_norm": 37.5,
      "learning_rate": 7.959008015493033e-05,
      "loss": 2.6572,
      "step": 19070
    },
    {
      "epoch": 3.0759310011284864,
      "grad_norm": 90.5,
      "learning_rate": 7.957932110387864e-05,
      "loss": 1.608,
      "step": 19080
    },
    {
      "epoch": 3.0775431242946962,
      "grad_norm": 111.0,
      "learning_rate": 7.956856205282695e-05,
      "loss": 3.7478,
      "step": 19090
    },
    {
      "epoch": 3.079155247460906,
      "grad_norm": 33.0,
      "learning_rate": 7.955780300177525e-05,
      "loss": 1.692,
      "step": 19100
    },
    {
      "epoch": 3.080767370627116,
      "grad_norm": 38.0,
      "learning_rate": 7.954704395072356e-05,
      "loss": 2.671,
      "step": 19110
    },
    {
      "epoch": 3.0823794937933258,
      "grad_norm": 32.75,
      "learning_rate": 7.953628489967185e-05,
      "loss": 3.8474,
      "step": 19120
    },
    {
      "epoch": 3.0839916169595356,
      "grad_norm": 63.75,
      "learning_rate": 7.952552584862015e-05,
      "loss": 2.5493,
      "step": 19130
    },
    {
      "epoch": 3.0856037401257455,
      "grad_norm": 40.75,
      "learning_rate": 7.951476679756846e-05,
      "loss": 3.9033,
      "step": 19140
    },
    {
      "epoch": 3.0872158632919553,
      "grad_norm": 2.453125,
      "learning_rate": 7.950400774651676e-05,
      "loss": 1.9272,
      "step": 19150
    },
    {
      "epoch": 3.0888279864581656,
      "grad_norm": 4.125,
      "learning_rate": 7.949324869546507e-05,
      "loss": 2.1779,
      "step": 19160
    },
    {
      "epoch": 3.0904401096243754,
      "grad_norm": 76.0,
      "learning_rate": 7.948248964441338e-05,
      "loss": 2.1917,
      "step": 19170
    },
    {
      "epoch": 3.0920522327905853,
      "grad_norm": 17.875,
      "learning_rate": 7.947173059336167e-05,
      "loss": 2.0355,
      "step": 19180
    },
    {
      "epoch": 3.093664355956795,
      "grad_norm": 0.10791015625,
      "learning_rate": 7.946097154230997e-05,
      "loss": 0.7224,
      "step": 19190
    },
    {
      "epoch": 3.095276479123005,
      "grad_norm": 64.0,
      "learning_rate": 7.945021249125828e-05,
      "loss": 1.51,
      "step": 19200
    },
    {
      "epoch": 3.096888602289215,
      "grad_norm": 222.0,
      "learning_rate": 7.943945344020658e-05,
      "loss": 2.5321,
      "step": 19210
    },
    {
      "epoch": 3.0985007254554247,
      "grad_norm": 40.5,
      "learning_rate": 7.942869438915487e-05,
      "loss": 2.7807,
      "step": 19220
    },
    {
      "epoch": 3.1001128486216345,
      "grad_norm": 8.6875,
      "learning_rate": 7.941793533810318e-05,
      "loss": 1.326,
      "step": 19230
    },
    {
      "epoch": 3.101724971787845,
      "grad_norm": 17.625,
      "learning_rate": 7.940717628705149e-05,
      "loss": 2.4722,
      "step": 19240
    },
    {
      "epoch": 3.1033370949540546,
      "grad_norm": 91.5,
      "learning_rate": 7.939641723599979e-05,
      "loss": 1.4676,
      "step": 19250
    },
    {
      "epoch": 3.1049492181202645,
      "grad_norm": 22.5,
      "learning_rate": 7.938565818494809e-05,
      "loss": 1.7207,
      "step": 19260
    },
    {
      "epoch": 3.1065613412864743,
      "grad_norm": 30.0,
      "learning_rate": 7.93748991338964e-05,
      "loss": 1.0241,
      "step": 19270
    },
    {
      "epoch": 3.108173464452684,
      "grad_norm": 34.5,
      "learning_rate": 7.936414008284469e-05,
      "loss": 1.9518,
      "step": 19280
    },
    {
      "epoch": 3.109785587618894,
      "grad_norm": 79.5,
      "learning_rate": 7.935338103179299e-05,
      "loss": 2.0453,
      "step": 19290
    },
    {
      "epoch": 3.111397710785104,
      "grad_norm": 350.0,
      "learning_rate": 7.934262198074131e-05,
      "loss": 2.5525,
      "step": 19300
    },
    {
      "epoch": 3.1130098339513137,
      "grad_norm": 9.9375,
      "learning_rate": 7.933186292968961e-05,
      "loss": 1.7065,
      "step": 19310
    },
    {
      "epoch": 3.114621957117524,
      "grad_norm": 66.0,
      "learning_rate": 7.93211038786379e-05,
      "loss": 2.3304,
      "step": 19320
    },
    {
      "epoch": 3.116234080283734,
      "grad_norm": 34.25,
      "learning_rate": 7.931034482758621e-05,
      "loss": 2.2692,
      "step": 19330
    },
    {
      "epoch": 3.1178462034499437,
      "grad_norm": 46.0,
      "learning_rate": 7.929958577653451e-05,
      "loss": 2.4033,
      "step": 19340
    },
    {
      "epoch": 3.1194583266161535,
      "grad_norm": 33.0,
      "learning_rate": 7.928882672548281e-05,
      "loss": 1.1085,
      "step": 19350
    },
    {
      "epoch": 3.1210704497823634,
      "grad_norm": 38.25,
      "learning_rate": 7.927806767443112e-05,
      "loss": 2.9305,
      "step": 19360
    },
    {
      "epoch": 3.122682572948573,
      "grad_norm": 36.25,
      "learning_rate": 7.926730862337943e-05,
      "loss": 1.5568,
      "step": 19370
    },
    {
      "epoch": 3.124294696114783,
      "grad_norm": 22.0,
      "learning_rate": 7.925654957232772e-05,
      "loss": 2.4001,
      "step": 19380
    },
    {
      "epoch": 3.125906819280993,
      "grad_norm": 37.25,
      "learning_rate": 7.924579052127603e-05,
      "loss": 0.8876,
      "step": 19390
    },
    {
      "epoch": 3.127518942447203,
      "grad_norm": 30.125,
      "learning_rate": 7.923503147022433e-05,
      "loss": 0.786,
      "step": 19400
    },
    {
      "epoch": 3.129131065613413,
      "grad_norm": 29.25,
      "learning_rate": 7.922427241917263e-05,
      "loss": 2.8795,
      "step": 19410
    },
    {
      "epoch": 3.130743188779623,
      "grad_norm": 37.75,
      "learning_rate": 7.921351336812094e-05,
      "loss": 1.5228,
      "step": 19420
    },
    {
      "epoch": 3.1323553119458327,
      "grad_norm": 11.125,
      "learning_rate": 7.920275431706923e-05,
      "loss": 1.2489,
      "step": 19430
    },
    {
      "epoch": 3.1339674351120426,
      "grad_norm": 21.125,
      "learning_rate": 7.919199526601754e-05,
      "loss": 2.0167,
      "step": 19440
    },
    {
      "epoch": 3.1355795582782524,
      "grad_norm": 34.25,
      "learning_rate": 7.918123621496585e-05,
      "loss": 3.3703,
      "step": 19450
    },
    {
      "epoch": 3.1371916814444623,
      "grad_norm": 39.75,
      "learning_rate": 7.917047716391415e-05,
      "loss": 2.3774,
      "step": 19460
    },
    {
      "epoch": 3.138803804610672,
      "grad_norm": 15.125,
      "learning_rate": 7.915971811286245e-05,
      "loss": 0.8975,
      "step": 19470
    },
    {
      "epoch": 3.140415927776882,
      "grad_norm": 4.21875,
      "learning_rate": 7.914895906181076e-05,
      "loss": 1.2696,
      "step": 19480
    },
    {
      "epoch": 3.1420280509430922,
      "grad_norm": 14.875,
      "learning_rate": 7.913820001075905e-05,
      "loss": 1.3484,
      "step": 19490
    },
    {
      "epoch": 3.143640174109302,
      "grad_norm": 41.5,
      "learning_rate": 7.912744095970735e-05,
      "loss": 3.1153,
      "step": 19500
    },
    {
      "epoch": 3.145252297275512,
      "grad_norm": 16.875,
      "learning_rate": 7.911668190865566e-05,
      "loss": 1.654,
      "step": 19510
    },
    {
      "epoch": 3.1468644204417218,
      "grad_norm": 18.0,
      "learning_rate": 7.910592285760397e-05,
      "loss": 2.4441,
      "step": 19520
    },
    {
      "epoch": 3.1484765436079316,
      "grad_norm": 46.25,
      "learning_rate": 7.909516380655227e-05,
      "loss": 1.2798,
      "step": 19530
    },
    {
      "epoch": 3.1500886667741415,
      "grad_norm": 42.5,
      "learning_rate": 7.908440475550058e-05,
      "loss": 2.3402,
      "step": 19540
    },
    {
      "epoch": 3.1517007899403513,
      "grad_norm": 4.40625,
      "learning_rate": 7.907364570444887e-05,
      "loss": 2.0158,
      "step": 19550
    },
    {
      "epoch": 3.153312913106561,
      "grad_norm": 43.0,
      "learning_rate": 7.906288665339717e-05,
      "loss": 2.3238,
      "step": 19560
    },
    {
      "epoch": 3.1549250362727714,
      "grad_norm": 3.046875,
      "learning_rate": 7.905212760234547e-05,
      "loss": 2.1194,
      "step": 19570
    },
    {
      "epoch": 3.1565371594389813,
      "grad_norm": 33.25,
      "learning_rate": 7.904136855129378e-05,
      "loss": 2.5629,
      "step": 19580
    },
    {
      "epoch": 3.158149282605191,
      "grad_norm": 0.0,
      "learning_rate": 7.903060950024209e-05,
      "loss": 3.1262,
      "step": 19590
    },
    {
      "epoch": 3.159761405771401,
      "grad_norm": 29.75,
      "learning_rate": 7.901985044919038e-05,
      "loss": 1.2284,
      "step": 19600
    },
    {
      "epoch": 3.161373528937611,
      "grad_norm": 42.25,
      "learning_rate": 7.900909139813869e-05,
      "loss": 2.1358,
      "step": 19610
    },
    {
      "epoch": 3.1629856521038207,
      "grad_norm": 16.875,
      "learning_rate": 7.899833234708699e-05,
      "loss": 2.1502,
      "step": 19620
    },
    {
      "epoch": 3.1645977752700305,
      "grad_norm": 61.75,
      "learning_rate": 7.898757329603529e-05,
      "loss": 1.8474,
      "step": 19630
    },
    {
      "epoch": 3.1662098984362403,
      "grad_norm": 13.0625,
      "learning_rate": 7.89768142449836e-05,
      "loss": 1.1871,
      "step": 19640
    },
    {
      "epoch": 3.1678220216024506,
      "grad_norm": 30.875,
      "learning_rate": 7.896605519393189e-05,
      "loss": 2.6756,
      "step": 19650
    },
    {
      "epoch": 3.1694341447686605,
      "grad_norm": 23.125,
      "learning_rate": 7.89552961428802e-05,
      "loss": 2.7989,
      "step": 19660
    },
    {
      "epoch": 3.1710462679348703,
      "grad_norm": 36.5,
      "learning_rate": 7.894453709182851e-05,
      "loss": 1.1145,
      "step": 19670
    },
    {
      "epoch": 3.17265839110108,
      "grad_norm": 11.5,
      "learning_rate": 7.893377804077681e-05,
      "loss": 2.8717,
      "step": 19680
    },
    {
      "epoch": 3.17427051426729,
      "grad_norm": 18.875,
      "learning_rate": 7.89230189897251e-05,
      "loss": 1.7221,
      "step": 19690
    },
    {
      "epoch": 3.1758826374335,
      "grad_norm": 0.056640625,
      "learning_rate": 7.891225993867342e-05,
      "loss": 2.6814,
      "step": 19700
    },
    {
      "epoch": 3.1774947605997097,
      "grad_norm": 84.0,
      "learning_rate": 7.890150088762171e-05,
      "loss": 1.6555,
      "step": 19710
    },
    {
      "epoch": 3.1791068837659195,
      "grad_norm": 76.0,
      "learning_rate": 7.889074183657001e-05,
      "loss": 1.7204,
      "step": 19720
    },
    {
      "epoch": 3.1807190069321294,
      "grad_norm": 42.25,
      "learning_rate": 7.887998278551832e-05,
      "loss": 2.1805,
      "step": 19730
    },
    {
      "epoch": 3.1823311300983397,
      "grad_norm": 52.75,
      "learning_rate": 7.886922373446663e-05,
      "loss": 3.6684,
      "step": 19740
    },
    {
      "epoch": 3.1839432532645495,
      "grad_norm": 30.0,
      "learning_rate": 7.885846468341493e-05,
      "loss": 1.1507,
      "step": 19750
    },
    {
      "epoch": 3.1855553764307594,
      "grad_norm": 42.5,
      "learning_rate": 7.884770563236324e-05,
      "loss": 2.2056,
      "step": 19760
    },
    {
      "epoch": 3.187167499596969,
      "grad_norm": 45.0,
      "learning_rate": 7.883694658131153e-05,
      "loss": 2.7445,
      "step": 19770
    },
    {
      "epoch": 3.188779622763179,
      "grad_norm": 42.0,
      "learning_rate": 7.882618753025983e-05,
      "loss": 1.0621,
      "step": 19780
    },
    {
      "epoch": 3.190391745929389,
      "grad_norm": 26.625,
      "learning_rate": 7.881542847920814e-05,
      "loss": 1.3967,
      "step": 19790
    },
    {
      "epoch": 3.1920038690955987,
      "grad_norm": 0.84765625,
      "learning_rate": 7.880466942815644e-05,
      "loss": 0.6775,
      "step": 19800
    },
    {
      "epoch": 3.193615992261809,
      "grad_norm": 77.5,
      "learning_rate": 7.879391037710475e-05,
      "loss": 2.5658,
      "step": 19810
    },
    {
      "epoch": 3.195228115428019,
      "grad_norm": 27.125,
      "learning_rate": 7.878315132605306e-05,
      "loss": 0.786,
      "step": 19820
    },
    {
      "epoch": 3.1968402385942287,
      "grad_norm": 24.125,
      "learning_rate": 7.877239227500135e-05,
      "loss": 2.0149,
      "step": 19830
    },
    {
      "epoch": 3.1984523617604386,
      "grad_norm": 3.109375,
      "learning_rate": 7.876163322394965e-05,
      "loss": 1.2481,
      "step": 19840
    },
    {
      "epoch": 3.2000644849266484,
      "grad_norm": 18.375,
      "learning_rate": 7.875087417289796e-05,
      "loss": 2.6936,
      "step": 19850
    },
    {
      "epoch": 3.2016766080928583,
      "grad_norm": 26.625,
      "learning_rate": 7.874011512184625e-05,
      "loss": 2.1449,
      "step": 19860
    },
    {
      "epoch": 3.203288731259068,
      "grad_norm": 19.875,
      "learning_rate": 7.872935607079455e-05,
      "loss": 2.4666,
      "step": 19870
    },
    {
      "epoch": 3.204900854425278,
      "grad_norm": 0.095703125,
      "learning_rate": 7.871859701974286e-05,
      "loss": 1.3128,
      "step": 19880
    },
    {
      "epoch": 3.206512977591488,
      "grad_norm": 26.125,
      "learning_rate": 7.870783796869117e-05,
      "loss": 1.38,
      "step": 19890
    },
    {
      "epoch": 3.208125100757698,
      "grad_norm": 92.5,
      "learning_rate": 7.869707891763947e-05,
      "loss": 3.0556,
      "step": 19900
    },
    {
      "epoch": 3.209737223923908,
      "grad_norm": 23.75,
      "learning_rate": 7.868631986658776e-05,
      "loss": 2.512,
      "step": 19910
    },
    {
      "epoch": 3.2113493470901178,
      "grad_norm": 80.5,
      "learning_rate": 7.867556081553607e-05,
      "loss": 2.7325,
      "step": 19920
    },
    {
      "epoch": 3.2129614702563276,
      "grad_norm": 5.28125,
      "learning_rate": 7.866480176448437e-05,
      "loss": 2.9916,
      "step": 19930
    },
    {
      "epoch": 3.2145735934225375,
      "grad_norm": 20.75,
      "learning_rate": 7.865404271343267e-05,
      "loss": 1.0038,
      "step": 19940
    },
    {
      "epoch": 3.2161857165887473,
      "grad_norm": 40.25,
      "learning_rate": 7.864328366238098e-05,
      "loss": 1.8329,
      "step": 19950
    },
    {
      "epoch": 3.217797839754957,
      "grad_norm": 62.0,
      "learning_rate": 7.863252461132929e-05,
      "loss": 1.0637,
      "step": 19960
    },
    {
      "epoch": 3.219409962921167,
      "grad_norm": 53.25,
      "learning_rate": 7.862176556027758e-05,
      "loss": 2.8001,
      "step": 19970
    },
    {
      "epoch": 3.2210220860873773,
      "grad_norm": 7.15625,
      "learning_rate": 7.86110065092259e-05,
      "loss": 1.7975,
      "step": 19980
    },
    {
      "epoch": 3.222634209253587,
      "grad_norm": 225.0,
      "learning_rate": 7.860024745817419e-05,
      "loss": 2.9764,
      "step": 19990
    },
    {
      "epoch": 3.224246332419797,
      "grad_norm": 28.125,
      "learning_rate": 7.858948840712249e-05,
      "loss": 2.6535,
      "step": 20000
    },
    {
      "epoch": 3.225858455586007,
      "grad_norm": 10.625,
      "learning_rate": 7.85787293560708e-05,
      "loss": 2.7832,
      "step": 20010
    },
    {
      "epoch": 3.2274705787522167,
      "grad_norm": 0.1669921875,
      "learning_rate": 7.856797030501911e-05,
      "loss": 2.8778,
      "step": 20020
    },
    {
      "epoch": 3.2290827019184265,
      "grad_norm": 37.0,
      "learning_rate": 7.85572112539674e-05,
      "loss": 2.0644,
      "step": 20030
    },
    {
      "epoch": 3.2306948250846363,
      "grad_norm": 66.5,
      "learning_rate": 7.854645220291571e-05,
      "loss": 2.3668,
      "step": 20040
    },
    {
      "epoch": 3.232306948250846,
      "grad_norm": 53.0,
      "learning_rate": 7.853569315186401e-05,
      "loss": 2.623,
      "step": 20050
    },
    {
      "epoch": 3.2339190714170565,
      "grad_norm": 140.0,
      "learning_rate": 7.852493410081231e-05,
      "loss": 1.7703,
      "step": 20060
    },
    {
      "epoch": 3.2355311945832663,
      "grad_norm": 19.0,
      "learning_rate": 7.851417504976062e-05,
      "loss": 2.3213,
      "step": 20070
    },
    {
      "epoch": 3.237143317749476,
      "grad_norm": 141.0,
      "learning_rate": 7.850341599870891e-05,
      "loss": 2.3962,
      "step": 20080
    },
    {
      "epoch": 3.238755440915686,
      "grad_norm": 45.5,
      "learning_rate": 7.849265694765722e-05,
      "loss": 3.3603,
      "step": 20090
    },
    {
      "epoch": 3.240367564081896,
      "grad_norm": 580.0,
      "learning_rate": 7.848189789660553e-05,
      "loss": 3.1565,
      "step": 20100
    },
    {
      "epoch": 3.2419796872481057,
      "grad_norm": 12.3125,
      "learning_rate": 7.847113884555383e-05,
      "loss": 1.5262,
      "step": 20110
    },
    {
      "epoch": 3.2435918104143155,
      "grad_norm": 24.5,
      "learning_rate": 7.846037979450213e-05,
      "loss": 3.1712,
      "step": 20120
    },
    {
      "epoch": 3.2452039335805254,
      "grad_norm": 26.875,
      "learning_rate": 7.844962074345044e-05,
      "loss": 2.672,
      "step": 20130
    },
    {
      "epoch": 3.2468160567467352,
      "grad_norm": 14.75,
      "learning_rate": 7.843886169239873e-05,
      "loss": 1.7064,
      "step": 20140
    },
    {
      "epoch": 3.2484281799129455,
      "grad_norm": 29.875,
      "learning_rate": 7.842810264134703e-05,
      "loss": 2.6324,
      "step": 20150
    },
    {
      "epoch": 3.2500403030791554,
      "grad_norm": 9.25,
      "learning_rate": 7.841734359029534e-05,
      "loss": 1.5613,
      "step": 20160
    },
    {
      "epoch": 3.251652426245365,
      "grad_norm": 11.5,
      "learning_rate": 7.840658453924365e-05,
      "loss": 3.0282,
      "step": 20170
    },
    {
      "epoch": 3.253264549411575,
      "grad_norm": 33.0,
      "learning_rate": 7.839582548819195e-05,
      "loss": 1.5497,
      "step": 20180
    },
    {
      "epoch": 3.254876672577785,
      "grad_norm": 61.25,
      "learning_rate": 7.838506643714026e-05,
      "loss": 2.3494,
      "step": 20190
    },
    {
      "epoch": 3.2564887957439947,
      "grad_norm": 32.5,
      "learning_rate": 7.837430738608855e-05,
      "loss": 1.7688,
      "step": 20200
    },
    {
      "epoch": 3.2581009189102046,
      "grad_norm": 23.625,
      "learning_rate": 7.836354833503685e-05,
      "loss": 2.1232,
      "step": 20210
    },
    {
      "epoch": 3.259713042076415,
      "grad_norm": 34.75,
      "learning_rate": 7.835278928398515e-05,
      "loss": 1.3265,
      "step": 20220
    },
    {
      "epoch": 3.2613251652426247,
      "grad_norm": 4.09375,
      "learning_rate": 7.834203023293346e-05,
      "loss": 1.4829,
      "step": 20230
    },
    {
      "epoch": 3.2629372884088346,
      "grad_norm": 3.0,
      "learning_rate": 7.833127118188177e-05,
      "loss": 2.7038,
      "step": 20240
    },
    {
      "epoch": 3.2645494115750444,
      "grad_norm": 76.5,
      "learning_rate": 7.832051213083006e-05,
      "loss": 1.6524,
      "step": 20250
    },
    {
      "epoch": 3.2661615347412543,
      "grad_norm": 27.625,
      "learning_rate": 7.830975307977837e-05,
      "loss": 0.9805,
      "step": 20260
    },
    {
      "epoch": 3.267773657907464,
      "grad_norm": 30.625,
      "learning_rate": 7.829899402872667e-05,
      "loss": 1.999,
      "step": 20270
    },
    {
      "epoch": 3.269385781073674,
      "grad_norm": 58.25,
      "learning_rate": 7.828823497767497e-05,
      "loss": 1.4936,
      "step": 20280
    },
    {
      "epoch": 3.270997904239884,
      "grad_norm": 33.25,
      "learning_rate": 7.827747592662328e-05,
      "loss": 2.5576,
      "step": 20290
    },
    {
      "epoch": 3.2726100274060936,
      "grad_norm": 13.25,
      "learning_rate": 7.826671687557157e-05,
      "loss": 2.0512,
      "step": 20300
    },
    {
      "epoch": 3.274222150572304,
      "grad_norm": 17.0,
      "learning_rate": 7.825595782451988e-05,
      "loss": 1.4867,
      "step": 20310
    },
    {
      "epoch": 3.2758342737385138,
      "grad_norm": 26.25,
      "learning_rate": 7.824519877346819e-05,
      "loss": 3.5326,
      "step": 20320
    },
    {
      "epoch": 3.2774463969047236,
      "grad_norm": 37.75,
      "learning_rate": 7.823443972241649e-05,
      "loss": 1.8548,
      "step": 20330
    },
    {
      "epoch": 3.2790585200709335,
      "grad_norm": 32.25,
      "learning_rate": 7.822368067136479e-05,
      "loss": 1.9154,
      "step": 20340
    },
    {
      "epoch": 3.2806706432371433,
      "grad_norm": 24.625,
      "learning_rate": 7.82129216203131e-05,
      "loss": 2.141,
      "step": 20350
    },
    {
      "epoch": 3.282282766403353,
      "grad_norm": 0.25390625,
      "learning_rate": 7.820216256926139e-05,
      "loss": 1.2286,
      "step": 20360
    },
    {
      "epoch": 3.283894889569563,
      "grad_norm": 17.0,
      "learning_rate": 7.819140351820969e-05,
      "loss": 2.3968,
      "step": 20370
    },
    {
      "epoch": 3.285507012735773,
      "grad_norm": 19.25,
      "learning_rate": 7.8180644467158e-05,
      "loss": 1.7956,
      "step": 20380
    },
    {
      "epoch": 3.2871191359019827,
      "grad_norm": 54.0,
      "learning_rate": 7.816988541610631e-05,
      "loss": 2.5217,
      "step": 20390
    },
    {
      "epoch": 3.288731259068193,
      "grad_norm": 48.75,
      "learning_rate": 7.81591263650546e-05,
      "loss": 1.6309,
      "step": 20400
    },
    {
      "epoch": 3.290343382234403,
      "grad_norm": 194.0,
      "learning_rate": 7.814836731400291e-05,
      "loss": 3.3612,
      "step": 20410
    },
    {
      "epoch": 3.2919555054006127,
      "grad_norm": 19.875,
      "learning_rate": 7.813760826295121e-05,
      "loss": 1.6502,
      "step": 20420
    },
    {
      "epoch": 3.2935676285668225,
      "grad_norm": 28.625,
      "learning_rate": 7.812684921189951e-05,
      "loss": 2.7015,
      "step": 20430
    },
    {
      "epoch": 3.2951797517330323,
      "grad_norm": 1.8828125,
      "learning_rate": 7.811609016084782e-05,
      "loss": 1.6321,
      "step": 20440
    },
    {
      "epoch": 3.296791874899242,
      "grad_norm": 20.75,
      "learning_rate": 7.810533110979611e-05,
      "loss": 1.6632,
      "step": 20450
    },
    {
      "epoch": 3.298403998065452,
      "grad_norm": 26.25,
      "learning_rate": 7.809457205874442e-05,
      "loss": 1.9846,
      "step": 20460
    },
    {
      "epoch": 3.3000161212316623,
      "grad_norm": 94.5,
      "learning_rate": 7.808381300769273e-05,
      "loss": 2.0844,
      "step": 20470
    },
    {
      "epoch": 3.301628244397872,
      "grad_norm": 50.5,
      "learning_rate": 7.807305395664103e-05,
      "loss": 1.9213,
      "step": 20480
    },
    {
      "epoch": 3.303240367564082,
      "grad_norm": 0.0,
      "learning_rate": 7.806229490558933e-05,
      "loss": 1.7123,
      "step": 20490
    },
    {
      "epoch": 3.304852490730292,
      "grad_norm": 80.5,
      "learning_rate": 7.805153585453764e-05,
      "loss": 1.6518,
      "step": 20500
    },
    {
      "epoch": 3.3064646138965017,
      "grad_norm": 79.0,
      "learning_rate": 7.804077680348593e-05,
      "loss": 2.2052,
      "step": 20510
    },
    {
      "epoch": 3.3080767370627115,
      "grad_norm": 59.25,
      "learning_rate": 7.803001775243423e-05,
      "loss": 2.4141,
      "step": 20520
    },
    {
      "epoch": 3.3096888602289214,
      "grad_norm": 10.625,
      "learning_rate": 7.801925870138254e-05,
      "loss": 2.3344,
      "step": 20530
    },
    {
      "epoch": 3.3113009833951312,
      "grad_norm": 20.625,
      "learning_rate": 7.800849965033085e-05,
      "loss": 1.3998,
      "step": 20540
    },
    {
      "epoch": 3.312913106561341,
      "grad_norm": 227.0,
      "learning_rate": 7.799774059927915e-05,
      "loss": 3.9333,
      "step": 20550
    },
    {
      "epoch": 3.3145252297275514,
      "grad_norm": 62.5,
      "learning_rate": 7.798698154822744e-05,
      "loss": 1.3226,
      "step": 20560
    },
    {
      "epoch": 3.316137352893761,
      "grad_norm": 0.228515625,
      "learning_rate": 7.797622249717575e-05,
      "loss": 2.3477,
      "step": 20570
    },
    {
      "epoch": 3.317749476059971,
      "grad_norm": 0.0,
      "learning_rate": 7.796546344612405e-05,
      "loss": 2.7387,
      "step": 20580
    },
    {
      "epoch": 3.319361599226181,
      "grad_norm": 21.0,
      "learning_rate": 7.795470439507235e-05,
      "loss": 1.9121,
      "step": 20590
    },
    {
      "epoch": 3.3209737223923907,
      "grad_norm": 35.5,
      "learning_rate": 7.794394534402066e-05,
      "loss": 3.0893,
      "step": 20600
    },
    {
      "epoch": 3.3225858455586006,
      "grad_norm": 1.109375,
      "learning_rate": 7.793318629296897e-05,
      "loss": 0.4956,
      "step": 20610
    },
    {
      "epoch": 3.3241979687248104,
      "grad_norm": 5.5,
      "learning_rate": 7.792242724191726e-05,
      "loss": 1.5757,
      "step": 20620
    },
    {
      "epoch": 3.3258100918910203,
      "grad_norm": 38.5,
      "learning_rate": 7.791166819086557e-05,
      "loss": 2.8208,
      "step": 20630
    },
    {
      "epoch": 3.32742221505723,
      "grad_norm": 76.5,
      "learning_rate": 7.790090913981387e-05,
      "loss": 2.4752,
      "step": 20640
    },
    {
      "epoch": 3.3290343382234404,
      "grad_norm": 39.5,
      "learning_rate": 7.789015008876217e-05,
      "loss": 1.3827,
      "step": 20650
    },
    {
      "epoch": 3.3306464613896503,
      "grad_norm": 0.27734375,
      "learning_rate": 7.787939103771048e-05,
      "loss": 3.0358,
      "step": 20660
    },
    {
      "epoch": 3.33225858455586,
      "grad_norm": 0.0216064453125,
      "learning_rate": 7.786863198665879e-05,
      "loss": 1.6707,
      "step": 20670
    },
    {
      "epoch": 3.33387070772207,
      "grad_norm": 25.875,
      "learning_rate": 7.785787293560708e-05,
      "loss": 3.1695,
      "step": 20680
    },
    {
      "epoch": 3.33548283088828,
      "grad_norm": 16.75,
      "learning_rate": 7.784711388455539e-05,
      "loss": 1.9084,
      "step": 20690
    },
    {
      "epoch": 3.3370949540544896,
      "grad_norm": 57.0,
      "learning_rate": 7.783635483350369e-05,
      "loss": 3.029,
      "step": 20700
    },
    {
      "epoch": 3.3387070772206995,
      "grad_norm": 56.5,
      "learning_rate": 7.782559578245199e-05,
      "loss": 2.4953,
      "step": 20710
    },
    {
      "epoch": 3.3403192003869098,
      "grad_norm": 34.75,
      "learning_rate": 7.78148367314003e-05,
      "loss": 0.7288,
      "step": 20720
    },
    {
      "epoch": 3.3419313235531196,
      "grad_norm": 30.875,
      "learning_rate": 7.780407768034859e-05,
      "loss": 2.4572,
      "step": 20730
    },
    {
      "epoch": 3.3435434467193295,
      "grad_norm": 33.25,
      "learning_rate": 7.77933186292969e-05,
      "loss": 0.792,
      "step": 20740
    },
    {
      "epoch": 3.3451555698855393,
      "grad_norm": 48.25,
      "learning_rate": 7.778255957824521e-05,
      "loss": 1.3197,
      "step": 20750
    },
    {
      "epoch": 3.346767693051749,
      "grad_norm": 101.5,
      "learning_rate": 7.777180052719351e-05,
      "loss": 1.2324,
      "step": 20760
    },
    {
      "epoch": 3.348379816217959,
      "grad_norm": 27.125,
      "learning_rate": 7.77610414761418e-05,
      "loss": 3.8419,
      "step": 20770
    },
    {
      "epoch": 3.349991939384169,
      "grad_norm": 35.75,
      "learning_rate": 7.775028242509012e-05,
      "loss": 1.2317,
      "step": 20780
    },
    {
      "epoch": 3.3516040625503787,
      "grad_norm": 43.25,
      "learning_rate": 7.773952337403841e-05,
      "loss": 2.712,
      "step": 20790
    },
    {
      "epoch": 3.3532161857165885,
      "grad_norm": 16.625,
      "learning_rate": 7.772876432298671e-05,
      "loss": 0.9118,
      "step": 20800
    },
    {
      "epoch": 3.354828308882799,
      "grad_norm": 0.177734375,
      "learning_rate": 7.771800527193502e-05,
      "loss": 1.5099,
      "step": 20810
    },
    {
      "epoch": 3.3564404320490087,
      "grad_norm": 0.9140625,
      "learning_rate": 7.770724622088333e-05,
      "loss": 3.1353,
      "step": 20820
    },
    {
      "epoch": 3.3580525552152185,
      "grad_norm": 17.5,
      "learning_rate": 7.769648716983163e-05,
      "loss": 2.7532,
      "step": 20830
    },
    {
      "epoch": 3.3596646783814283,
      "grad_norm": 43.5,
      "learning_rate": 7.768572811877994e-05,
      "loss": 3.1632,
      "step": 20840
    },
    {
      "epoch": 3.361276801547638,
      "grad_norm": 44.5,
      "learning_rate": 7.767496906772823e-05,
      "loss": 1.7535,
      "step": 20850
    },
    {
      "epoch": 3.362888924713848,
      "grad_norm": 40.0,
      "learning_rate": 7.766421001667653e-05,
      "loss": 1.6029,
      "step": 20860
    },
    {
      "epoch": 3.364501047880058,
      "grad_norm": 122.5,
      "learning_rate": 7.765345096562484e-05,
      "loss": 2.2982,
      "step": 20870
    },
    {
      "epoch": 3.366113171046268,
      "grad_norm": 37.0,
      "learning_rate": 7.764269191457314e-05,
      "loss": 1.1458,
      "step": 20880
    },
    {
      "epoch": 3.367725294212478,
      "grad_norm": 27.125,
      "learning_rate": 7.763193286352145e-05,
      "loss": 1.8293,
      "step": 20890
    },
    {
      "epoch": 3.369337417378688,
      "grad_norm": 2.6875,
      "learning_rate": 7.762117381246974e-05,
      "loss": 2.8808,
      "step": 20900
    },
    {
      "epoch": 3.3709495405448977,
      "grad_norm": 59.75,
      "learning_rate": 7.761041476141805e-05,
      "loss": 4.577,
      "step": 20910
    },
    {
      "epoch": 3.3725616637111075,
      "grad_norm": 87.5,
      "learning_rate": 7.759965571036635e-05,
      "loss": 3.7129,
      "step": 20920
    },
    {
      "epoch": 3.3741737868773174,
      "grad_norm": 27.875,
      "learning_rate": 7.758889665931464e-05,
      "loss": 2.5719,
      "step": 20930
    },
    {
      "epoch": 3.3757859100435272,
      "grad_norm": 0.0,
      "learning_rate": 7.757813760826295e-05,
      "loss": 1.5114,
      "step": 20940
    },
    {
      "epoch": 3.377398033209737,
      "grad_norm": 17.25,
      "learning_rate": 7.756737855721125e-05,
      "loss": 2.9723,
      "step": 20950
    },
    {
      "epoch": 3.379010156375947,
      "grad_norm": 18.75,
      "learning_rate": 7.755661950615956e-05,
      "loss": 1.0217,
      "step": 20960
    },
    {
      "epoch": 3.380622279542157,
      "grad_norm": 15.125,
      "learning_rate": 7.754586045510787e-05,
      "loss": 2.2646,
      "step": 20970
    },
    {
      "epoch": 3.382234402708367,
      "grad_norm": 58.5,
      "learning_rate": 7.753510140405617e-05,
      "loss": 2.6278,
      "step": 20980
    },
    {
      "epoch": 3.383846525874577,
      "grad_norm": 53.0,
      "learning_rate": 7.752434235300446e-05,
      "loss": 3.0676,
      "step": 20990
    },
    {
      "epoch": 3.3854586490407867,
      "grad_norm": 16.375,
      "learning_rate": 7.751358330195277e-05,
      "loss": 2.6615,
      "step": 21000
    },
    {
      "epoch": 3.3870707722069966,
      "grad_norm": 74.0,
      "learning_rate": 7.750282425090107e-05,
      "loss": 2.2032,
      "step": 21010
    },
    {
      "epoch": 3.3886828953732064,
      "grad_norm": 260.0,
      "learning_rate": 7.749206519984937e-05,
      "loss": 1.975,
      "step": 21020
    },
    {
      "epoch": 3.3902950185394163,
      "grad_norm": 74.5,
      "learning_rate": 7.748130614879768e-05,
      "loss": 2.7338,
      "step": 21030
    },
    {
      "epoch": 3.391907141705626,
      "grad_norm": 22.625,
      "learning_rate": 7.747054709774599e-05,
      "loss": 2.7658,
      "step": 21040
    },
    {
      "epoch": 3.393519264871836,
      "grad_norm": 43.0,
      "learning_rate": 7.745978804669428e-05,
      "loss": 2.1736,
      "step": 21050
    },
    {
      "epoch": 3.3951313880380463,
      "grad_norm": 45.5,
      "learning_rate": 7.74490289956426e-05,
      "loss": 1.7153,
      "step": 21060
    },
    {
      "epoch": 3.396743511204256,
      "grad_norm": 18.625,
      "learning_rate": 7.743826994459089e-05,
      "loss": 3.2836,
      "step": 21070
    },
    {
      "epoch": 3.398355634370466,
      "grad_norm": 58.75,
      "learning_rate": 7.742751089353919e-05,
      "loss": 2.1405,
      "step": 21080
    },
    {
      "epoch": 3.399967757536676,
      "grad_norm": 13.375,
      "learning_rate": 7.74167518424875e-05,
      "loss": 2.8831,
      "step": 21090
    },
    {
      "epoch": 3.4015798807028856,
      "grad_norm": 84.5,
      "learning_rate": 7.74059927914358e-05,
      "loss": 2.7893,
      "step": 21100
    },
    {
      "epoch": 3.4031920038690955,
      "grad_norm": 41.25,
      "learning_rate": 7.73952337403841e-05,
      "loss": 1.8538,
      "step": 21110
    },
    {
      "epoch": 3.4048041270353053,
      "grad_norm": 60.0,
      "learning_rate": 7.738447468933241e-05,
      "loss": 2.6675,
      "step": 21120
    },
    {
      "epoch": 3.4064162502015156,
      "grad_norm": 0.98046875,
      "learning_rate": 7.737371563828071e-05,
      "loss": 1.1497,
      "step": 21130
    },
    {
      "epoch": 3.4080283733677255,
      "grad_norm": 36.0,
      "learning_rate": 7.736295658722901e-05,
      "loss": 2.1049,
      "step": 21140
    },
    {
      "epoch": 3.4096404965339353,
      "grad_norm": 63.75,
      "learning_rate": 7.735219753617732e-05,
      "loss": 2.0938,
      "step": 21150
    },
    {
      "epoch": 3.411252619700145,
      "grad_norm": 13.9375,
      "learning_rate": 7.734143848512561e-05,
      "loss": 1.2466,
      "step": 21160
    },
    {
      "epoch": 3.412864742866355,
      "grad_norm": 25.375,
      "learning_rate": 7.733067943407391e-05,
      "loss": 2.7398,
      "step": 21170
    },
    {
      "epoch": 3.414476866032565,
      "grad_norm": 22.25,
      "learning_rate": 7.731992038302222e-05,
      "loss": 2.4091,
      "step": 21180
    },
    {
      "epoch": 3.4160889891987747,
      "grad_norm": 30.5,
      "learning_rate": 7.730916133197053e-05,
      "loss": 1.9153,
      "step": 21190
    },
    {
      "epoch": 3.4177011123649845,
      "grad_norm": 16.125,
      "learning_rate": 7.729840228091883e-05,
      "loss": 3.0684,
      "step": 21200
    },
    {
      "epoch": 3.4193132355311944,
      "grad_norm": 0.296875,
      "learning_rate": 7.728764322986714e-05,
      "loss": 1.7619,
      "step": 21210
    },
    {
      "epoch": 3.4209253586974047,
      "grad_norm": 19.375,
      "learning_rate": 7.727688417881543e-05,
      "loss": 2.8299,
      "step": 21220
    },
    {
      "epoch": 3.4225374818636145,
      "grad_norm": 58.5,
      "learning_rate": 7.726612512776373e-05,
      "loss": 2.5729,
      "step": 21230
    },
    {
      "epoch": 3.4241496050298244,
      "grad_norm": 23.375,
      "learning_rate": 7.725536607671203e-05,
      "loss": 3.9322,
      "step": 21240
    },
    {
      "epoch": 3.425761728196034,
      "grad_norm": 81.5,
      "learning_rate": 7.724460702566034e-05,
      "loss": 1.1114,
      "step": 21250
    },
    {
      "epoch": 3.427373851362244,
      "grad_norm": 41.75,
      "learning_rate": 7.723384797460865e-05,
      "loss": 1.4444,
      "step": 21260
    },
    {
      "epoch": 3.428985974528454,
      "grad_norm": 19.125,
      "learning_rate": 7.722308892355694e-05,
      "loss": 2.1794,
      "step": 21270
    },
    {
      "epoch": 3.4305980976946637,
      "grad_norm": 0.06982421875,
      "learning_rate": 7.721232987250525e-05,
      "loss": 2.8433,
      "step": 21280
    },
    {
      "epoch": 3.432210220860874,
      "grad_norm": 184.0,
      "learning_rate": 7.720157082145355e-05,
      "loss": 1.7824,
      "step": 21290
    },
    {
      "epoch": 3.433822344027084,
      "grad_norm": 16.125,
      "learning_rate": 7.719081177040185e-05,
      "loss": 2.1118,
      "step": 21300
    },
    {
      "epoch": 3.4354344671932937,
      "grad_norm": 13.1875,
      "learning_rate": 7.718005271935016e-05,
      "loss": 1.8628,
      "step": 21310
    },
    {
      "epoch": 3.4370465903595036,
      "grad_norm": 98.5,
      "learning_rate": 7.716929366829847e-05,
      "loss": 2.0897,
      "step": 21320
    },
    {
      "epoch": 3.4386587135257134,
      "grad_norm": 9.5,
      "learning_rate": 7.715853461724676e-05,
      "loss": 2.511,
      "step": 21330
    },
    {
      "epoch": 3.4402708366919232,
      "grad_norm": 126.0,
      "learning_rate": 7.714777556619507e-05,
      "loss": 1.9992,
      "step": 21340
    },
    {
      "epoch": 3.441882959858133,
      "grad_norm": 10.375,
      "learning_rate": 7.713701651514337e-05,
      "loss": 1.3164,
      "step": 21350
    },
    {
      "epoch": 3.443495083024343,
      "grad_norm": 36.25,
      "learning_rate": 7.712625746409167e-05,
      "loss": 2.5768,
      "step": 21360
    },
    {
      "epoch": 3.4451072061905528,
      "grad_norm": 28.0,
      "learning_rate": 7.711549841303998e-05,
      "loss": 1.8302,
      "step": 21370
    },
    {
      "epoch": 3.446719329356763,
      "grad_norm": 48.75,
      "learning_rate": 7.710473936198827e-05,
      "loss": 2.8434,
      "step": 21380
    },
    {
      "epoch": 3.448331452522973,
      "grad_norm": 67.5,
      "learning_rate": 7.709398031093658e-05,
      "loss": 2.5773,
      "step": 21390
    },
    {
      "epoch": 3.4499435756891828,
      "grad_norm": 33.0,
      "learning_rate": 7.708322125988489e-05,
      "loss": 1.5918,
      "step": 21400
    },
    {
      "epoch": 3.4515556988553926,
      "grad_norm": 13.4375,
      "learning_rate": 7.707246220883319e-05,
      "loss": 1.8227,
      "step": 21410
    },
    {
      "epoch": 3.4531678220216024,
      "grad_norm": 1.921875,
      "learning_rate": 7.706170315778149e-05,
      "loss": 1.5495,
      "step": 21420
    },
    {
      "epoch": 3.4547799451878123,
      "grad_norm": 18.75,
      "learning_rate": 7.70509441067298e-05,
      "loss": 3.6448,
      "step": 21430
    },
    {
      "epoch": 3.456392068354022,
      "grad_norm": 21.625,
      "learning_rate": 7.704018505567809e-05,
      "loss": 1.3847,
      "step": 21440
    },
    {
      "epoch": 3.458004191520232,
      "grad_norm": 0.0,
      "learning_rate": 7.702942600462639e-05,
      "loss": 2.555,
      "step": 21450
    },
    {
      "epoch": 3.459616314686442,
      "grad_norm": 22.5,
      "learning_rate": 7.70186669535747e-05,
      "loss": 1.0245,
      "step": 21460
    },
    {
      "epoch": 3.461228437852652,
      "grad_norm": 27.375,
      "learning_rate": 7.700790790252301e-05,
      "loss": 2.1771,
      "step": 21470
    },
    {
      "epoch": 3.462840561018862,
      "grad_norm": 37.0,
      "learning_rate": 7.69971488514713e-05,
      "loss": 0.6685,
      "step": 21480
    },
    {
      "epoch": 3.464452684185072,
      "grad_norm": 55.5,
      "learning_rate": 7.698638980041961e-05,
      "loss": 3.3608,
      "step": 21490
    },
    {
      "epoch": 3.4660648073512816,
      "grad_norm": 48.0,
      "learning_rate": 7.697563074936791e-05,
      "loss": 1.3301,
      "step": 21500
    },
    {
      "epoch": 3.4676769305174915,
      "grad_norm": 63.5,
      "learning_rate": 7.696487169831621e-05,
      "loss": 1.3611,
      "step": 21510
    },
    {
      "epoch": 3.4692890536837013,
      "grad_norm": 38.75,
      "learning_rate": 7.695411264726452e-05,
      "loss": 1.0714,
      "step": 21520
    },
    {
      "epoch": 3.470901176849911,
      "grad_norm": 41.0,
      "learning_rate": 7.694335359621281e-05,
      "loss": 1.5693,
      "step": 21530
    },
    {
      "epoch": 3.4725133000161215,
      "grad_norm": 82.0,
      "learning_rate": 7.693259454516112e-05,
      "loss": 2.7305,
      "step": 21540
    },
    {
      "epoch": 3.4741254231823313,
      "grad_norm": 53.5,
      "learning_rate": 7.692183549410942e-05,
      "loss": 3.0796,
      "step": 21550
    },
    {
      "epoch": 3.475737546348541,
      "grad_norm": 92.0,
      "learning_rate": 7.691107644305773e-05,
      "loss": 1.6412,
      "step": 21560
    },
    {
      "epoch": 3.477349669514751,
      "grad_norm": 48.0,
      "learning_rate": 7.690031739200603e-05,
      "loss": 1.5152,
      "step": 21570
    },
    {
      "epoch": 3.478961792680961,
      "grad_norm": 9.5,
      "learning_rate": 7.688955834095432e-05,
      "loss": 2.6231,
      "step": 21580
    },
    {
      "epoch": 3.4805739158471707,
      "grad_norm": 154.0,
      "learning_rate": 7.687879928990263e-05,
      "loss": 2.156,
      "step": 21590
    },
    {
      "epoch": 3.4821860390133805,
      "grad_norm": 23.75,
      "learning_rate": 7.686804023885093e-05,
      "loss": 1.4518,
      "step": 21600
    },
    {
      "epoch": 3.4837981621795904,
      "grad_norm": 75.0,
      "learning_rate": 7.685728118779924e-05,
      "loss": 1.8862,
      "step": 21610
    },
    {
      "epoch": 3.4854102853458,
      "grad_norm": 32.25,
      "learning_rate": 7.684652213674755e-05,
      "loss": 1.4961,
      "step": 21620
    },
    {
      "epoch": 3.4870224085120105,
      "grad_norm": 40.5,
      "learning_rate": 7.683576308569585e-05,
      "loss": 2.1207,
      "step": 21630
    },
    {
      "epoch": 3.4886345316782204,
      "grad_norm": 31.25,
      "learning_rate": 7.682500403464414e-05,
      "loss": 1.728,
      "step": 21640
    },
    {
      "epoch": 3.49024665484443,
      "grad_norm": 25.625,
      "learning_rate": 7.681424498359245e-05,
      "loss": 1.8305,
      "step": 21650
    },
    {
      "epoch": 3.49185877801064,
      "grad_norm": 9.8125,
      "learning_rate": 7.680348593254075e-05,
      "loss": 2.5628,
      "step": 21660
    },
    {
      "epoch": 3.49347090117685,
      "grad_norm": 25.875,
      "learning_rate": 7.679272688148905e-05,
      "loss": 1.3791,
      "step": 21670
    },
    {
      "epoch": 3.4950830243430597,
      "grad_norm": 37.5,
      "learning_rate": 7.678196783043736e-05,
      "loss": 1.7925,
      "step": 21680
    },
    {
      "epoch": 3.4966951475092696,
      "grad_norm": 27.5,
      "learning_rate": 7.677120877938567e-05,
      "loss": 2.7332,
      "step": 21690
    },
    {
      "epoch": 3.4983072706754794,
      "grad_norm": 17.5,
      "learning_rate": 7.676044972833396e-05,
      "loss": 0.7916,
      "step": 21700
    },
    {
      "epoch": 3.4999193938416893,
      "grad_norm": 0.1220703125,
      "learning_rate": 7.674969067728227e-05,
      "loss": 2.0084,
      "step": 21710
    },
    {
      "epoch": 3.5015315170078996,
      "grad_norm": 16.375,
      "learning_rate": 7.673893162623057e-05,
      "loss": 3.8077,
      "step": 21720
    },
    {
      "epoch": 3.5031436401741094,
      "grad_norm": 10.375,
      "learning_rate": 7.672817257517887e-05,
      "loss": 1.1491,
      "step": 21730
    },
    {
      "epoch": 3.5047557633403192,
      "grad_norm": 59.5,
      "learning_rate": 7.671741352412718e-05,
      "loss": 3.6156,
      "step": 21740
    },
    {
      "epoch": 3.506367886506529,
      "grad_norm": 0.0,
      "learning_rate": 7.670665447307547e-05,
      "loss": 2.7862,
      "step": 21750
    },
    {
      "epoch": 3.507980009672739,
      "grad_norm": 39.75,
      "learning_rate": 7.669589542202378e-05,
      "loss": 1.4079,
      "step": 21760
    },
    {
      "epoch": 3.5095921328389488,
      "grad_norm": 36.5,
      "learning_rate": 7.668513637097209e-05,
      "loss": 2.1596,
      "step": 21770
    },
    {
      "epoch": 3.5112042560051586,
      "grad_norm": 30.75,
      "learning_rate": 7.667437731992039e-05,
      "loss": 3.6101,
      "step": 21780
    },
    {
      "epoch": 3.512816379171369,
      "grad_norm": 127.5,
      "learning_rate": 7.666361826886869e-05,
      "loss": 2.0107,
      "step": 21790
    },
    {
      "epoch": 3.5144285023375788,
      "grad_norm": 4.6875,
      "learning_rate": 7.6652859217817e-05,
      "loss": 1.1082,
      "step": 21800
    },
    {
      "epoch": 3.5160406255037886,
      "grad_norm": 49.5,
      "learning_rate": 7.664210016676529e-05,
      "loss": 2.7188,
      "step": 21810
    },
    {
      "epoch": 3.5176527486699984,
      "grad_norm": 56.5,
      "learning_rate": 7.663134111571359e-05,
      "loss": 2.4098,
      "step": 21820
    },
    {
      "epoch": 3.5192648718362083,
      "grad_norm": 24.75,
      "learning_rate": 7.66205820646619e-05,
      "loss": 2.5729,
      "step": 21830
    },
    {
      "epoch": 3.520876995002418,
      "grad_norm": 12.3125,
      "learning_rate": 7.660982301361021e-05,
      "loss": 2.1833,
      "step": 21840
    },
    {
      "epoch": 3.522489118168628,
      "grad_norm": 24.125,
      "learning_rate": 7.65990639625585e-05,
      "loss": 2.018,
      "step": 21850
    },
    {
      "epoch": 3.524101241334838,
      "grad_norm": 57.5,
      "learning_rate": 7.658830491150682e-05,
      "loss": 1.9989,
      "step": 21860
    },
    {
      "epoch": 3.5257133645010477,
      "grad_norm": 114.5,
      "learning_rate": 7.657754586045511e-05,
      "loss": 2.0549,
      "step": 21870
    },
    {
      "epoch": 3.527325487667258,
      "grad_norm": 57.0,
      "learning_rate": 7.656678680940341e-05,
      "loss": 3.2842,
      "step": 21880
    },
    {
      "epoch": 3.528937610833468,
      "grad_norm": 47.0,
      "learning_rate": 7.65560277583517e-05,
      "loss": 1.0747,
      "step": 21890
    },
    {
      "epoch": 3.5305497339996776,
      "grad_norm": 55.5,
      "learning_rate": 7.654526870730002e-05,
      "loss": 1.6451,
      "step": 21900
    },
    {
      "epoch": 3.5321618571658875,
      "grad_norm": 61.0,
      "learning_rate": 7.653450965624833e-05,
      "loss": 1.8378,
      "step": 21910
    },
    {
      "epoch": 3.5337739803320973,
      "grad_norm": 30.25,
      "learning_rate": 7.652375060519662e-05,
      "loss": 2.7093,
      "step": 21920
    },
    {
      "epoch": 3.535386103498307,
      "grad_norm": 46.75,
      "learning_rate": 7.651299155414493e-05,
      "loss": 1.833,
      "step": 21930
    },
    {
      "epoch": 3.536998226664517,
      "grad_norm": 0.0103759765625,
      "learning_rate": 7.650223250309323e-05,
      "loss": 1.1572,
      "step": 21940
    },
    {
      "epoch": 3.5386103498307273,
      "grad_norm": 16.875,
      "learning_rate": 7.649147345204153e-05,
      "loss": 3.7768,
      "step": 21950
    },
    {
      "epoch": 3.5402224729969367,
      "grad_norm": 7.5,
      "learning_rate": 7.648071440098984e-05,
      "loss": 3.518,
      "step": 21960
    },
    {
      "epoch": 3.541834596163147,
      "grad_norm": 18.875,
      "learning_rate": 7.646995534993813e-05,
      "loss": 1.2854,
      "step": 21970
    },
    {
      "epoch": 3.543446719329357,
      "grad_norm": 27.375,
      "learning_rate": 7.645919629888644e-05,
      "loss": 1.4075,
      "step": 21980
    },
    {
      "epoch": 3.5450588424955667,
      "grad_norm": 35.75,
      "learning_rate": 7.644843724783475e-05,
      "loss": 2.6297,
      "step": 21990
    },
    {
      "epoch": 3.5466709656617765,
      "grad_norm": 21.625,
      "learning_rate": 7.643767819678305e-05,
      "loss": 0.662,
      "step": 22000
    },
    {
      "epoch": 3.5482830888279864,
      "grad_norm": 15.0625,
      "learning_rate": 7.642691914573134e-05,
      "loss": 2.403,
      "step": 22010
    },
    {
      "epoch": 3.549895211994196,
      "grad_norm": 0.1533203125,
      "learning_rate": 7.641616009467965e-05,
      "loss": 1.1909,
      "step": 22020
    },
    {
      "epoch": 3.551507335160406,
      "grad_norm": 1.765625,
      "learning_rate": 7.640540104362795e-05,
      "loss": 3.4391,
      "step": 22030
    },
    {
      "epoch": 3.5531194583266164,
      "grad_norm": 35.0,
      "learning_rate": 7.639464199257626e-05,
      "loss": 2.4002,
      "step": 22040
    },
    {
      "epoch": 3.554731581492826,
      "grad_norm": 33.75,
      "learning_rate": 7.638388294152457e-05,
      "loss": 2.717,
      "step": 22050
    },
    {
      "epoch": 3.556343704659036,
      "grad_norm": 23.875,
      "learning_rate": 7.637312389047287e-05,
      "loss": 2.3823,
      "step": 22060
    },
    {
      "epoch": 3.557955827825246,
      "grad_norm": 33.5,
      "learning_rate": 7.636236483942116e-05,
      "loss": 2.7611,
      "step": 22070
    },
    {
      "epoch": 3.5595679509914557,
      "grad_norm": 10.5625,
      "learning_rate": 7.635160578836947e-05,
      "loss": 1.4046,
      "step": 22080
    },
    {
      "epoch": 3.5611800741576656,
      "grad_norm": 8.75,
      "learning_rate": 7.634084673731777e-05,
      "loss": 2.2815,
      "step": 22090
    },
    {
      "epoch": 3.5627921973238754,
      "grad_norm": 48.75,
      "learning_rate": 7.633008768626607e-05,
      "loss": 1.8085,
      "step": 22100
    },
    {
      "epoch": 3.5644043204900857,
      "grad_norm": 112.5,
      "learning_rate": 7.631932863521438e-05,
      "loss": 2.2228,
      "step": 22110
    },
    {
      "epoch": 3.566016443656295,
      "grad_norm": 18.25,
      "learning_rate": 7.630856958416269e-05,
      "loss": 2.3106,
      "step": 22120
    },
    {
      "epoch": 3.5676285668225054,
      "grad_norm": 14.625,
      "learning_rate": 7.629781053311098e-05,
      "loss": 0.5455,
      "step": 22130
    },
    {
      "epoch": 3.5692406899887152,
      "grad_norm": 30.375,
      "learning_rate": 7.62870514820593e-05,
      "loss": 1.2542,
      "step": 22140
    },
    {
      "epoch": 3.570852813154925,
      "grad_norm": 8.875,
      "learning_rate": 7.627629243100759e-05,
      "loss": 1.5896,
      "step": 22150
    },
    {
      "epoch": 3.572464936321135,
      "grad_norm": 26.875,
      "learning_rate": 7.626553337995589e-05,
      "loss": 1.3174,
      "step": 22160
    },
    {
      "epoch": 3.5740770594873448,
      "grad_norm": 27.25,
      "learning_rate": 7.62547743289042e-05,
      "loss": 1.9782,
      "step": 22170
    },
    {
      "epoch": 3.5756891826535546,
      "grad_norm": 30.625,
      "learning_rate": 7.62440152778525e-05,
      "loss": 2.2225,
      "step": 22180
    },
    {
      "epoch": 3.5773013058197645,
      "grad_norm": 28.5,
      "learning_rate": 7.62332562268008e-05,
      "loss": 2.5115,
      "step": 22190
    },
    {
      "epoch": 3.5789134289859748,
      "grad_norm": 0.310546875,
      "learning_rate": 7.622249717574911e-05,
      "loss": 3.0342,
      "step": 22200
    },
    {
      "epoch": 3.580525552152184,
      "grad_norm": 13.6875,
      "learning_rate": 7.621173812469741e-05,
      "loss": 0.9383,
      "step": 22210
    },
    {
      "epoch": 3.5821376753183944,
      "grad_norm": 17.25,
      "learning_rate": 7.620097907364571e-05,
      "loss": 0.7511,
      "step": 22220
    },
    {
      "epoch": 3.5837497984846043,
      "grad_norm": 64.0,
      "learning_rate": 7.6190220022594e-05,
      "loss": 1.3657,
      "step": 22230
    },
    {
      "epoch": 3.585361921650814,
      "grad_norm": 75.0,
      "learning_rate": 7.617946097154231e-05,
      "loss": 1.4627,
      "step": 22240
    },
    {
      "epoch": 3.586974044817024,
      "grad_norm": 43.0,
      "learning_rate": 7.616870192049061e-05,
      "loss": 1.2812,
      "step": 22250
    },
    {
      "epoch": 3.588586167983234,
      "grad_norm": 21.0,
      "learning_rate": 7.615794286943892e-05,
      "loss": 1.6789,
      "step": 22260
    },
    {
      "epoch": 3.5901982911494437,
      "grad_norm": 3.296875,
      "learning_rate": 7.614718381838723e-05,
      "loss": 1.5064,
      "step": 22270
    },
    {
      "epoch": 3.5918104143156535,
      "grad_norm": 32.25,
      "learning_rate": 7.613642476733553e-05,
      "loss": 2.8089,
      "step": 22280
    },
    {
      "epoch": 3.593422537481864,
      "grad_norm": 44.0,
      "learning_rate": 7.612566571628382e-05,
      "loss": 0.894,
      "step": 22290
    },
    {
      "epoch": 3.5950346606480736,
      "grad_norm": 19.875,
      "learning_rate": 7.611490666523213e-05,
      "loss": 0.9854,
      "step": 22300
    },
    {
      "epoch": 3.5966467838142835,
      "grad_norm": 109.5,
      "learning_rate": 7.610414761418043e-05,
      "loss": 2.6353,
      "step": 22310
    },
    {
      "epoch": 3.5982589069804933,
      "grad_norm": 10.875,
      "learning_rate": 7.609338856312873e-05,
      "loss": 2.2219,
      "step": 22320
    },
    {
      "epoch": 3.599871030146703,
      "grad_norm": 33.25,
      "learning_rate": 7.608262951207704e-05,
      "loss": 0.6958,
      "step": 22330
    },
    {
      "epoch": 3.601483153312913,
      "grad_norm": 51.5,
      "learning_rate": 7.607187046102535e-05,
      "loss": 2.1839,
      "step": 22340
    },
    {
      "epoch": 3.603095276479123,
      "grad_norm": 108.0,
      "learning_rate": 7.606111140997364e-05,
      "loss": 2.0519,
      "step": 22350
    },
    {
      "epoch": 3.604707399645333,
      "grad_norm": 38.25,
      "learning_rate": 7.605035235892195e-05,
      "loss": 1.3552,
      "step": 22360
    },
    {
      "epoch": 3.6063195228115426,
      "grad_norm": 31.375,
      "learning_rate": 7.603959330787025e-05,
      "loss": 2.9038,
      "step": 22370
    },
    {
      "epoch": 3.607931645977753,
      "grad_norm": 27.25,
      "learning_rate": 7.602883425681855e-05,
      "loss": 1.2041,
      "step": 22380
    },
    {
      "epoch": 3.6095437691439627,
      "grad_norm": 34.75,
      "learning_rate": 7.601807520576686e-05,
      "loss": 2.0217,
      "step": 22390
    },
    {
      "epoch": 3.6111558923101725,
      "grad_norm": 76.5,
      "learning_rate": 7.600731615471515e-05,
      "loss": 2.1175,
      "step": 22400
    },
    {
      "epoch": 3.6127680154763824,
      "grad_norm": 5.4375,
      "learning_rate": 7.599655710366346e-05,
      "loss": 1.0459,
      "step": 22410
    },
    {
      "epoch": 3.614380138642592,
      "grad_norm": 39.5,
      "learning_rate": 7.598579805261177e-05,
      "loss": 2.0857,
      "step": 22420
    },
    {
      "epoch": 3.615992261808802,
      "grad_norm": 38.75,
      "learning_rate": 7.597503900156007e-05,
      "loss": 2.1556,
      "step": 22430
    },
    {
      "epoch": 3.617604384975012,
      "grad_norm": 79.0,
      "learning_rate": 7.596427995050837e-05,
      "loss": 2.5418,
      "step": 22440
    },
    {
      "epoch": 3.619216508141222,
      "grad_norm": 53.25,
      "learning_rate": 7.595352089945668e-05,
      "loss": 2.1393,
      "step": 22450
    },
    {
      "epoch": 3.620828631307432,
      "grad_norm": 50.75,
      "learning_rate": 7.594276184840497e-05,
      "loss": 2.3063,
      "step": 22460
    },
    {
      "epoch": 3.622440754473642,
      "grad_norm": 102.0,
      "learning_rate": 7.593200279735327e-05,
      "loss": 2.6115,
      "step": 22470
    },
    {
      "epoch": 3.6240528776398517,
      "grad_norm": 36.25,
      "learning_rate": 7.592124374630158e-05,
      "loss": 3.9844,
      "step": 22480
    },
    {
      "epoch": 3.6256650008060616,
      "grad_norm": 32.0,
      "learning_rate": 7.591048469524989e-05,
      "loss": 1.993,
      "step": 22490
    },
    {
      "epoch": 3.6272771239722714,
      "grad_norm": 20.625,
      "learning_rate": 7.589972564419819e-05,
      "loss": 2.1938,
      "step": 22500
    },
    {
      "epoch": 3.6288892471384813,
      "grad_norm": 36.5,
      "learning_rate": 7.58889665931465e-05,
      "loss": 1.5597,
      "step": 22510
    },
    {
      "epoch": 3.6305013703046916,
      "grad_norm": 43.75,
      "learning_rate": 7.587820754209479e-05,
      "loss": 4.3725,
      "step": 22520
    },
    {
      "epoch": 3.632113493470901,
      "grad_norm": 81.5,
      "learning_rate": 7.586744849104309e-05,
      "loss": 0.7095,
      "step": 22530
    },
    {
      "epoch": 3.6337256166371112,
      "grad_norm": 0.169921875,
      "learning_rate": 7.58566894399914e-05,
      "loss": 3.8662,
      "step": 22540
    },
    {
      "epoch": 3.635337739803321,
      "grad_norm": 65.5,
      "learning_rate": 7.58459303889397e-05,
      "loss": 2.4291,
      "step": 22550
    },
    {
      "epoch": 3.636949862969531,
      "grad_norm": 11.4375,
      "learning_rate": 7.5835171337888e-05,
      "loss": 3.3872,
      "step": 22560
    },
    {
      "epoch": 3.6385619861357408,
      "grad_norm": 1.859375,
      "learning_rate": 7.58244122868363e-05,
      "loss": 2.2914,
      "step": 22570
    },
    {
      "epoch": 3.6401741093019506,
      "grad_norm": 7.78125,
      "learning_rate": 7.581365323578461e-05,
      "loss": 1.2474,
      "step": 22580
    },
    {
      "epoch": 3.6417862324681605,
      "grad_norm": 32.75,
      "learning_rate": 7.580289418473291e-05,
      "loss": 1.4561,
      "step": 22590
    },
    {
      "epoch": 3.6433983556343703,
      "grad_norm": 39.25,
      "learning_rate": 7.57921351336812e-05,
      "loss": 2.9823,
      "step": 22600
    },
    {
      "epoch": 3.6450104788005806,
      "grad_norm": 4.375,
      "learning_rate": 7.578137608262951e-05,
      "loss": 1.8589,
      "step": 22610
    },
    {
      "epoch": 3.64662260196679,
      "grad_norm": 44.75,
      "learning_rate": 7.577061703157781e-05,
      "loss": 1.8646,
      "step": 22620
    },
    {
      "epoch": 3.6482347251330003,
      "grad_norm": 43.25,
      "learning_rate": 7.575985798052612e-05,
      "loss": 2.9602,
      "step": 22630
    },
    {
      "epoch": 3.64984684829921,
      "grad_norm": 53.0,
      "learning_rate": 7.574909892947443e-05,
      "loss": 3.1727,
      "step": 22640
    },
    {
      "epoch": 3.65145897146542,
      "grad_norm": 101.0,
      "learning_rate": 7.573833987842273e-05,
      "loss": 2.9266,
      "step": 22650
    },
    {
      "epoch": 3.65307109463163,
      "grad_norm": 1.375,
      "learning_rate": 7.572758082737102e-05,
      "loss": 2.0182,
      "step": 22660
    },
    {
      "epoch": 3.6546832177978397,
      "grad_norm": 82.0,
      "learning_rate": 7.571682177631933e-05,
      "loss": 2.4329,
      "step": 22670
    },
    {
      "epoch": 3.6562953409640495,
      "grad_norm": 40.0,
      "learning_rate": 7.570606272526763e-05,
      "loss": 3.1195,
      "step": 22680
    },
    {
      "epoch": 3.6579074641302594,
      "grad_norm": 234.0,
      "learning_rate": 7.569530367421594e-05,
      "loss": 2.523,
      "step": 22690
    },
    {
      "epoch": 3.6595195872964696,
      "grad_norm": 0.0,
      "learning_rate": 7.568454462316425e-05,
      "loss": 1.4688,
      "step": 22700
    },
    {
      "epoch": 3.6611317104626795,
      "grad_norm": 42.25,
      "learning_rate": 7.567378557211255e-05,
      "loss": 2.7004,
      "step": 22710
    },
    {
      "epoch": 3.6627438336288893,
      "grad_norm": 28.625,
      "learning_rate": 7.566302652106084e-05,
      "loss": 1.9866,
      "step": 22720
    },
    {
      "epoch": 3.664355956795099,
      "grad_norm": 53.75,
      "learning_rate": 7.565226747000915e-05,
      "loss": 2.5174,
      "step": 22730
    },
    {
      "epoch": 3.665968079961309,
      "grad_norm": 93.5,
      "learning_rate": 7.564150841895745e-05,
      "loss": 1.6796,
      "step": 22740
    },
    {
      "epoch": 3.667580203127519,
      "grad_norm": 12.125,
      "learning_rate": 7.563074936790575e-05,
      "loss": 2.4396,
      "step": 22750
    },
    {
      "epoch": 3.6691923262937287,
      "grad_norm": 29.5,
      "learning_rate": 7.561999031685406e-05,
      "loss": 1.9078,
      "step": 22760
    },
    {
      "epoch": 3.670804449459939,
      "grad_norm": 51.25,
      "learning_rate": 7.560923126580237e-05,
      "loss": 2.3421,
      "step": 22770
    },
    {
      "epoch": 3.6724165726261484,
      "grad_norm": 38.75,
      "learning_rate": 7.559847221475066e-05,
      "loss": 1.8396,
      "step": 22780
    },
    {
      "epoch": 3.6740286957923587,
      "grad_norm": 36.25,
      "learning_rate": 7.558771316369897e-05,
      "loss": 1.2408,
      "step": 22790
    },
    {
      "epoch": 3.6756408189585685,
      "grad_norm": 20.625,
      "learning_rate": 7.557695411264727e-05,
      "loss": 1.4041,
      "step": 22800
    },
    {
      "epoch": 3.6772529421247784,
      "grad_norm": 22.25,
      "learning_rate": 7.556619506159557e-05,
      "loss": 1.4295,
      "step": 22810
    },
    {
      "epoch": 3.6788650652909882,
      "grad_norm": 15.375,
      "learning_rate": 7.555543601054388e-05,
      "loss": 3.6505,
      "step": 22820
    },
    {
      "epoch": 3.680477188457198,
      "grad_norm": 27.0,
      "learning_rate": 7.554467695949217e-05,
      "loss": 1.5655,
      "step": 22830
    },
    {
      "epoch": 3.682089311623408,
      "grad_norm": 0.0,
      "learning_rate": 7.553391790844048e-05,
      "loss": 1.5195,
      "step": 22840
    },
    {
      "epoch": 3.6837014347896178,
      "grad_norm": 27.875,
      "learning_rate": 7.552315885738879e-05,
      "loss": 2.4261,
      "step": 22850
    },
    {
      "epoch": 3.685313557955828,
      "grad_norm": 95.5,
      "learning_rate": 7.551239980633709e-05,
      "loss": 2.0204,
      "step": 22860
    },
    {
      "epoch": 3.686925681122038,
      "grad_norm": 26.25,
      "learning_rate": 7.550164075528539e-05,
      "loss": 2.0197,
      "step": 22870
    },
    {
      "epoch": 3.6885378042882477,
      "grad_norm": 73.5,
      "learning_rate": 7.549088170423368e-05,
      "loss": 2.1868,
      "step": 22880
    },
    {
      "epoch": 3.6901499274544576,
      "grad_norm": 4.8125,
      "learning_rate": 7.548012265318199e-05,
      "loss": 3.0752,
      "step": 22890
    },
    {
      "epoch": 3.6917620506206674,
      "grad_norm": 43.25,
      "learning_rate": 7.546936360213029e-05,
      "loss": 2.0901,
      "step": 22900
    },
    {
      "epoch": 3.6933741737868773,
      "grad_norm": 0.51171875,
      "learning_rate": 7.54586045510786e-05,
      "loss": 1.7831,
      "step": 22910
    },
    {
      "epoch": 3.694986296953087,
      "grad_norm": 12.25,
      "learning_rate": 7.544784550002691e-05,
      "loss": 1.3566,
      "step": 22920
    },
    {
      "epoch": 3.696598420119297,
      "grad_norm": 49.0,
      "learning_rate": 7.54370864489752e-05,
      "loss": 3.35,
      "step": 22930
    },
    {
      "epoch": 3.698210543285507,
      "grad_norm": 40.75,
      "learning_rate": 7.54263273979235e-05,
      "loss": 2.7337,
      "step": 22940
    },
    {
      "epoch": 3.699822666451717,
      "grad_norm": 6.03125,
      "learning_rate": 7.541556834687181e-05,
      "loss": 1.8779,
      "step": 22950
    },
    {
      "epoch": 3.701434789617927,
      "grad_norm": 18.5,
      "learning_rate": 7.540480929582011e-05,
      "loss": 1.9631,
      "step": 22960
    },
    {
      "epoch": 3.703046912784137,
      "grad_norm": 84.0,
      "learning_rate": 7.53940502447684e-05,
      "loss": 1.2819,
      "step": 22970
    },
    {
      "epoch": 3.7046590359503466,
      "grad_norm": 22.5,
      "learning_rate": 7.538329119371672e-05,
      "loss": 2.5719,
      "step": 22980
    },
    {
      "epoch": 3.7062711591165565,
      "grad_norm": 0.003875732421875,
      "learning_rate": 7.537253214266503e-05,
      "loss": 1.8937,
      "step": 22990
    },
    {
      "epoch": 3.7078832822827663,
      "grad_norm": 47.25,
      "learning_rate": 7.536177309161332e-05,
      "loss": 0.9563,
      "step": 23000
    },
    {
      "epoch": 3.709495405448976,
      "grad_norm": 22.375,
      "learning_rate": 7.535101404056163e-05,
      "loss": 1.2342,
      "step": 23010
    },
    {
      "epoch": 3.7111075286151864,
      "grad_norm": 22.625,
      "learning_rate": 7.534025498950993e-05,
      "loss": 1.5934,
      "step": 23020
    },
    {
      "epoch": 3.712719651781396,
      "grad_norm": 64.0,
      "learning_rate": 7.532949593845822e-05,
      "loss": 4.2814,
      "step": 23030
    },
    {
      "epoch": 3.714331774947606,
      "grad_norm": 0.0,
      "learning_rate": 7.531873688740654e-05,
      "loss": 1.6395,
      "step": 23040
    },
    {
      "epoch": 3.715943898113816,
      "grad_norm": 0.63671875,
      "learning_rate": 7.530797783635483e-05,
      "loss": 2.6123,
      "step": 23050
    },
    {
      "epoch": 3.717556021280026,
      "grad_norm": 59.25,
      "learning_rate": 7.529721878530314e-05,
      "loss": 3.0447,
      "step": 23060
    },
    {
      "epoch": 3.7191681444462357,
      "grad_norm": 14.125,
      "learning_rate": 7.528645973425145e-05,
      "loss": 1.2025,
      "step": 23070
    },
    {
      "epoch": 3.7207802676124455,
      "grad_norm": 28.125,
      "learning_rate": 7.527570068319975e-05,
      "loss": 1.7909,
      "step": 23080
    },
    {
      "epoch": 3.7223923907786554,
      "grad_norm": 39.5,
      "learning_rate": 7.526494163214804e-05,
      "loss": 3.0516,
      "step": 23090
    },
    {
      "epoch": 3.724004513944865,
      "grad_norm": 29.625,
      "learning_rate": 7.525418258109635e-05,
      "loss": 1.5159,
      "step": 23100
    },
    {
      "epoch": 3.7256166371110755,
      "grad_norm": 42.0,
      "learning_rate": 7.524342353004465e-05,
      "loss": 2.5393,
      "step": 23110
    },
    {
      "epoch": 3.7272287602772853,
      "grad_norm": 87.0,
      "learning_rate": 7.523266447899295e-05,
      "loss": 1.3016,
      "step": 23120
    },
    {
      "epoch": 3.728840883443495,
      "grad_norm": 31.25,
      "learning_rate": 7.522190542794126e-05,
      "loss": 3.3315,
      "step": 23130
    },
    {
      "epoch": 3.730453006609705,
      "grad_norm": 27.125,
      "learning_rate": 7.521114637688957e-05,
      "loss": 1.6306,
      "step": 23140
    },
    {
      "epoch": 3.732065129775915,
      "grad_norm": 10.0,
      "learning_rate": 7.520038732583786e-05,
      "loss": 2.3694,
      "step": 23150
    },
    {
      "epoch": 3.7336772529421247,
      "grad_norm": 38.5,
      "learning_rate": 7.518962827478617e-05,
      "loss": 1.3691,
      "step": 23160
    },
    {
      "epoch": 3.7352893761083346,
      "grad_norm": 1.0546875,
      "learning_rate": 7.517886922373447e-05,
      "loss": 1.0411,
      "step": 23170
    },
    {
      "epoch": 3.736901499274545,
      "grad_norm": 60.0,
      "learning_rate": 7.516811017268277e-05,
      "loss": 2.636,
      "step": 23180
    },
    {
      "epoch": 3.7385136224407542,
      "grad_norm": 19.375,
      "learning_rate": 7.515735112163108e-05,
      "loss": 2.3408,
      "step": 23190
    },
    {
      "epoch": 3.7401257456069645,
      "grad_norm": 47.75,
      "learning_rate": 7.514659207057937e-05,
      "loss": 3.3749,
      "step": 23200
    },
    {
      "epoch": 3.7417378687731744,
      "grad_norm": 17.125,
      "learning_rate": 7.513583301952768e-05,
      "loss": 1.2533,
      "step": 23210
    },
    {
      "epoch": 3.7433499919393842,
      "grad_norm": 48.0,
      "learning_rate": 7.512507396847598e-05,
      "loss": 3.8177,
      "step": 23220
    },
    {
      "epoch": 3.744962115105594,
      "grad_norm": 160.0,
      "learning_rate": 7.511431491742429e-05,
      "loss": 2.8955,
      "step": 23230
    },
    {
      "epoch": 3.746574238271804,
      "grad_norm": 32.5,
      "learning_rate": 7.510355586637259e-05,
      "loss": 2.6559,
      "step": 23240
    },
    {
      "epoch": 3.7481863614380138,
      "grad_norm": 1.03125,
      "learning_rate": 7.509279681532088e-05,
      "loss": 2.1874,
      "step": 23250
    },
    {
      "epoch": 3.7497984846042236,
      "grad_norm": 54.5,
      "learning_rate": 7.50820377642692e-05,
      "loss": 1.6559,
      "step": 23260
    },
    {
      "epoch": 3.751410607770434,
      "grad_norm": 346.0,
      "learning_rate": 7.507127871321749e-05,
      "loss": 2.2777,
      "step": 23270
    },
    {
      "epoch": 3.7530227309366433,
      "grad_norm": 6.8125,
      "learning_rate": 7.50605196621658e-05,
      "loss": 1.0948,
      "step": 23280
    },
    {
      "epoch": 3.7546348541028536,
      "grad_norm": 16.75,
      "learning_rate": 7.504976061111411e-05,
      "loss": 3.0242,
      "step": 23290
    },
    {
      "epoch": 3.7562469772690634,
      "grad_norm": 0.12158203125,
      "learning_rate": 7.50390015600624e-05,
      "loss": 4.1666,
      "step": 23300
    },
    {
      "epoch": 3.7578591004352733,
      "grad_norm": 12.4375,
      "learning_rate": 7.50282425090107e-05,
      "loss": 2.0552,
      "step": 23310
    },
    {
      "epoch": 3.759471223601483,
      "grad_norm": 25.5,
      "learning_rate": 7.501748345795901e-05,
      "loss": 1.36,
      "step": 23320
    },
    {
      "epoch": 3.761083346767693,
      "grad_norm": 95.0,
      "learning_rate": 7.500672440690731e-05,
      "loss": 2.3443,
      "step": 23330
    },
    {
      "epoch": 3.762695469933903,
      "grad_norm": 65.5,
      "learning_rate": 7.49959653558556e-05,
      "loss": 2.0826,
      "step": 23340
    },
    {
      "epoch": 3.7643075931001126,
      "grad_norm": 55.25,
      "learning_rate": 7.498520630480393e-05,
      "loss": 1.4725,
      "step": 23350
    },
    {
      "epoch": 3.765919716266323,
      "grad_norm": 43.25,
      "learning_rate": 7.497444725375223e-05,
      "loss": 2.8191,
      "step": 23360
    },
    {
      "epoch": 3.767531839432533,
      "grad_norm": 25.625,
      "learning_rate": 7.496368820270052e-05,
      "loss": 1.9074,
      "step": 23370
    },
    {
      "epoch": 3.7691439625987426,
      "grad_norm": 31.625,
      "learning_rate": 7.495292915164883e-05,
      "loss": 2.5472,
      "step": 23380
    },
    {
      "epoch": 3.7707560857649525,
      "grad_norm": 33.0,
      "learning_rate": 7.494217010059713e-05,
      "loss": 2.7221,
      "step": 23390
    },
    {
      "epoch": 3.7723682089311623,
      "grad_norm": 6.125,
      "learning_rate": 7.493141104954543e-05,
      "loss": 2.2214,
      "step": 23400
    },
    {
      "epoch": 3.773980332097372,
      "grad_norm": 61.75,
      "learning_rate": 7.492065199849374e-05,
      "loss": 3.3777,
      "step": 23410
    },
    {
      "epoch": 3.775592455263582,
      "grad_norm": 81.5,
      "learning_rate": 7.490989294744205e-05,
      "loss": 3.5531,
      "step": 23420
    },
    {
      "epoch": 3.7772045784297923,
      "grad_norm": 0.169921875,
      "learning_rate": 7.489913389639034e-05,
      "loss": 1.9117,
      "step": 23430
    },
    {
      "epoch": 3.7788167015960017,
      "grad_norm": 35.0,
      "learning_rate": 7.488837484533865e-05,
      "loss": 2.4393,
      "step": 23440
    },
    {
      "epoch": 3.780428824762212,
      "grad_norm": 0.21875,
      "learning_rate": 7.487761579428695e-05,
      "loss": 1.6604,
      "step": 23450
    },
    {
      "epoch": 3.782040947928422,
      "grad_norm": 112.5,
      "learning_rate": 7.486685674323525e-05,
      "loss": 3.1244,
      "step": 23460
    },
    {
      "epoch": 3.7836530710946317,
      "grad_norm": 61.75,
      "learning_rate": 7.485609769218356e-05,
      "loss": 1.1749,
      "step": 23470
    },
    {
      "epoch": 3.7852651942608415,
      "grad_norm": 1.296875,
      "learning_rate": 7.484533864113185e-05,
      "loss": 1.2844,
      "step": 23480
    },
    {
      "epoch": 3.7868773174270514,
      "grad_norm": 0.02197265625,
      "learning_rate": 7.483457959008016e-05,
      "loss": 2.6383,
      "step": 23490
    },
    {
      "epoch": 3.788489440593261,
      "grad_norm": 31.5,
      "learning_rate": 7.482382053902847e-05,
      "loss": 2.2214,
      "step": 23500
    },
    {
      "epoch": 3.790101563759471,
      "grad_norm": 36.5,
      "learning_rate": 7.481306148797677e-05,
      "loss": 2.1488,
      "step": 23510
    },
    {
      "epoch": 3.7917136869256813,
      "grad_norm": 65.5,
      "learning_rate": 7.480230243692507e-05,
      "loss": 1.3325,
      "step": 23520
    },
    {
      "epoch": 3.793325810091891,
      "grad_norm": 143.0,
      "learning_rate": 7.479154338587338e-05,
      "loss": 3.085,
      "step": 23530
    },
    {
      "epoch": 3.794937933258101,
      "grad_norm": 134.0,
      "learning_rate": 7.478078433482167e-05,
      "loss": 1.3541,
      "step": 23540
    },
    {
      "epoch": 3.796550056424311,
      "grad_norm": 14.3125,
      "learning_rate": 7.477002528376997e-05,
      "loss": 2.6087,
      "step": 23550
    },
    {
      "epoch": 3.7981621795905207,
      "grad_norm": 36.75,
      "learning_rate": 7.475926623271828e-05,
      "loss": 2.4287,
      "step": 23560
    },
    {
      "epoch": 3.7997743027567306,
      "grad_norm": 82.0,
      "learning_rate": 7.474850718166659e-05,
      "loss": 2.8273,
      "step": 23570
    },
    {
      "epoch": 3.8013864259229404,
      "grad_norm": 9.5,
      "learning_rate": 7.473774813061489e-05,
      "loss": 2.3654,
      "step": 23580
    },
    {
      "epoch": 3.8029985490891502,
      "grad_norm": 1.0,
      "learning_rate": 7.472698907956318e-05,
      "loss": 2.7047,
      "step": 23590
    },
    {
      "epoch": 3.80461067225536,
      "grad_norm": 20.125,
      "learning_rate": 7.471623002851149e-05,
      "loss": 1.5799,
      "step": 23600
    },
    {
      "epoch": 3.8062227954215704,
      "grad_norm": 46.5,
      "learning_rate": 7.470547097745979e-05,
      "loss": 1.3048,
      "step": 23610
    },
    {
      "epoch": 3.8078349185877802,
      "grad_norm": 105.5,
      "learning_rate": 7.469471192640808e-05,
      "loss": 3.1886,
      "step": 23620
    },
    {
      "epoch": 3.80944704175399,
      "grad_norm": 31.875,
      "learning_rate": 7.46839528753564e-05,
      "loss": 1.3058,
      "step": 23630
    },
    {
      "epoch": 3.8110591649202,
      "grad_norm": 8.5,
      "learning_rate": 7.46731938243047e-05,
      "loss": 1.8897,
      "step": 23640
    },
    {
      "epoch": 3.8126712880864098,
      "grad_norm": 63.0,
      "learning_rate": 7.4662434773253e-05,
      "loss": 1.6878,
      "step": 23650
    },
    {
      "epoch": 3.8142834112526196,
      "grad_norm": 61.25,
      "learning_rate": 7.465167572220131e-05,
      "loss": 3.1871,
      "step": 23660
    },
    {
      "epoch": 3.8158955344188294,
      "grad_norm": 0.0361328125,
      "learning_rate": 7.464091667114961e-05,
      "loss": 1.7039,
      "step": 23670
    },
    {
      "epoch": 3.8175076575850397,
      "grad_norm": 131.0,
      "learning_rate": 7.46301576200979e-05,
      "loss": 3.1234,
      "step": 23680
    },
    {
      "epoch": 3.819119780751249,
      "grad_norm": 1.25,
      "learning_rate": 7.461939856904621e-05,
      "loss": 2.192,
      "step": 23690
    },
    {
      "epoch": 3.8207319039174594,
      "grad_norm": 36.75,
      "learning_rate": 7.460863951799451e-05,
      "loss": 1.1572,
      "step": 23700
    },
    {
      "epoch": 3.8223440270836693,
      "grad_norm": 4.28125,
      "learning_rate": 7.459788046694282e-05,
      "loss": 1.3929,
      "step": 23710
    },
    {
      "epoch": 3.823956150249879,
      "grad_norm": 39.75,
      "learning_rate": 7.458712141589113e-05,
      "loss": 1.1784,
      "step": 23720
    },
    {
      "epoch": 3.825568273416089,
      "grad_norm": 56.0,
      "learning_rate": 7.457636236483943e-05,
      "loss": 2.5537,
      "step": 23730
    },
    {
      "epoch": 3.827180396582299,
      "grad_norm": 66.5,
      "learning_rate": 7.456560331378772e-05,
      "loss": 1.7463,
      "step": 23740
    },
    {
      "epoch": 3.8287925197485086,
      "grad_norm": 2.84375,
      "learning_rate": 7.455484426273603e-05,
      "loss": 1.3039,
      "step": 23750
    },
    {
      "epoch": 3.8304046429147185,
      "grad_norm": 0.515625,
      "learning_rate": 7.454408521168433e-05,
      "loss": 3.5997,
      "step": 23760
    },
    {
      "epoch": 3.832016766080929,
      "grad_norm": 70.5,
      "learning_rate": 7.453332616063263e-05,
      "loss": 2.285,
      "step": 23770
    },
    {
      "epoch": 3.8336288892471386,
      "grad_norm": 55.25,
      "learning_rate": 7.452256710958094e-05,
      "loss": 1.0172,
      "step": 23780
    },
    {
      "epoch": 3.8352410124133485,
      "grad_norm": 30.125,
      "learning_rate": 7.451180805852925e-05,
      "loss": 1.4826,
      "step": 23790
    },
    {
      "epoch": 3.8368531355795583,
      "grad_norm": 29.25,
      "learning_rate": 7.450104900747754e-05,
      "loss": 1.8964,
      "step": 23800
    },
    {
      "epoch": 3.838465258745768,
      "grad_norm": 68.0,
      "learning_rate": 7.449028995642585e-05,
      "loss": 1.5542,
      "step": 23810
    },
    {
      "epoch": 3.840077381911978,
      "grad_norm": 73.0,
      "learning_rate": 7.447953090537415e-05,
      "loss": 2.4568,
      "step": 23820
    },
    {
      "epoch": 3.841689505078188,
      "grad_norm": 121.5,
      "learning_rate": 7.446877185432245e-05,
      "loss": 2.7283,
      "step": 23830
    },
    {
      "epoch": 3.843301628244398,
      "grad_norm": 49.0,
      "learning_rate": 7.445801280327076e-05,
      "loss": 2.7744,
      "step": 23840
    },
    {
      "epoch": 3.8449137514106075,
      "grad_norm": 0.0,
      "learning_rate": 7.444725375221905e-05,
      "loss": 1.7417,
      "step": 23850
    },
    {
      "epoch": 3.846525874576818,
      "grad_norm": 28.625,
      "learning_rate": 7.443649470116736e-05,
      "loss": 3.2271,
      "step": 23860
    },
    {
      "epoch": 3.8481379977430277,
      "grad_norm": 0.0,
      "learning_rate": 7.442573565011567e-05,
      "loss": 1.9933,
      "step": 23870
    },
    {
      "epoch": 3.8497501209092375,
      "grad_norm": 7.96875,
      "learning_rate": 7.441497659906397e-05,
      "loss": 1.3284,
      "step": 23880
    },
    {
      "epoch": 3.8513622440754474,
      "grad_norm": 29.0,
      "learning_rate": 7.440421754801227e-05,
      "loss": 2.0585,
      "step": 23890
    },
    {
      "epoch": 3.852974367241657,
      "grad_norm": 58.75,
      "learning_rate": 7.439345849696056e-05,
      "loss": 2.6964,
      "step": 23900
    },
    {
      "epoch": 3.854586490407867,
      "grad_norm": 38.0,
      "learning_rate": 7.438269944590887e-05,
      "loss": 1.8927,
      "step": 23910
    },
    {
      "epoch": 3.856198613574077,
      "grad_norm": 42.75,
      "learning_rate": 7.437194039485717e-05,
      "loss": 2.299,
      "step": 23920
    },
    {
      "epoch": 3.857810736740287,
      "grad_norm": 1.7578125,
      "learning_rate": 7.436118134380548e-05,
      "loss": 0.7456,
      "step": 23930
    },
    {
      "epoch": 3.8594228599064966,
      "grad_norm": 119.5,
      "learning_rate": 7.435042229275379e-05,
      "loss": 2.5389,
      "step": 23940
    },
    {
      "epoch": 3.861034983072707,
      "grad_norm": 99.5,
      "learning_rate": 7.433966324170209e-05,
      "loss": 2.8672,
      "step": 23950
    },
    {
      "epoch": 3.8626471062389167,
      "grad_norm": 0.1669921875,
      "learning_rate": 7.432890419065038e-05,
      "loss": 2.2588,
      "step": 23960
    },
    {
      "epoch": 3.8642592294051266,
      "grad_norm": 106.0,
      "learning_rate": 7.431814513959869e-05,
      "loss": 1.6638,
      "step": 23970
    },
    {
      "epoch": 3.8658713525713364,
      "grad_norm": 24.125,
      "learning_rate": 7.430738608854699e-05,
      "loss": 2.5775,
      "step": 23980
    },
    {
      "epoch": 3.8674834757375463,
      "grad_norm": 36.0,
      "learning_rate": 7.429662703749529e-05,
      "loss": 3.5562,
      "step": 23990
    },
    {
      "epoch": 3.869095598903756,
      "grad_norm": 0.0,
      "learning_rate": 7.428586798644361e-05,
      "loss": 3.1499,
      "step": 24000
    },
    {
      "epoch": 3.870707722069966,
      "grad_norm": 101.5,
      "learning_rate": 7.42751089353919e-05,
      "loss": 1.3991,
      "step": 24010
    },
    {
      "epoch": 3.8723198452361762,
      "grad_norm": 0.0,
      "learning_rate": 7.42643498843402e-05,
      "loss": 1.8368,
      "step": 24020
    },
    {
      "epoch": 3.873931968402386,
      "grad_norm": 20.375,
      "learning_rate": 7.425359083328851e-05,
      "loss": 1.066,
      "step": 24030
    },
    {
      "epoch": 3.875544091568596,
      "grad_norm": 17.75,
      "learning_rate": 7.424283178223681e-05,
      "loss": 2.0994,
      "step": 24040
    },
    {
      "epoch": 3.8771562147348058,
      "grad_norm": 55.0,
      "learning_rate": 7.42320727311851e-05,
      "loss": 2.3819,
      "step": 24050
    },
    {
      "epoch": 3.8787683379010156,
      "grad_norm": 54.5,
      "learning_rate": 7.422131368013342e-05,
      "loss": 2.4368,
      "step": 24060
    },
    {
      "epoch": 3.8803804610672255,
      "grad_norm": 16.625,
      "learning_rate": 7.421055462908173e-05,
      "loss": 3.1705,
      "step": 24070
    },
    {
      "epoch": 3.8819925842334353,
      "grad_norm": 110.0,
      "learning_rate": 7.419979557803002e-05,
      "loss": 1.2346,
      "step": 24080
    },
    {
      "epoch": 3.8836047073996456,
      "grad_norm": 17.625,
      "learning_rate": 7.418903652697833e-05,
      "loss": 1.7791,
      "step": 24090
    },
    {
      "epoch": 3.885216830565855,
      "grad_norm": 9.0625,
      "learning_rate": 7.417827747592663e-05,
      "loss": 1.7622,
      "step": 24100
    },
    {
      "epoch": 3.8868289537320653,
      "grad_norm": 54.75,
      "learning_rate": 7.416751842487492e-05,
      "loss": 0.8892,
      "step": 24110
    },
    {
      "epoch": 3.888441076898275,
      "grad_norm": 45.0,
      "learning_rate": 7.415675937382323e-05,
      "loss": 1.4952,
      "step": 24120
    },
    {
      "epoch": 3.890053200064485,
      "grad_norm": 330.0,
      "learning_rate": 7.414600032277153e-05,
      "loss": 2.4702,
      "step": 24130
    },
    {
      "epoch": 3.891665323230695,
      "grad_norm": 50.25,
      "learning_rate": 7.413524127171984e-05,
      "loss": 1.3848,
      "step": 24140
    },
    {
      "epoch": 3.8932774463969047,
      "grad_norm": 57.5,
      "learning_rate": 7.412448222066815e-05,
      "loss": 2.3523,
      "step": 24150
    },
    {
      "epoch": 3.8948895695631145,
      "grad_norm": 23.625,
      "learning_rate": 7.411372316961645e-05,
      "loss": 1.5028,
      "step": 24160
    },
    {
      "epoch": 3.8965016927293243,
      "grad_norm": 174.0,
      "learning_rate": 7.410296411856474e-05,
      "loss": 2.265,
      "step": 24170
    },
    {
      "epoch": 3.8981138158955346,
      "grad_norm": 50.5,
      "learning_rate": 7.409220506751305e-05,
      "loss": 1.2815,
      "step": 24180
    },
    {
      "epoch": 3.8997259390617445,
      "grad_norm": 55.25,
      "learning_rate": 7.408144601646135e-05,
      "loss": 0.4349,
      "step": 24190
    },
    {
      "epoch": 3.9013380622279543,
      "grad_norm": 0.1591796875,
      "learning_rate": 7.407068696540965e-05,
      "loss": 1.6384,
      "step": 24200
    },
    {
      "epoch": 3.902950185394164,
      "grad_norm": 1.09375,
      "learning_rate": 7.405992791435796e-05,
      "loss": 1.6709,
      "step": 24210
    },
    {
      "epoch": 3.904562308560374,
      "grad_norm": 0.056396484375,
      "learning_rate": 7.404916886330627e-05,
      "loss": 1.7895,
      "step": 24220
    },
    {
      "epoch": 3.906174431726584,
      "grad_norm": 53.0,
      "learning_rate": 7.403840981225456e-05,
      "loss": 2.1341,
      "step": 24230
    },
    {
      "epoch": 3.9077865548927937,
      "grad_norm": 2.796875,
      "learning_rate": 7.402765076120286e-05,
      "loss": 2.4945,
      "step": 24240
    },
    {
      "epoch": 3.909398678059004,
      "grad_norm": 41.75,
      "learning_rate": 7.401689171015117e-05,
      "loss": 1.3659,
      "step": 24250
    },
    {
      "epoch": 3.9110108012252134,
      "grad_norm": 84.5,
      "learning_rate": 7.400613265909947e-05,
      "loss": 3.7994,
      "step": 24260
    },
    {
      "epoch": 3.9126229243914237,
      "grad_norm": 45.75,
      "learning_rate": 7.399537360804776e-05,
      "loss": 2.766,
      "step": 24270
    },
    {
      "epoch": 3.9142350475576335,
      "grad_norm": 106.0,
      "learning_rate": 7.398461455699607e-05,
      "loss": 3.3849,
      "step": 24280
    },
    {
      "epoch": 3.9158471707238434,
      "grad_norm": 42.25,
      "learning_rate": 7.397385550594438e-05,
      "loss": 1.6078,
      "step": 24290
    },
    {
      "epoch": 3.917459293890053,
      "grad_norm": 42.5,
      "learning_rate": 7.396309645489268e-05,
      "loss": 2.2259,
      "step": 24300
    },
    {
      "epoch": 3.919071417056263,
      "grad_norm": 18.625,
      "learning_rate": 7.395233740384099e-05,
      "loss": 1.9822,
      "step": 24310
    },
    {
      "epoch": 3.920683540222473,
      "grad_norm": 38.0,
      "learning_rate": 7.394157835278929e-05,
      "loss": 1.7798,
      "step": 24320
    },
    {
      "epoch": 3.9222956633886827,
      "grad_norm": 55.75,
      "learning_rate": 7.393081930173758e-05,
      "loss": 2.0656,
      "step": 24330
    },
    {
      "epoch": 3.923907786554893,
      "grad_norm": 0.318359375,
      "learning_rate": 7.39200602506859e-05,
      "loss": 1.7544,
      "step": 24340
    },
    {
      "epoch": 3.9255199097211024,
      "grad_norm": 52.25,
      "learning_rate": 7.390930119963419e-05,
      "loss": 4.2853,
      "step": 24350
    },
    {
      "epoch": 3.9271320328873127,
      "grad_norm": 0.494140625,
      "learning_rate": 7.38985421485825e-05,
      "loss": 2.9581,
      "step": 24360
    },
    {
      "epoch": 3.9287441560535226,
      "grad_norm": 18.25,
      "learning_rate": 7.388778309753081e-05,
      "loss": 1.3367,
      "step": 24370
    },
    {
      "epoch": 3.9303562792197324,
      "grad_norm": 0.0301513671875,
      "learning_rate": 7.38770240464791e-05,
      "loss": 2.0752,
      "step": 24380
    },
    {
      "epoch": 3.9319684023859423,
      "grad_norm": 11.0,
      "learning_rate": 7.38662649954274e-05,
      "loss": 0.931,
      "step": 24390
    },
    {
      "epoch": 3.933580525552152,
      "grad_norm": 0.3046875,
      "learning_rate": 7.385550594437571e-05,
      "loss": 0.9576,
      "step": 24400
    },
    {
      "epoch": 3.935192648718362,
      "grad_norm": 60.25,
      "learning_rate": 7.384474689332401e-05,
      "loss": 2.9229,
      "step": 24410
    },
    {
      "epoch": 3.936804771884572,
      "grad_norm": 38.75,
      "learning_rate": 7.38339878422723e-05,
      "loss": 2.7717,
      "step": 24420
    },
    {
      "epoch": 3.938416895050782,
      "grad_norm": 151.0,
      "learning_rate": 7.382322879122062e-05,
      "loss": 3.2217,
      "step": 24430
    },
    {
      "epoch": 3.940029018216992,
      "grad_norm": 23.625,
      "learning_rate": 7.381246974016893e-05,
      "loss": 2.3542,
      "step": 24440
    },
    {
      "epoch": 3.9416411413832018,
      "grad_norm": 33.5,
      "learning_rate": 7.380171068911722e-05,
      "loss": 2.1253,
      "step": 24450
    },
    {
      "epoch": 3.9432532645494116,
      "grad_norm": 54.0,
      "learning_rate": 7.379095163806553e-05,
      "loss": 1.3519,
      "step": 24460
    },
    {
      "epoch": 3.9448653877156215,
      "grad_norm": 15.875,
      "learning_rate": 7.378019258701383e-05,
      "loss": 3.0089,
      "step": 24470
    },
    {
      "epoch": 3.9464775108818313,
      "grad_norm": 52.25,
      "learning_rate": 7.376943353596213e-05,
      "loss": 2.1568,
      "step": 24480
    },
    {
      "epoch": 3.948089634048041,
      "grad_norm": 46.25,
      "learning_rate": 7.375867448491044e-05,
      "loss": 1.7222,
      "step": 24490
    },
    {
      "epoch": 3.9497017572142514,
      "grad_norm": 44.75,
      "learning_rate": 7.374791543385873e-05,
      "loss": 1.6792,
      "step": 24500
    },
    {
      "epoch": 3.951313880380461,
      "grad_norm": 4.09375,
      "learning_rate": 7.373715638280704e-05,
      "loss": 2.6157,
      "step": 24510
    },
    {
      "epoch": 3.952926003546671,
      "grad_norm": 32.0,
      "learning_rate": 7.372639733175535e-05,
      "loss": 1.4707,
      "step": 24520
    },
    {
      "epoch": 3.954538126712881,
      "grad_norm": 28.625,
      "learning_rate": 7.371563828070365e-05,
      "loss": 1.8827,
      "step": 24530
    },
    {
      "epoch": 3.956150249879091,
      "grad_norm": 0.0,
      "learning_rate": 7.370487922965195e-05,
      "loss": 2.1537,
      "step": 24540
    },
    {
      "epoch": 3.9577623730453007,
      "grad_norm": 12.1875,
      "learning_rate": 7.369412017860024e-05,
      "loss": 0.854,
      "step": 24550
    },
    {
      "epoch": 3.9593744962115105,
      "grad_norm": 40.5,
      "learning_rate": 7.368336112754855e-05,
      "loss": 2.2048,
      "step": 24560
    },
    {
      "epoch": 3.9609866193777203,
      "grad_norm": 32.0,
      "learning_rate": 7.367260207649685e-05,
      "loss": 1.954,
      "step": 24570
    },
    {
      "epoch": 3.96259874254393,
      "grad_norm": 19.5,
      "learning_rate": 7.366184302544516e-05,
      "loss": 2.8648,
      "step": 24580
    },
    {
      "epoch": 3.9642108657101405,
      "grad_norm": 41.5,
      "learning_rate": 7.365108397439347e-05,
      "loss": 2.6384,
      "step": 24590
    },
    {
      "epoch": 3.9658229888763503,
      "grad_norm": 38.75,
      "learning_rate": 7.364032492334177e-05,
      "loss": 1.2574,
      "step": 24600
    },
    {
      "epoch": 3.96743511204256,
      "grad_norm": 86.5,
      "learning_rate": 7.362956587229006e-05,
      "loss": 1.2624,
      "step": 24610
    },
    {
      "epoch": 3.96904723520877,
      "grad_norm": 15.9375,
      "learning_rate": 7.361880682123837e-05,
      "loss": 1.2551,
      "step": 24620
    },
    {
      "epoch": 3.97065935837498,
      "grad_norm": 0.1748046875,
      "learning_rate": 7.360804777018667e-05,
      "loss": 0.9242,
      "step": 24630
    },
    {
      "epoch": 3.9722714815411897,
      "grad_norm": 0.345703125,
      "learning_rate": 7.359728871913496e-05,
      "loss": 2.2888,
      "step": 24640
    },
    {
      "epoch": 3.9738836047073995,
      "grad_norm": 13.4375,
      "learning_rate": 7.358652966808327e-05,
      "loss": 2.0418,
      "step": 24650
    },
    {
      "epoch": 3.9754957278736094,
      "grad_norm": 55.5,
      "learning_rate": 7.357577061703158e-05,
      "loss": 1.4124,
      "step": 24660
    },
    {
      "epoch": 3.9771078510398192,
      "grad_norm": 52.0,
      "learning_rate": 7.356501156597988e-05,
      "loss": 1.7381,
      "step": 24670
    },
    {
      "epoch": 3.9787199742060295,
      "grad_norm": 0.48046875,
      "learning_rate": 7.355425251492819e-05,
      "loss": 0.6358,
      "step": 24680
    },
    {
      "epoch": 3.9803320973722394,
      "grad_norm": 28.0,
      "learning_rate": 7.354349346387649e-05,
      "loss": 1.5249,
      "step": 24690
    },
    {
      "epoch": 3.981944220538449,
      "grad_norm": 41.0,
      "learning_rate": 7.353273441282478e-05,
      "loss": 1.6734,
      "step": 24700
    },
    {
      "epoch": 3.983556343704659,
      "grad_norm": 1.15625,
      "learning_rate": 7.35219753617731e-05,
      "loss": 3.2181,
      "step": 24710
    },
    {
      "epoch": 3.985168466870869,
      "grad_norm": 17.75,
      "learning_rate": 7.35112163107214e-05,
      "loss": 2.179,
      "step": 24720
    },
    {
      "epoch": 3.9867805900370787,
      "grad_norm": 32.25,
      "learning_rate": 7.35004572596697e-05,
      "loss": 2.0637,
      "step": 24730
    },
    {
      "epoch": 3.9883927132032886,
      "grad_norm": 83.0,
      "learning_rate": 7.348969820861801e-05,
      "loss": 2.2965,
      "step": 24740
    },
    {
      "epoch": 3.990004836369499,
      "grad_norm": 42.0,
      "learning_rate": 7.347893915756631e-05,
      "loss": 1.5299,
      "step": 24750
    },
    {
      "epoch": 3.9916169595357083,
      "grad_norm": 2.96875,
      "learning_rate": 7.34681801065146e-05,
      "loss": 3.4036,
      "step": 24760
    },
    {
      "epoch": 3.9932290827019186,
      "grad_norm": 95.0,
      "learning_rate": 7.345742105546291e-05,
      "loss": 2.4857,
      "step": 24770
    },
    {
      "epoch": 3.9948412058681284,
      "grad_norm": 0.0,
      "learning_rate": 7.344666200441121e-05,
      "loss": 2.7778,
      "step": 24780
    },
    {
      "epoch": 3.9964533290343383,
      "grad_norm": 45.0,
      "learning_rate": 7.343590295335952e-05,
      "loss": 1.6273,
      "step": 24790
    },
    {
      "epoch": 3.998065452200548,
      "grad_norm": 39.75,
      "learning_rate": 7.342514390230783e-05,
      "loss": 2.7951,
      "step": 24800
    },
    {
      "epoch": 3.999677575366758,
      "grad_norm": 7.625,
      "learning_rate": 7.341438485125613e-05,
      "loss": 0.7246,
      "step": 24810
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.984109401702881,
      "eval_runtime": 24.0104,
      "eval_samples_per_second": 4.165,
      "eval_steps_per_second": 4.165,
      "step": 24812
    }
  ],
  "logging_steps": 10,
  "max_steps": 93045,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.078991092912896e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
