model	experiment	trained_samples	batch_size	learning_rate	epochs	evaluated_samples	accuracy	avg_similarity	total_time	avg_time_per_sample	training_time
meta-llama/Llama-3.1-8B-Instruct	baseline	0	0	0.0	0	1133	39.1	70.69	984.7597	0.8692	0.0
meta-llama/Llama-3.1-8B-Instruct	adapter	50	1	0.0001	15	1133	6.71	54.02	1211.7991	1.0695	1649.828
meta-llama/Llama-3.1-8B-Instruct	adapter	50	1	0.0001	15	1133	10.59	55.26	1214.2709	1.0717	1407.395
meta-llama/Llama-3.1-8B-Instruct	adapter	50	1	3e-05	15	1133	7.5	53.98	1214.1928	1.0717	1481.0996
meta-llama/Llama-3.1-8B-Instruct	adapter	50	1	0.001	15	1133	0.0	10.87	1226.6669	1.0827	1469.8407
meta-llama/Llama-3.1-8B-Instruct	adapter	50	2	0.0001	15	1133	8.12	54.56	1218.6615	1.0756	1630.9404
meta-llama/Llama-3.1-8B-Instruct	adapter	50	2	3e-05	15	1133	4.06	52.82	901.1227	0.7953	1496.256
meta-llama/Llama-3.1-8B-Instruct	adapter	50	2	0.001	15	1133	0.0	0.0	268.1648	0.2367	1532.3025
meta-llama/Llama-3.1-8B-Instruct	adapter	50	4	0.0001	15	1133	10.06	55.14	1213.5494	1.0711	1935.7656
meta-llama/Llama-3.1-8B-Instruct	adapter	50	4	3e-05	15	1133	1.15	50.62	945.6609	0.8347	1701.5548
meta-llama/Llama-3.1-8B-Instruct	adapter	50	4	0.001	15	1133	0.0	0.0	267.8441	0.2364	1691.0619
meta-llama/Llama-3.1-8B-Instruct	adapter	200	1	0.0001	15	1133	16.95	58.5	1218.2333	1.0752	3234.6817
meta-llama/Llama-3.1-8B-Instruct	adapter	200	1	3e-05	15	1133	16.33	58.28	1217.5479	1.0746	3414.5004
meta-llama/Llama-3.1-8B-Instruct	adapter	200	1	0.001	15	1133	0.0	10.87	1219.8811	1.0767	3319.0864
meta-llama/Llama-3.1-8B-Instruct	adapter	200	2	0.0001	15	1133	12.89	57.31	1215.0898	1.0725	3763.2295
meta-llama/Llama-3.1-8B-Instruct	adapter	200	2	3e-05	15	1133	10.86	55.32	1071.0124	0.9453	3582.698
meta-llama/Llama-3.1-8B-Instruct	adapter	200	2	0.001	15	1133	0.0	10.87	1204.3093	1.0629	3987.7517
meta-llama/Llama-3.1-8B-Instruct	adapter	200	4	0.0001	15	1133	7.33	54.26	1196.9479	1.0564	4152.7767
meta-llama/Llama-3.1-8B-Instruct	adapter	200	4	3e-05	15	1133	9.36	55.07	1004.1639	0.8863	4057.0341
