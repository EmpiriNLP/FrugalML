model	experiment	trained_samples	epochs	batch_size	evaluated_samples	accuracy	avg_similarity	total_time	avg_time_per_sample	training_time
meta-llama/Llama-3.1-8B-Instruct	baseline	0	0	0	22	54.55	75.91	18.9913	0.8632	0.0
meta-llama/Llama-3.1-8B-Instruct	adapter	1	1	1	22	0.0	7.14	23.984	1.0902	25.17301273345948
meta-llama/Llama-3.1-8B-Instruct	adapter	1	5	1	22	0.0	7.14	24.0365	1.0926	29.93606686592102
meta-llama/Llama-3.1-8B-Instruct	adapter	1	15	1	22	0.0	7.14	23.6036	1.0729	31.61394453048706
meta-llama/Llama-3.1-8B-Instruct	adapter	1	1	2	22	0.0	0.0	23.8505	1.0841	30.63619351387024
meta-llama/Llama-3.1-8B-Instruct	adapter	1	5	2	22	0.0	0.91	23.6434	1.0747	27.625210285186768
meta-llama/Llama-3.1-8B-Instruct	adapter	1	15	2	22	0.0	0.09	23.7797	1.0809	32.99980115890503
meta-llama/Llama-3.1-8B-Instruct	adapter	1	1	4	22	0.0	0.82	23.8801	1.0855	26.876181840896606
meta-llama/Llama-3.1-8B-Instruct	adapter	1	5	4	22	0.0	0.05	23.8729	1.0851	28.09623003005981
meta-llama/Llama-3.1-8B-Instruct	adapter	1	15	4	22	0.0	0.0	23.8711	1.0851	33.359073877334595
meta-llama/Llama-3.1-8B-Instruct	adapter	200	1	1	22	0.0	6.64	96.8077	4.4003	174.66782569885254
meta-llama/Llama-3.1-8B-Instruct	adapter	200	5	1	22	18.18	60.68	95.8796	4.3582	783.538400888443
