{
  "best_metric": 5.815000057220459,
  "best_model_checkpoint": "./models/adapter/checkpoint-13000",
  "epoch": 13.0,
  "eval_steps": 500,
  "global_step": 13000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 109.0,
      "learning_rate": 0.0001,
      "loss": 9.9375,
      "step": 10
    },
    {
      "epoch": 0.02,
      "grad_norm": 51.0,
      "learning_rate": 0.0002,
      "loss": 10.175,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 25.125,
      "learning_rate": 0.0003,
      "loss": 8.2063,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 18.0,
      "learning_rate": 0.0004,
      "loss": 8.8055,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 9.125,
      "learning_rate": 0.0005,
      "loss": 8.8156,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 4.84375,
      "learning_rate": 0.0006,
      "loss": 9.0312,
      "step": 60
    },
    {
      "epoch": 0.07,
      "grad_norm": 4.375,
      "learning_rate": 0.0007,
      "loss": 7.4328,
      "step": 70
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.40625,
      "learning_rate": 0.0008,
      "loss": 7.7042,
      "step": 80
    },
    {
      "epoch": 0.09,
      "grad_norm": 2.78125,
      "learning_rate": 0.0009000000000000001,
      "loss": 7.7242,
      "step": 90
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.953125,
      "learning_rate": 0.001,
      "loss": 7.725,
      "step": 100
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.6171875,
      "learning_rate": 0.0009993288590604027,
      "loss": 5.5687,
      "step": 110
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.8828125,
      "learning_rate": 0.0009986577181208055,
      "loss": 6.5984,
      "step": 120
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.140625,
      "learning_rate": 0.0009979865771812082,
      "loss": 6.8063,
      "step": 130
    },
    {
      "epoch": 0.14,
      "grad_norm": 2.140625,
      "learning_rate": 0.000997315436241611,
      "loss": 6.6932,
      "step": 140
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.625,
      "learning_rate": 0.0009966442953020134,
      "loss": 7.6906,
      "step": 150
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0009959731543624161,
      "loss": 6.2859,
      "step": 160
    },
    {
      "epoch": 0.17,
      "grad_norm": 3.109375,
      "learning_rate": 0.0009953020134228188,
      "loss": 5.8578,
      "step": 170
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0009946308724832216,
      "loss": 6.2125,
      "step": 180
    },
    {
      "epoch": 0.19,
      "grad_norm": 1.015625,
      "learning_rate": 0.0009939597315436243,
      "loss": 6.6078,
      "step": 190
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.859375,
      "learning_rate": 0.0009932885906040268,
      "loss": 6.5078,
      "step": 200
    },
    {
      "epoch": 0.21,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0009926174496644295,
      "loss": 5.5344,
      "step": 210
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.703125,
      "learning_rate": 0.0009919463087248322,
      "loss": 7.4422,
      "step": 220
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.5,
      "learning_rate": 0.000991275167785235,
      "loss": 5.6594,
      "step": 230
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.1015625,
      "learning_rate": 0.0009906040268456377,
      "loss": 4.6911,
      "step": 240
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.98828125,
      "learning_rate": 0.0009899328859060402,
      "loss": 7.0687,
      "step": 250
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.88671875,
      "learning_rate": 0.0009892617449664429,
      "loss": 6.4078,
      "step": 260
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0009885906040268456,
      "loss": 5.9562,
      "step": 270
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.734375,
      "learning_rate": 0.0009879194630872483,
      "loss": 6.775,
      "step": 280
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.546875,
      "learning_rate": 0.000987248322147651,
      "loss": 6.5156,
      "step": 290
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0009865771812080538,
      "loss": 6.65,
      "step": 300
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.46875,
      "learning_rate": 0.0009859060402684565,
      "loss": 7.7062,
      "step": 310
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.578125,
      "learning_rate": 0.0009852348993288592,
      "loss": 7.2625,
      "step": 320
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.40625,
      "learning_rate": 0.0009845637583892617,
      "loss": 7.2969,
      "step": 330
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.83984375,
      "learning_rate": 0.0009838926174496644,
      "loss": 5.1484,
      "step": 340
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.125,
      "learning_rate": 0.0009832214765100671,
      "loss": 7.0,
      "step": 350
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.83203125,
      "learning_rate": 0.0009825503355704699,
      "loss": 6.2453,
      "step": 360
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0009818791946308726,
      "loss": 6.1297,
      "step": 370
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0009812080536912753,
      "loss": 5.6406,
      "step": 380
    },
    {
      "epoch": 0.39,
      "grad_norm": 1.1171875,
      "learning_rate": 0.0009805369127516778,
      "loss": 6.5859,
      "step": 390
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.93359375,
      "learning_rate": 0.0009798657718120805,
      "loss": 5.9072,
      "step": 400
    },
    {
      "epoch": 0.41,
      "grad_norm": 1.28125,
      "learning_rate": 0.0009791946308724832,
      "loss": 5.5625,
      "step": 410
    },
    {
      "epoch": 0.42,
      "grad_norm": 2.296875,
      "learning_rate": 0.000978523489932886,
      "loss": 7.1516,
      "step": 420
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.75,
      "learning_rate": 0.0009778523489932887,
      "loss": 8.3219,
      "step": 430
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.87109375,
      "learning_rate": 0.0009771812080536912,
      "loss": 6.0339,
      "step": 440
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.0,
      "learning_rate": 0.000976510067114094,
      "loss": 6.7736,
      "step": 450
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.82421875,
      "learning_rate": 0.0009758389261744966,
      "loss": 5.0984,
      "step": 460
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.484375,
      "learning_rate": 0.0009751677852348993,
      "loss": 7.483,
      "step": 470
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.91015625,
      "learning_rate": 0.000974496644295302,
      "loss": 6.6437,
      "step": 480
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.96875,
      "learning_rate": 0.0009738255033557048,
      "loss": 7.5406,
      "step": 490
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.6796875,
      "learning_rate": 0.0009731543624161075,
      "loss": 6.1297,
      "step": 500
    },
    {
      "epoch": 0.51,
      "grad_norm": 2.640625,
      "learning_rate": 0.0009724832214765101,
      "loss": 7.1312,
      "step": 510
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.1484375,
      "learning_rate": 0.0009718120805369127,
      "loss": 6.4281,
      "step": 520
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.3046875,
      "learning_rate": 0.0009711409395973154,
      "loss": 6.6781,
      "step": 530
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.125,
      "learning_rate": 0.0009704697986577181,
      "loss": 7.1453,
      "step": 540
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.0,
      "learning_rate": 0.0009697986577181209,
      "loss": 6.1789,
      "step": 550
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.1015625,
      "learning_rate": 0.0009691275167785235,
      "loss": 7.4203,
      "step": 560
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.9609375,
      "learning_rate": 0.0009684563758389262,
      "loss": 6.6437,
      "step": 570
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0,
      "learning_rate": 0.0009677852348993289,
      "loss": 8.0609,
      "step": 580
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.84765625,
      "learning_rate": 0.0009671140939597316,
      "loss": 7.0625,
      "step": 590
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0009664429530201342,
      "loss": 6.3688,
      "step": 600
    },
    {
      "epoch": 0.61,
      "grad_norm": 2.28125,
      "learning_rate": 0.0009657718120805369,
      "loss": 6.2781,
      "step": 610
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.6875,
      "learning_rate": 0.0009651006711409396,
      "loss": 5.6312,
      "step": 620
    },
    {
      "epoch": 0.63,
      "grad_norm": 2.15625,
      "learning_rate": 0.0009644295302013423,
      "loss": 5.9969,
      "step": 630
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.3125,
      "learning_rate": 0.000963758389261745,
      "loss": 6.4802,
      "step": 640
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.8125,
      "learning_rate": 0.0009630872483221476,
      "loss": 5.0828,
      "step": 650
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.0,
      "learning_rate": 0.0009624161073825503,
      "loss": 5.952,
      "step": 660
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.76171875,
      "learning_rate": 0.0009617449664429531,
      "loss": 5.1437,
      "step": 670
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.3125,
      "learning_rate": 0.0009610738255033558,
      "loss": 6.3453,
      "step": 680
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0009604026845637585,
      "loss": 7.0047,
      "step": 690
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.4765625,
      "learning_rate": 0.000959731543624161,
      "loss": 7.3078,
      "step": 700
    },
    {
      "epoch": 0.71,
      "grad_norm": 2.109375,
      "learning_rate": 0.0009590604026845637,
      "loss": 4.6031,
      "step": 710
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0009583892617449664,
      "loss": 6.8812,
      "step": 720
    },
    {
      "epoch": 0.73,
      "grad_norm": 2.015625,
      "learning_rate": 0.0009577181208053692,
      "loss": 5.7562,
      "step": 730
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.8125,
      "learning_rate": 0.0009570469798657719,
      "loss": 4.8109,
      "step": 740
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.671875,
      "learning_rate": 0.0009563758389261745,
      "loss": 6.7438,
      "step": 750
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.76953125,
      "learning_rate": 0.0009557046979865772,
      "loss": 5.8359,
      "step": 760
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.75390625,
      "learning_rate": 0.0009550335570469799,
      "loss": 6.0406,
      "step": 770
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0009543624161073826,
      "loss": 6.2094,
      "step": 780
    },
    {
      "epoch": 0.79,
      "grad_norm": 2.21875,
      "learning_rate": 0.0009536912751677852,
      "loss": 6.2448,
      "step": 790
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.76953125,
      "learning_rate": 0.0009530201342281879,
      "loss": 5.0891,
      "step": 800
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0009523489932885906,
      "loss": 6.0563,
      "step": 810
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.7734375,
      "learning_rate": 0.0009516778523489933,
      "loss": 6.4719,
      "step": 820
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.1171875,
      "learning_rate": 0.000951006711409396,
      "loss": 6.9203,
      "step": 830
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.03125,
      "learning_rate": 0.0009503355704697986,
      "loss": 6.9281,
      "step": 840
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.9375,
      "learning_rate": 0.0009496644295302014,
      "loss": 4.5672,
      "step": 850
    },
    {
      "epoch": 0.86,
      "grad_norm": 2.15625,
      "learning_rate": 0.0009489932885906041,
      "loss": 7.2219,
      "step": 860
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.359375,
      "learning_rate": 0.0009483221476510068,
      "loss": 7.0656,
      "step": 870
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0009476510067114095,
      "loss": 6.3391,
      "step": 880
    },
    {
      "epoch": 0.89,
      "grad_norm": 1.1015625,
      "learning_rate": 0.000946979865771812,
      "loss": 7.0047,
      "step": 890
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.296875,
      "learning_rate": 0.0009463087248322147,
      "loss": 6.2734,
      "step": 900
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0009456375838926175,
      "loss": 6.15,
      "step": 910
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.203125,
      "learning_rate": 0.0009449664429530202,
      "loss": 7.2521,
      "step": 920
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.7421875,
      "learning_rate": 0.0009442953020134229,
      "loss": 6.7172,
      "step": 930
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0009436241610738255,
      "loss": 6.9172,
      "step": 940
    },
    {
      "epoch": 0.95,
      "grad_norm": 2.015625,
      "learning_rate": 0.0009429530201342282,
      "loss": 7.9531,
      "step": 950
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0009422818791946309,
      "loss": 4.1943,
      "step": 960
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.046875,
      "learning_rate": 0.0009416107382550337,
      "loss": 5.2531,
      "step": 970
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9375,
      "learning_rate": 0.0009409395973154362,
      "loss": 4.7188,
      "step": 980
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0009402684563758389,
      "loss": 5.5687,
      "step": 990
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0009395973154362416,
      "loss": 4.7687,
      "step": 1000
    },
    {
      "epoch": 1.0,
      "eval_loss": 5.95578145980835,
      "eval_runtime": 24.0083,
      "eval_samples_per_second": 4.165,
      "eval_steps_per_second": 4.165,
      "step": 1000
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.65625,
      "learning_rate": 0.0009389261744966443,
      "loss": 6.0344,
      "step": 1010
    },
    {
      "epoch": 1.02,
      "grad_norm": 2.15625,
      "learning_rate": 0.000938255033557047,
      "loss": 7.0687,
      "step": 1020
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.734375,
      "learning_rate": 0.0009375838926174497,
      "loss": 5.8445,
      "step": 1030
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.71875,
      "learning_rate": 0.0009369127516778524,
      "loss": 5.6656,
      "step": 1040
    },
    {
      "epoch": 1.05,
      "grad_norm": 2.140625,
      "learning_rate": 0.0009362416107382551,
      "loss": 6.2729,
      "step": 1050
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.71875,
      "learning_rate": 0.0009355704697986578,
      "loss": 5.6,
      "step": 1060
    },
    {
      "epoch": 1.07,
      "grad_norm": 1.3046875,
      "learning_rate": 0.0009348993288590604,
      "loss": 7.2484,
      "step": 1070
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.74609375,
      "learning_rate": 0.000934228187919463,
      "loss": 6.0563,
      "step": 1080
    },
    {
      "epoch": 1.09,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0009335570469798658,
      "loss": 5.2062,
      "step": 1090
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.96875,
      "learning_rate": 0.0009328859060402685,
      "loss": 5.9422,
      "step": 1100
    },
    {
      "epoch": 1.11,
      "grad_norm": 1.203125,
      "learning_rate": 0.0009322147651006712,
      "loss": 5.5547,
      "step": 1110
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.21875,
      "learning_rate": 0.0009315436241610739,
      "loss": 5.9828,
      "step": 1120
    },
    {
      "epoch": 1.13,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0009308724832214765,
      "loss": 6.7875,
      "step": 1130
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0009302013422818792,
      "loss": 6.2687,
      "step": 1140
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.7265625,
      "learning_rate": 0.000929530201342282,
      "loss": 6.1021,
      "step": 1150
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.78125,
      "learning_rate": 0.0009288590604026846,
      "loss": 5.125,
      "step": 1160
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.0,
      "learning_rate": 0.0009281879194630872,
      "loss": 5.3969,
      "step": 1170
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0009275167785234899,
      "loss": 7.0719,
      "step": 1180
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.71875,
      "learning_rate": 0.0009268456375838926,
      "loss": 5.1766,
      "step": 1190
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0009261744966442953,
      "loss": 5.2247,
      "step": 1200
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0009255033557046981,
      "loss": 5.9469,
      "step": 1210
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0009248322147651007,
      "loss": 6.8625,
      "step": 1220
    },
    {
      "epoch": 1.23,
      "grad_norm": 1.140625,
      "learning_rate": 0.0009241610738255034,
      "loss": 5.2062,
      "step": 1230
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.09375,
      "learning_rate": 0.0009234899328859061,
      "loss": 7.2344,
      "step": 1240
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0009228187919463087,
      "loss": 4.975,
      "step": 1250
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.328125,
      "learning_rate": 0.0009221476510067114,
      "loss": 7.0906,
      "step": 1260
    },
    {
      "epoch": 1.27,
      "grad_norm": 1.0078125,
      "learning_rate": 0.000921476510067114,
      "loss": 6.0469,
      "step": 1270
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0009208053691275168,
      "loss": 4.6191,
      "step": 1280
    },
    {
      "epoch": 1.29,
      "grad_norm": 2.109375,
      "learning_rate": 0.0009201342281879195,
      "loss": 6.2219,
      "step": 1290
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.734375,
      "learning_rate": 0.0009194630872483222,
      "loss": 5.2359,
      "step": 1300
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0009187919463087249,
      "loss": 6.2969,
      "step": 1310
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.25,
      "learning_rate": 0.0009181208053691275,
      "loss": 5.8703,
      "step": 1320
    },
    {
      "epoch": 1.33,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0009174496644295303,
      "loss": 5.4391,
      "step": 1330
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.515625,
      "learning_rate": 0.000916778523489933,
      "loss": 8.6625,
      "step": 1340
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0009161073825503356,
      "loss": 5.4188,
      "step": 1350
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.359375,
      "learning_rate": 0.0009154362416107382,
      "loss": 7.7188,
      "step": 1360
    },
    {
      "epoch": 1.37,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0009147651006711409,
      "loss": 5.8203,
      "step": 1370
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0009140939597315436,
      "loss": 5.7219,
      "step": 1380
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0009134228187919464,
      "loss": 6.1797,
      "step": 1390
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.0,
      "learning_rate": 0.0009127516778523491,
      "loss": 6.1828,
      "step": 1400
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.453125,
      "learning_rate": 0.0009120805369127517,
      "loss": 4.4969,
      "step": 1410
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.0,
      "learning_rate": 0.0009114093959731544,
      "loss": 4.467,
      "step": 1420
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0009107382550335571,
      "loss": 5.4875,
      "step": 1430
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0009100671140939597,
      "loss": 6.7109,
      "step": 1440
    },
    {
      "epoch": 1.45,
      "grad_norm": 2.09375,
      "learning_rate": 0.0009093959731543624,
      "loss": 5.5594,
      "step": 1450
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0009087248322147651,
      "loss": 5.1875,
      "step": 1460
    },
    {
      "epoch": 1.47,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0009080536912751678,
      "loss": 6.0781,
      "step": 1470
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.09375,
      "learning_rate": 0.0009073825503355705,
      "loss": 5.8469,
      "step": 1480
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.94921875,
      "learning_rate": 0.0009067114093959732,
      "loss": 5.2256,
      "step": 1490
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0009060402684563759,
      "loss": 6.6391,
      "step": 1500
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.109375,
      "learning_rate": 0.0009053691275167785,
      "loss": 6.4281,
      "step": 1510
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.6875,
      "learning_rate": 0.0009046979865771813,
      "loss": 5.3328,
      "step": 1520
    },
    {
      "epoch": 1.53,
      "grad_norm": 1.21875,
      "learning_rate": 0.0009040268456375839,
      "loss": 6.3688,
      "step": 1530
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.09375,
      "learning_rate": 0.0009033557046979866,
      "loss": 7.825,
      "step": 1540
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0009026845637583892,
      "loss": 6.8914,
      "step": 1550
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.79296875,
      "learning_rate": 0.0009020134228187919,
      "loss": 5.9109,
      "step": 1560
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 1.3125,
      "learning_rate": 0.0009013422818791946,
      "loss": 6.8563,
      "step": 1570
    },
    {
      "epoch": 1.58,
      "grad_norm": 2.078125,
      "learning_rate": 0.0009006711409395974,
      "loss": 6.2969,
      "step": 1580
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.71875,
      "learning_rate": 0.0009000000000000001,
      "loss": 4.4469,
      "step": 1590
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0008993288590604027,
      "loss": 6.0453,
      "step": 1600
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 1.0,
      "learning_rate": 0.0008986577181208054,
      "loss": 7.2406,
      "step": 1610
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.0,
      "learning_rate": 0.000897986577181208,
      "loss": 5.6409,
      "step": 1620
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.0,
      "learning_rate": 0.0008973154362416107,
      "loss": 7.2823,
      "step": 1630
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.046875,
      "learning_rate": 0.0008966442953020135,
      "loss": 5.8589,
      "step": 1640
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0008959731543624161,
      "loss": 5.5375,
      "step": 1650
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.171875,
      "learning_rate": 0.0008953020134228188,
      "loss": 4.7625,
      "step": 1660
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0008946308724832215,
      "loss": 4.7812,
      "step": 1670
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0008939597315436242,
      "loss": 5.225,
      "step": 1680
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0008932885906040268,
      "loss": 7.4978,
      "step": 1690
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.7890625,
      "learning_rate": 0.0008926174496644296,
      "loss": 5.217,
      "step": 1700
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.75390625,
      "learning_rate": 0.0008919463087248322,
      "loss": 6.2938,
      "step": 1710
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.71875,
      "learning_rate": 0.0008912751677852349,
      "loss": 4.7469,
      "step": 1720
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.75,
      "learning_rate": 0.0008906040268456376,
      "loss": 6.15,
      "step": 1730
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0008899328859060402,
      "loss": 5.8109,
      "step": 1740
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0008892617449664429,
      "loss": 4.6094,
      "step": 1750
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.71875,
      "learning_rate": 0.0008885906040268457,
      "loss": 6.4511,
      "step": 1760
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0008879194630872484,
      "loss": 4.9062,
      "step": 1770
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.5390625,
      "learning_rate": 0.0008872483221476511,
      "loss": 7.7313,
      "step": 1780
    },
    {
      "epoch": 1.79,
      "grad_norm": 2.1875,
      "learning_rate": 0.0008865771812080537,
      "loss": 7.1828,
      "step": 1790
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0008859060402684564,
      "loss": 8.9031,
      "step": 1800
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.4140625,
      "learning_rate": 0.000885234899328859,
      "loss": 5.1641,
      "step": 1810
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.421875,
      "learning_rate": 0.0008845637583892618,
      "loss": 5.3388,
      "step": 1820
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.53125,
      "learning_rate": 0.0008838926174496645,
      "loss": 6.0203,
      "step": 1830
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0008832214765100671,
      "loss": 4.3063,
      "step": 1840
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.703125,
      "learning_rate": 0.0008825503355704698,
      "loss": 5.825,
      "step": 1850
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0008818791946308725,
      "loss": 5.9781,
      "step": 1860
    },
    {
      "epoch": 1.87,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0008812080536912752,
      "loss": 7.2391,
      "step": 1870
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0008805369127516779,
      "loss": 6.4875,
      "step": 1880
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0008798657718120806,
      "loss": 4.6078,
      "step": 1890
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0008791946308724832,
      "loss": 8.4453,
      "step": 1900
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 2.015625,
      "learning_rate": 0.0008785234899328859,
      "loss": 6.5149,
      "step": 1910
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0008778523489932886,
      "loss": 6.6672,
      "step": 1920
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0008771812080536912,
      "loss": 6.4062,
      "step": 1930
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.703125,
      "learning_rate": 0.000876510067114094,
      "loss": 6.1625,
      "step": 1940
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.28125,
      "learning_rate": 0.0008758389261744967,
      "loss": 6.2281,
      "step": 1950
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0008751677852348994,
      "loss": 5.5656,
      "step": 1960
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0008744966442953021,
      "loss": 5.2281,
      "step": 1970
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.9296875,
      "learning_rate": 0.0008738255033557047,
      "loss": 5.7297,
      "step": 1980
    },
    {
      "epoch": 1.99,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0008731543624161073,
      "loss": 7.2464,
      "step": 1990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3203125,
      "learning_rate": 0.00087248322147651,
      "loss": 6.3625,
      "step": 2000
    },
    {
      "epoch": 2.0,
      "eval_loss": 5.913125038146973,
      "eval_runtime": 23.9876,
      "eval_samples_per_second": 4.169,
      "eval_steps_per_second": 4.169,
      "step": 2000
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.0,
      "learning_rate": 0.0008718120805369128,
      "loss": 4.443,
      "step": 2010
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0008711409395973155,
      "loss": 5.0187,
      "step": 2020
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.8984375,
      "learning_rate": 0.0008704697986577181,
      "loss": 5.6484,
      "step": 2030
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0008697986577181208,
      "loss": 3.9859,
      "step": 2040
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0008691275167785235,
      "loss": 5.4977,
      "step": 2050
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0008684563758389263,
      "loss": 6.7266,
      "step": 2060
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0008677852348993289,
      "loss": 4.6109,
      "step": 2070
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.25,
      "learning_rate": 0.0008671140939597315,
      "loss": 6.1469,
      "step": 2080
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0008664429530201342,
      "loss": 5.5406,
      "step": 2090
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.0,
      "learning_rate": 0.0008657718120805369,
      "loss": 5.1969,
      "step": 2100
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.0,
      "learning_rate": 0.0008651006711409396,
      "loss": 6.0259,
      "step": 2110
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0008644295302013422,
      "loss": 6.775,
      "step": 2120
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.98046875,
      "learning_rate": 0.000863758389261745,
      "loss": 6.2609,
      "step": 2130
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008630872483221477,
      "loss": 6.7625,
      "step": 2140
    },
    {
      "epoch": 2.15,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0008624161073825504,
      "loss": 7.4719,
      "step": 2150
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.46875,
      "learning_rate": 0.0008617449664429531,
      "loss": 5.6984,
      "step": 2160
    },
    {
      "epoch": 2.17,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0008610738255033556,
      "loss": 5.6672,
      "step": 2170
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.15625,
      "learning_rate": 0.0008604026845637583,
      "loss": 6.4594,
      "step": 2180
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9296875,
      "learning_rate": 0.0008597315436241611,
      "loss": 7.1375,
      "step": 2190
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0008590604026845638,
      "loss": 6.0859,
      "step": 2200
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.03125,
      "learning_rate": 0.0008583892617449665,
      "loss": 5.2078,
      "step": 2210
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0008577181208053691,
      "loss": 7.3359,
      "step": 2220
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.7421875,
      "learning_rate": 0.0008570469798657718,
      "loss": 6.1375,
      "step": 2230
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.6875,
      "learning_rate": 0.0008563758389261746,
      "loss": 6.1273,
      "step": 2240
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.296875,
      "learning_rate": 0.0008557046979865773,
      "loss": 4.7984,
      "step": 2250
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0008550335570469799,
      "loss": 6.9328,
      "step": 2260
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0008543624161073825,
      "loss": 6.4523,
      "step": 2270
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0008536912751677852,
      "loss": 5.0875,
      "step": 2280
    },
    {
      "epoch": 2.29,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0008530201342281879,
      "loss": 5.6516,
      "step": 2290
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.09375,
      "learning_rate": 0.0008523489932885907,
      "loss": 7.05,
      "step": 2300
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.6875,
      "learning_rate": 0.0008516778523489933,
      "loss": 4.6125,
      "step": 2310
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.2109375,
      "learning_rate": 0.000851006711409396,
      "loss": 4.5844,
      "step": 2320
    },
    {
      "epoch": 2.33,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0008503355704697987,
      "loss": 6.7969,
      "step": 2330
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.03125,
      "learning_rate": 0.0008496644295302014,
      "loss": 6.1953,
      "step": 2340
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.75,
      "learning_rate": 0.0008489932885906041,
      "loss": 6.1719,
      "step": 2350
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.64453125,
      "learning_rate": 0.0008483221476510066,
      "loss": 4.3937,
      "step": 2360
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.34375,
      "learning_rate": 0.0008476510067114094,
      "loss": 7.3203,
      "step": 2370
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.86328125,
      "learning_rate": 0.0008469798657718121,
      "loss": 6.7687,
      "step": 2380
    },
    {
      "epoch": 2.39,
      "grad_norm": 2.03125,
      "learning_rate": 0.0008463087248322148,
      "loss": 6.4469,
      "step": 2390
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.6875,
      "learning_rate": 0.0008456375838926175,
      "loss": 6.3906,
      "step": 2400
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0008449664429530201,
      "loss": 5.025,
      "step": 2410
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008442953020134228,
      "loss": 4.5375,
      "step": 2420
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.703125,
      "learning_rate": 0.0008436241610738256,
      "loss": 4.5547,
      "step": 2430
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.1484375,
      "learning_rate": 0.0008429530201342283,
      "loss": 6.4207,
      "step": 2440
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0008422818791946308,
      "loss": 5.9656,
      "step": 2450
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0008416107382550335,
      "loss": 6.3406,
      "step": 2460
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 2.0625,
      "learning_rate": 0.0008409395973154362,
      "loss": 5.6047,
      "step": 2470
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.703125,
      "learning_rate": 0.000840268456375839,
      "loss": 6.425,
      "step": 2480
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.40625,
      "learning_rate": 0.0008395973154362417,
      "loss": 6.5922,
      "step": 2490
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.28125,
      "learning_rate": 0.0008389261744966443,
      "loss": 6.6578,
      "step": 2500
    },
    {
      "epoch": 2.51,
      "grad_norm": 2.03125,
      "learning_rate": 0.000838255033557047,
      "loss": 4.8953,
      "step": 2510
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.71875,
      "learning_rate": 0.0008375838926174497,
      "loss": 6.3031,
      "step": 2520
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0008369127516778524,
      "loss": 6.6734,
      "step": 2530
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.7109375,
      "learning_rate": 0.000836241610738255,
      "loss": 5.2109,
      "step": 2540
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.0,
      "learning_rate": 0.0008355704697986577,
      "loss": 4.4344,
      "step": 2550
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.6875,
      "learning_rate": 0.0008348993288590604,
      "loss": 5.5234,
      "step": 2560
    },
    {
      "epoch": 2.57,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0008342281879194631,
      "loss": 5.5219,
      "step": 2570
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008335570469798658,
      "loss": 5.0187,
      "step": 2580
    },
    {
      "epoch": 2.59,
      "grad_norm": 2.0625,
      "learning_rate": 0.0008328859060402685,
      "loss": 6.4042,
      "step": 2590
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0008322147651006711,
      "loss": 4.0413,
      "step": 2600
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008315436241610739,
      "loss": 6.5687,
      "step": 2610
    },
    {
      "epoch": 2.62,
      "grad_norm": 2.046875,
      "learning_rate": 0.0008308724832214766,
      "loss": 6.3219,
      "step": 2620
    },
    {
      "epoch": 2.63,
      "grad_norm": 1.0,
      "learning_rate": 0.0008302013422818792,
      "loss": 6.5,
      "step": 2630
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0008295302013422818,
      "loss": 6.575,
      "step": 2640
    },
    {
      "epoch": 2.65,
      "grad_norm": 2.015625,
      "learning_rate": 0.0008288590604026845,
      "loss": 6.3922,
      "step": 2650
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.046875,
      "learning_rate": 0.0008281879194630872,
      "loss": 6.3438,
      "step": 2660
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.72265625,
      "learning_rate": 0.00082751677852349,
      "loss": 5.2062,
      "step": 2670
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0008268456375838927,
      "loss": 4.6437,
      "step": 2680
    },
    {
      "epoch": 2.69,
      "grad_norm": 2.078125,
      "learning_rate": 0.0008261744966442953,
      "loss": 7.6156,
      "step": 2690
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.078125,
      "learning_rate": 0.000825503355704698,
      "loss": 5.925,
      "step": 2700
    },
    {
      "epoch": 2.71,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0008248322147651007,
      "loss": 4.623,
      "step": 2710
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.0,
      "learning_rate": 0.0008241610738255034,
      "loss": 6.3531,
      "step": 2720
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0008234899328859061,
      "loss": 6.5078,
      "step": 2730
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0008228187919463087,
      "loss": 6.3031,
      "step": 2740
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0008221476510067114,
      "loss": 6.6578,
      "step": 2750
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.265625,
      "learning_rate": 0.0008214765100671141,
      "loss": 7.3125,
      "step": 2760
    },
    {
      "epoch": 2.77,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0008208053691275168,
      "loss": 7.575,
      "step": 2770
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0008201342281879195,
      "loss": 5.4922,
      "step": 2780
    },
    {
      "epoch": 2.79,
      "grad_norm": 1.046875,
      "learning_rate": 0.0008194630872483222,
      "loss": 5.5406,
      "step": 2790
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0008187919463087249,
      "loss": 6.5953,
      "step": 2800
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0008181208053691276,
      "loss": 7.1363,
      "step": 2810
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.296875,
      "learning_rate": 0.0008174496644295302,
      "loss": 6.9844,
      "step": 2820
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0008167785234899328,
      "loss": 6.8578,
      "step": 2830
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0008161073825503355,
      "loss": 4.8266,
      "step": 2840
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0008154362416107383,
      "loss": 5.7297,
      "step": 2850
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.984375,
      "learning_rate": 0.000814765100671141,
      "loss": 7.6203,
      "step": 2860
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0008140939597315437,
      "loss": 6.8359,
      "step": 2870
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0008134228187919463,
      "loss": 5.3666,
      "step": 2880
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.7109375,
      "learning_rate": 0.000812751677852349,
      "loss": 8.0578,
      "step": 2890
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0008120805369127517,
      "loss": 5.8297,
      "step": 2900
    },
    {
      "epoch": 2.91,
      "grad_norm": 1.4375,
      "learning_rate": 0.0008114093959731544,
      "loss": 6.3875,
      "step": 2910
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.71875,
      "learning_rate": 0.0008107382550335571,
      "loss": 5.3641,
      "step": 2920
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0008100671140939597,
      "loss": 5.3891,
      "step": 2930
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.90625,
      "learning_rate": 0.0008093959731543624,
      "loss": 4.5453,
      "step": 2940
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.6875,
      "learning_rate": 0.0008087248322147651,
      "loss": 6.1578,
      "step": 2950
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.3125,
      "learning_rate": 0.0008080536912751678,
      "loss": 5.1109,
      "step": 2960
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 1.21875,
      "learning_rate": 0.0008073825503355706,
      "loss": 5.1422,
      "step": 2970
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008067114093959732,
      "loss": 6.8969,
      "step": 2980
    },
    {
      "epoch": 2.99,
      "grad_norm": 1.390625,
      "learning_rate": 0.0008060402684563759,
      "loss": 5.0531,
      "step": 2990
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0008053691275167785,
      "loss": 5.9313,
      "step": 3000
    },
    {
      "epoch": 3.0,
      "eval_loss": 5.841875076293945,
      "eval_runtime": 23.9895,
      "eval_samples_per_second": 4.168,
      "eval_steps_per_second": 4.168,
      "step": 3000
    },
    {
      "epoch": 3.01,
      "grad_norm": 1.484375,
      "learning_rate": 0.0008046979865771812,
      "loss": 8.375,
      "step": 3010
    },
    {
      "epoch": 3.02,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0008040268456375838,
      "loss": 5.5531,
      "step": 3020
    },
    {
      "epoch": 3.03,
      "grad_norm": 1.03125,
      "learning_rate": 0.0008033557046979866,
      "loss": 5.8625,
      "step": 3030
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0008026845637583893,
      "loss": 5.8719,
      "step": 3040
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.6796875,
      "learning_rate": 0.000802013422818792,
      "loss": 4.6891,
      "step": 3050
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0008013422818791947,
      "loss": 5.5781,
      "step": 3060
    },
    {
      "epoch": 3.07,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0008006711409395973,
      "loss": 5.8922,
      "step": 3070
    },
    {
      "epoch": 3.08,
      "grad_norm": 1.046875,
      "learning_rate": 0.0008,
      "loss": 5.2406,
      "step": 3080
    },
    {
      "epoch": 3.09,
      "grad_norm": 1.5,
      "learning_rate": 0.0007993288590604026,
      "loss": 6.1453,
      "step": 3090
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007986577181208054,
      "loss": 5.1016,
      "step": 3100
    },
    {
      "epoch": 3.11,
      "grad_norm": 1.1015625,
      "learning_rate": 0.0007979865771812081,
      "loss": 4.8312,
      "step": 3110
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.671875,
      "learning_rate": 0.0007973154362416107,
      "loss": 7.1578,
      "step": 3120
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0007966442953020134,
      "loss": 6.8672,
      "step": 3130
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0007959731543624161,
      "loss": 6.4844,
      "step": 3140
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007953020134228189,
      "loss": 5.5938,
      "step": 3150
    },
    {
      "epoch": 3.16,
      "grad_norm": 2.09375,
      "learning_rate": 0.0007946308724832216,
      "loss": 7.4469,
      "step": 3160
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.87109375,
      "learning_rate": 0.0007939597315436242,
      "loss": 6.8219,
      "step": 3170
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0007932885906040269,
      "loss": 7.0141,
      "step": 3180
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0007926174496644295,
      "loss": 5.7188,
      "step": 3190
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.125,
      "learning_rate": 0.0007919463087248322,
      "loss": 5.7266,
      "step": 3200
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0007912751677852348,
      "loss": 5.7609,
      "step": 3210
    },
    {
      "epoch": 3.22,
      "grad_norm": 1.375,
      "learning_rate": 0.0007906040268456376,
      "loss": 6.0438,
      "step": 3220
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0007899328859060403,
      "loss": 4.8609,
      "step": 3230
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.94921875,
      "learning_rate": 0.000789261744966443,
      "loss": 4.6766,
      "step": 3240
    },
    {
      "epoch": 3.25,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0007885906040268457,
      "loss": 7.1641,
      "step": 3250
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0007879194630872483,
      "loss": 4.9328,
      "step": 3260
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.68359375,
      "learning_rate": 0.000787248322147651,
      "loss": 5.3797,
      "step": 3270
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.875,
      "learning_rate": 0.0007865771812080537,
      "loss": 5.5281,
      "step": 3280
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0007859060402684564,
      "loss": 4.7203,
      "step": 3290
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0007852348993288591,
      "loss": 5.8547,
      "step": 3300
    },
    {
      "epoch": 3.31,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007845637583892617,
      "loss": 6.3605,
      "step": 3310
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0007838926174496644,
      "loss": 5.0122,
      "step": 3320
    },
    {
      "epoch": 3.33,
      "grad_norm": 2.015625,
      "learning_rate": 0.0007832214765100672,
      "loss": 6.0266,
      "step": 3330
    },
    {
      "epoch": 3.34,
      "grad_norm": 1.015625,
      "learning_rate": 0.0007825503355704699,
      "loss": 4.6891,
      "step": 3340
    },
    {
      "epoch": 3.35,
      "grad_norm": 1.453125,
      "learning_rate": 0.0007818791946308726,
      "loss": 4.5656,
      "step": 3350
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.421875,
      "learning_rate": 0.0007812080536912752,
      "loss": 5.6531,
      "step": 3360
    },
    {
      "epoch": 3.37,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0007805369127516778,
      "loss": 4.9375,
      "step": 3370
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0007798657718120805,
      "loss": 5.3531,
      "step": 3380
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007791946308724832,
      "loss": 5.0141,
      "step": 3390
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0007785234899328859,
      "loss": 6.3922,
      "step": 3400
    },
    {
      "epoch": 3.41,
      "grad_norm": 1.09375,
      "learning_rate": 0.0007778523489932886,
      "loss": 7.1594,
      "step": 3410
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0007771812080536913,
      "loss": 6.3187,
      "step": 3420
    },
    {
      "epoch": 3.43,
      "grad_norm": 1.3046875,
      "learning_rate": 0.000776510067114094,
      "loss": 7.4313,
      "step": 3430
    },
    {
      "epoch": 3.44,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0007758389261744967,
      "loss": 7.4852,
      "step": 3440
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0007751677852348993,
      "loss": 5.725,
      "step": 3450
    },
    {
      "epoch": 3.46,
      "grad_norm": 1.5390625,
      "learning_rate": 0.000774496644295302,
      "loss": 6.1047,
      "step": 3460
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 1.5234375,
      "learning_rate": 0.0007738255033557047,
      "loss": 6.9047,
      "step": 3470
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0007731543624161074,
      "loss": 7.3609,
      "step": 3480
    },
    {
      "epoch": 3.49,
      "grad_norm": 1.140625,
      "learning_rate": 0.0007724832214765101,
      "loss": 5.4219,
      "step": 3490
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.375,
      "learning_rate": 0.0007718120805369127,
      "loss": 6.0422,
      "step": 3500
    },
    {
      "epoch": 3.51,
      "grad_norm": 1.140625,
      "learning_rate": 0.0007711409395973154,
      "loss": 5.2719,
      "step": 3510
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.3125,
      "learning_rate": 0.0007704697986577182,
      "loss": 6.2109,
      "step": 3520
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 0.859375,
      "learning_rate": 0.0007697986577181209,
      "loss": 5.5938,
      "step": 3530
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0007691275167785236,
      "loss": 4.1453,
      "step": 3540
    },
    {
      "epoch": 3.55,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0007684563758389261,
      "loss": 6.2906,
      "step": 3550
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.125,
      "learning_rate": 0.0007677852348993288,
      "loss": 5.6469,
      "step": 3560
    },
    {
      "epoch": 3.57,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0007671140939597315,
      "loss": 5.6047,
      "step": 3570
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.71875,
      "learning_rate": 0.0007664429530201343,
      "loss": 5.235,
      "step": 3580
    },
    {
      "epoch": 3.59,
      "grad_norm": 1.25,
      "learning_rate": 0.0007657718120805369,
      "loss": 6.4672,
      "step": 3590
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0007651006711409396,
      "loss": 6.6422,
      "step": 3600
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0007644295302013423,
      "loss": 4.225,
      "step": 3610
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.6875,
      "learning_rate": 0.000763758389261745,
      "loss": 6.6703,
      "step": 3620
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0007630872483221478,
      "loss": 4.8906,
      "step": 3630
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.09375,
      "learning_rate": 0.0007624161073825504,
      "loss": 6.4875,
      "step": 3640
    },
    {
      "epoch": 3.65,
      "grad_norm": 1.203125,
      "learning_rate": 0.000761744966442953,
      "loss": 6.4613,
      "step": 3650
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.0,
      "learning_rate": 0.0007610738255033557,
      "loss": 5.7372,
      "step": 3660
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.71875,
      "learning_rate": 0.0007604026845637584,
      "loss": 5.3656,
      "step": 3670
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.96875,
      "learning_rate": 0.0007597315436241611,
      "loss": 5.5687,
      "step": 3680
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0007590604026845637,
      "loss": 6.0516,
      "step": 3690
    },
    {
      "epoch": 3.7,
      "grad_norm": 1.421875,
      "learning_rate": 0.0007583892617449665,
      "loss": 6.0344,
      "step": 3700
    },
    {
      "epoch": 3.71,
      "grad_norm": 1.125,
      "learning_rate": 0.0007577181208053692,
      "loss": 5.3453,
      "step": 3710
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0007570469798657719,
      "loss": 5.825,
      "step": 3720
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0007563758389261746,
      "loss": 5.7328,
      "step": 3730
    },
    {
      "epoch": 3.74,
      "grad_norm": 1.53125,
      "learning_rate": 0.0007557046979865771,
      "loss": 6.0187,
      "step": 3740
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0007550335570469798,
      "loss": 6.1297,
      "step": 3750
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.890625,
      "learning_rate": 0.0007543624161073826,
      "loss": 5.9984,
      "step": 3760
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007536912751677853,
      "loss": 8.3547,
      "step": 3770
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 1.203125,
      "learning_rate": 0.0007530201342281879,
      "loss": 6.2359,
      "step": 3780
    },
    {
      "epoch": 3.79,
      "grad_norm": 2.0625,
      "learning_rate": 0.0007523489932885906,
      "loss": 8.2734,
      "step": 3790
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.9375,
      "learning_rate": 0.0007516778523489933,
      "loss": 6.1234,
      "step": 3800
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.69921875,
      "learning_rate": 0.000751006711409396,
      "loss": 4.9258,
      "step": 3810
    },
    {
      "epoch": 3.82,
      "grad_norm": 1.21875,
      "learning_rate": 0.0007503355704697988,
      "loss": 6.4437,
      "step": 3820
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0007496644295302013,
      "loss": 7.0125,
      "step": 3830
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.70703125,
      "learning_rate": 0.000748993288590604,
      "loss": 4.5187,
      "step": 3840
    },
    {
      "epoch": 3.85,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0007483221476510067,
      "loss": 5.3797,
      "step": 3850
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.62109375,
      "learning_rate": 0.0007476510067114094,
      "loss": 5.7896,
      "step": 3860
    },
    {
      "epoch": 3.87,
      "grad_norm": 2.0,
      "learning_rate": 0.0007469798657718121,
      "loss": 8.5893,
      "step": 3870
    },
    {
      "epoch": 3.88,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0007463087248322148,
      "loss": 6.1156,
      "step": 3880
    },
    {
      "epoch": 3.89,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0007456375838926175,
      "loss": 4.9313,
      "step": 3890
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0007449664429530202,
      "loss": 5.3125,
      "step": 3900
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.6875,
      "learning_rate": 0.0007442953020134229,
      "loss": 6.3125,
      "step": 3910
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.6875,
      "learning_rate": 0.0007436241610738254,
      "loss": 4.7266,
      "step": 3920
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0007429530201342281,
      "loss": 6.3016,
      "step": 3930
    },
    {
      "epoch": 3.94,
      "grad_norm": 1.234375,
      "learning_rate": 0.0007422818791946309,
      "loss": 5.0547,
      "step": 3940
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.91796875,
      "learning_rate": 0.0007416107382550336,
      "loss": 5.7082,
      "step": 3950
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007409395973154363,
      "loss": 4.8438,
      "step": 3960
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 0.96875,
      "learning_rate": 0.0007402684563758389,
      "loss": 7.3266,
      "step": 3970
    },
    {
      "epoch": 3.98,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0007395973154362416,
      "loss": 6.0219,
      "step": 3980
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.76953125,
      "learning_rate": 0.0007389261744966443,
      "loss": 8.1906,
      "step": 3990
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.9140625,
      "learning_rate": 0.0007382550335570471,
      "loss": 5.6394,
      "step": 4000
    },
    {
      "epoch": 4.0,
      "eval_loss": 5.840156078338623,
      "eval_runtime": 23.9862,
      "eval_samples_per_second": 4.169,
      "eval_steps_per_second": 4.169,
      "step": 4000
    },
    {
      "epoch": 4.01,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007375838926174497,
      "loss": 7.075,
      "step": 4010
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.703125,
      "learning_rate": 0.0007369127516778523,
      "loss": 5.6109,
      "step": 4020
    },
    {
      "epoch": 4.03,
      "grad_norm": 2.046875,
      "learning_rate": 0.000736241610738255,
      "loss": 7.7406,
      "step": 4030
    },
    {
      "epoch": 4.04,
      "grad_norm": 2.03125,
      "learning_rate": 0.0007355704697986577,
      "loss": 6.1406,
      "step": 4040
    },
    {
      "epoch": 4.05,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0007348993288590604,
      "loss": 4.975,
      "step": 4050
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0007342281879194632,
      "loss": 5.2453,
      "step": 4060
    },
    {
      "epoch": 4.07,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0007335570469798658,
      "loss": 6.5703,
      "step": 4070
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0007328859060402685,
      "loss": 6.0375,
      "step": 4080
    },
    {
      "epoch": 4.09,
      "grad_norm": 2.03125,
      "learning_rate": 0.0007322147651006712,
      "loss": 5.0594,
      "step": 4090
    },
    {
      "epoch": 4.1,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0007315436241610739,
      "loss": 4.3859,
      "step": 4100
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0007308724832214764,
      "loss": 5.2547,
      "step": 4110
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0007302013422818791,
      "loss": 6.2469,
      "step": 4120
    },
    {
      "epoch": 4.13,
      "grad_norm": 1.015625,
      "learning_rate": 0.0007295302013422819,
      "loss": 5.8297,
      "step": 4130
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.703125,
      "learning_rate": 0.0007288590604026846,
      "loss": 5.1797,
      "step": 4140
    },
    {
      "epoch": 4.15,
      "grad_norm": 1.078125,
      "learning_rate": 0.0007281879194630873,
      "loss": 6.1828,
      "step": 4150
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0007275167785234899,
      "loss": 6.1453,
      "step": 4160
    },
    {
      "epoch": 4.17,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0007268456375838926,
      "loss": 6.0732,
      "step": 4170
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.703125,
      "learning_rate": 0.0007261744966442954,
      "loss": 5.3453,
      "step": 4180
    },
    {
      "epoch": 4.19,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007255033557046981,
      "loss": 6.3219,
      "step": 4190
    },
    {
      "epoch": 4.2,
      "grad_norm": 1.015625,
      "learning_rate": 0.0007248322147651007,
      "loss": 6.0491,
      "step": 4200
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0007241610738255033,
      "loss": 7.0563,
      "step": 4210
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.67578125,
      "learning_rate": 0.000723489932885906,
      "loss": 5.9342,
      "step": 4220
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.8828125,
      "learning_rate": 0.0007228187919463087,
      "loss": 5.4125,
      "step": 4230
    },
    {
      "epoch": 4.24,
      "grad_norm": 1.953125,
      "learning_rate": 0.0007221476510067115,
      "loss": 7.0203,
      "step": 4240
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007214765100671142,
      "loss": 5.325,
      "step": 4250
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0007208053691275168,
      "loss": 6.3141,
      "step": 4260
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.0,
      "learning_rate": 0.0007201342281879195,
      "loss": 5.39,
      "step": 4270
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0007194630872483222,
      "loss": 4.2434,
      "step": 4280
    },
    {
      "epoch": 4.29,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007187919463087248,
      "loss": 4.1328,
      "step": 4290
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0007181208053691274,
      "loss": 5.5172,
      "step": 4300
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0007174496644295302,
      "loss": 7.1672,
      "step": 4310
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0007167785234899329,
      "loss": 5.6469,
      "step": 4320
    },
    {
      "epoch": 4.33,
      "grad_norm": 1.15625,
      "learning_rate": 0.0007161073825503356,
      "loss": 5.2,
      "step": 4330
    },
    {
      "epoch": 4.34,
      "grad_norm": 1.984375,
      "learning_rate": 0.0007154362416107383,
      "loss": 6.0844,
      "step": 4340
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0007147651006711409,
      "loss": 5.1878,
      "step": 4350
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0007140939597315436,
      "loss": 5.9891,
      "step": 4360
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.265625,
      "learning_rate": 0.0007134228187919464,
      "loss": 6.3375,
      "step": 4370
    },
    {
      "epoch": 4.38,
      "grad_norm": 1.2421875,
      "learning_rate": 0.000712751677852349,
      "loss": 5.7953,
      "step": 4380
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0007120805369127517,
      "loss": 5.3891,
      "step": 4390
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.125,
      "learning_rate": 0.0007114093959731543,
      "loss": 5.3552,
      "step": 4400
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.65625,
      "learning_rate": 0.000710738255033557,
      "loss": 4.9719,
      "step": 4410
    },
    {
      "epoch": 4.42,
      "grad_norm": 2.09375,
      "learning_rate": 0.0007100671140939597,
      "loss": 5.4594,
      "step": 4420
    },
    {
      "epoch": 4.43,
      "grad_norm": 1.1875,
      "learning_rate": 0.0007093959731543625,
      "loss": 6.8719,
      "step": 4430
    },
    {
      "epoch": 4.44,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0007087248322147652,
      "loss": 6.9219,
      "step": 4440
    },
    {
      "epoch": 4.45,
      "grad_norm": 1.0625,
      "learning_rate": 0.0007080536912751678,
      "loss": 8.0062,
      "step": 4450
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0007073825503355705,
      "loss": 5.8656,
      "step": 4460
    },
    {
      "epoch": 4.47,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0007067114093959731,
      "loss": 4.7828,
      "step": 4470
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.984375,
      "learning_rate": 0.0007060402684563758,
      "loss": 6.0375,
      "step": 4480
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.91796875,
      "learning_rate": 0.0007053691275167785,
      "loss": 5.0187,
      "step": 4490
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.64453125,
      "learning_rate": 0.0007046979865771812,
      "loss": 6.7,
      "step": 4500
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.6875,
      "learning_rate": 0.0007040268456375839,
      "loss": 5.5484,
      "step": 4510
    },
    {
      "epoch": 4.52,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0007033557046979866,
      "loss": 7.5891,
      "step": 4520
    },
    {
      "epoch": 4.53,
      "grad_norm": 1.90625,
      "learning_rate": 0.0007026845637583893,
      "loss": 5.6328,
      "step": 4530
    },
    {
      "epoch": 4.54,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0007020134228187919,
      "loss": 7.2812,
      "step": 4540
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0007013422818791947,
      "loss": 5.4062,
      "step": 4550
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0007006711409395974,
      "loss": 5.0734,
      "step": 4560
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0007,
      "loss": 5.9688,
      "step": 4570
    },
    {
      "epoch": 4.58,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0006993288590604027,
      "loss": 7.426,
      "step": 4580
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0006986577181208053,
      "loss": 4.3,
      "step": 4590
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.70703125,
      "learning_rate": 0.000697986577181208,
      "loss": 6.3156,
      "step": 4600
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.71875,
      "learning_rate": 0.0006973154362416108,
      "loss": 4.6047,
      "step": 4610
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0006966442953020135,
      "loss": 5.7875,
      "step": 4620
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.6875,
      "learning_rate": 0.0006959731543624162,
      "loss": 5.7703,
      "step": 4630
    },
    {
      "epoch": 4.64,
      "grad_norm": 1.203125,
      "learning_rate": 0.0006953020134228188,
      "loss": 8.3906,
      "step": 4640
    },
    {
      "epoch": 4.65,
      "grad_norm": 1.078125,
      "learning_rate": 0.0006946308724832215,
      "loss": 5.3734,
      "step": 4650
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0006939597315436241,
      "loss": 4.5591,
      "step": 4660
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.9765625,
      "learning_rate": 0.0006932885906040269,
      "loss": 5.5203,
      "step": 4670
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.671875,
      "learning_rate": 0.0006926174496644295,
      "loss": 4.8344,
      "step": 4680
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0006919463087248322,
      "loss": 5.6203,
      "step": 4690
    },
    {
      "epoch": 4.7,
      "grad_norm": 1.03125,
      "learning_rate": 0.0006912751677852349,
      "loss": 4.2875,
      "step": 4700
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.65625,
      "learning_rate": 0.0006906040268456376,
      "loss": 4.1984,
      "step": 4710
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.0,
      "learning_rate": 0.0006899328859060403,
      "loss": 5.5773,
      "step": 4720
    },
    {
      "epoch": 4.73,
      "grad_norm": 1.2265625,
      "learning_rate": 0.000689261744966443,
      "loss": 6.6828,
      "step": 4730
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0006885906040268457,
      "loss": 6.2984,
      "step": 4740
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.96875,
      "learning_rate": 0.0006879194630872483,
      "loss": 5.9688,
      "step": 4750
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.7109375,
      "learning_rate": 0.000687248322147651,
      "loss": 6.6688,
      "step": 4760
    },
    {
      "epoch": 4.77,
      "grad_norm": 1.0625,
      "learning_rate": 0.0006865771812080537,
      "loss": 6.1953,
      "step": 4770
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0006859060402684563,
      "loss": 5.7656,
      "step": 4780
    },
    {
      "epoch": 4.79,
      "grad_norm": 2.0625,
      "learning_rate": 0.0006852348993288591,
      "loss": 6.7359,
      "step": 4790
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.9375,
      "learning_rate": 0.0006845637583892618,
      "loss": 6.0922,
      "step": 4800
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 1.25,
      "learning_rate": 0.0006838926174496645,
      "loss": 6.217,
      "step": 4810
    },
    {
      "epoch": 4.82,
      "grad_norm": 1.4375,
      "learning_rate": 0.0006832214765100672,
      "loss": 7.4891,
      "step": 4820
    },
    {
      "epoch": 4.83,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0006825503355704698,
      "loss": 6.0969,
      "step": 4830
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0006818791946308724,
      "loss": 4.6531,
      "step": 4840
    },
    {
      "epoch": 4.85,
      "grad_norm": 1.3046875,
      "learning_rate": 0.0006812080536912752,
      "loss": 6.3156,
      "step": 4850
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0006805369127516779,
      "loss": 5.3266,
      "step": 4860
    },
    {
      "epoch": 4.87,
      "grad_norm": 2.046875,
      "learning_rate": 0.0006798657718120805,
      "loss": 7.0393,
      "step": 4870
    },
    {
      "epoch": 4.88,
      "grad_norm": 1.140625,
      "learning_rate": 0.0006791946308724832,
      "loss": 5.7344,
      "step": 4880
    },
    {
      "epoch": 4.89,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0006785234899328859,
      "loss": 5.6094,
      "step": 4890
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006778523489932886,
      "loss": 6.9062,
      "step": 4900
    },
    {
      "epoch": 4.91,
      "grad_norm": 1.078125,
      "learning_rate": 0.0006771812080536914,
      "loss": 7.4094,
      "step": 4910
    },
    {
      "epoch": 4.92,
      "grad_norm": 1.203125,
      "learning_rate": 0.000676510067114094,
      "loss": 6.0859,
      "step": 4920
    },
    {
      "epoch": 4.93,
      "grad_norm": 2.109375,
      "learning_rate": 0.0006758389261744966,
      "loss": 5.4984,
      "step": 4930
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0006751677852348993,
      "loss": 4.8312,
      "step": 4940
    },
    {
      "epoch": 4.95,
      "grad_norm": 1.2421875,
      "learning_rate": 0.000674496644295302,
      "loss": 6.0728,
      "step": 4950
    },
    {
      "epoch": 4.96,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0006738255033557047,
      "loss": 6.5466,
      "step": 4960
    },
    {
      "epoch": 4.97,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0006731543624161074,
      "loss": 7.0297,
      "step": 4970
    },
    {
      "epoch": 4.98,
      "grad_norm": 1.984375,
      "learning_rate": 0.0006724832214765101,
      "loss": 7.6391,
      "step": 4980
    },
    {
      "epoch": 4.99,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0006718120805369128,
      "loss": 5.7938,
      "step": 4990
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0006711409395973155,
      "loss": 5.2016,
      "step": 5000
    },
    {
      "epoch": 5.0,
      "eval_loss": 5.844687461853027,
      "eval_runtime": 24.0006,
      "eval_samples_per_second": 4.167,
      "eval_steps_per_second": 4.167,
      "step": 5000
    },
    {
      "epoch": 5.01,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0006704697986577182,
      "loss": 6.8484,
      "step": 5010
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.74609375,
      "learning_rate": 0.0006697986577181208,
      "loss": 5.9766,
      "step": 5020
    },
    {
      "epoch": 5.03,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006691275167785234,
      "loss": 6.15,
      "step": 5030
    },
    {
      "epoch": 5.04,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0006684563758389262,
      "loss": 5.8875,
      "step": 5040
    },
    {
      "epoch": 5.05,
      "grad_norm": 1.6015625,
      "learning_rate": 0.0006677852348993289,
      "loss": 5.9609,
      "step": 5050
    },
    {
      "epoch": 5.06,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0006671140939597315,
      "loss": 4.7766,
      "step": 5060
    },
    {
      "epoch": 5.07,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0006664429530201342,
      "loss": 4.7828,
      "step": 5070
    },
    {
      "epoch": 5.08,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0006657718120805369,
      "loss": 5.8094,
      "step": 5080
    },
    {
      "epoch": 5.09,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0006651006711409397,
      "loss": 6.4359,
      "step": 5090
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.9921875,
      "learning_rate": 0.0006644295302013424,
      "loss": 5.1646,
      "step": 5100
    },
    {
      "epoch": 5.11,
      "grad_norm": 2.046875,
      "learning_rate": 0.000663758389261745,
      "loss": 5.0187,
      "step": 5110
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.703125,
      "learning_rate": 0.0006630872483221476,
      "loss": 4.9297,
      "step": 5120
    },
    {
      "epoch": 5.13,
      "grad_norm": 1.9375,
      "learning_rate": 0.0006624161073825503,
      "loss": 5.3034,
      "step": 5130
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.6640625,
      "learning_rate": 0.000661744966442953,
      "loss": 5.0078,
      "step": 5140
    },
    {
      "epoch": 5.15,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0006610738255033558,
      "loss": 8.3918,
      "step": 5150
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0006604026845637584,
      "loss": 4.4719,
      "step": 5160
    },
    {
      "epoch": 5.17,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0006597315436241611,
      "loss": 5.5484,
      "step": 5170
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0006590604026845638,
      "loss": 4.6281,
      "step": 5180
    },
    {
      "epoch": 5.19,
      "grad_norm": 0.96875,
      "learning_rate": 0.0006583892617449665,
      "loss": 5.4484,
      "step": 5190
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0006577181208053692,
      "loss": 4.8594,
      "step": 5200
    },
    {
      "epoch": 5.21,
      "grad_norm": 1.15625,
      "learning_rate": 0.0006570469798657717,
      "loss": 4.6469,
      "step": 5210
    },
    {
      "epoch": 5.22,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0006563758389261745,
      "loss": 5.2932,
      "step": 5220
    },
    {
      "epoch": 5.23,
      "grad_norm": 1.125,
      "learning_rate": 0.0006557046979865772,
      "loss": 5.1766,
      "step": 5230
    },
    {
      "epoch": 5.24,
      "grad_norm": 1.3359375,
      "learning_rate": 0.0006550335570469799,
      "loss": 5.3344,
      "step": 5240
    },
    {
      "epoch": 5.25,
      "grad_norm": 1.484375,
      "learning_rate": 0.0006543624161073825,
      "loss": 7.0375,
      "step": 5250
    },
    {
      "epoch": 5.26,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0006536912751677852,
      "loss": 5.6219,
      "step": 5260
    },
    {
      "epoch": 5.27,
      "grad_norm": 1.171875,
      "learning_rate": 0.000653020134228188,
      "loss": 5.7727,
      "step": 5270
    },
    {
      "epoch": 5.28,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0006523489932885907,
      "loss": 5.6781,
      "step": 5280
    },
    {
      "epoch": 5.29,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0006516778523489934,
      "loss": 6.0062,
      "step": 5290
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0006510067114093959,
      "loss": 6.3469,
      "step": 5300
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.93359375,
      "learning_rate": 0.0006503355704697986,
      "loss": 6.8047,
      "step": 5310
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0006496644295302013,
      "loss": 6.9328,
      "step": 5320
    },
    {
      "epoch": 5.33,
      "grad_norm": 0.75,
      "learning_rate": 0.000648993288590604,
      "loss": 6.6609,
      "step": 5330
    },
    {
      "epoch": 5.34,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0006483221476510068,
      "loss": 5.3672,
      "step": 5340
    },
    {
      "epoch": 5.35,
      "grad_norm": 1.984375,
      "learning_rate": 0.0006476510067114094,
      "loss": 6.8927,
      "step": 5350
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006469798657718121,
      "loss": 4.8937,
      "step": 5360
    },
    {
      "epoch": 5.37,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0006463087248322148,
      "loss": 6.3109,
      "step": 5370
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006456375838926175,
      "loss": 5.5906,
      "step": 5380
    },
    {
      "epoch": 5.39,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0006449664429530201,
      "loss": 6.3422,
      "step": 5390
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006442953020134228,
      "loss": 5.2453,
      "step": 5400
    },
    {
      "epoch": 5.41,
      "grad_norm": 1.0625,
      "learning_rate": 0.0006436241610738255,
      "loss": 5.6799,
      "step": 5410
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0006429530201342282,
      "loss": 5.8328,
      "step": 5420
    },
    {
      "epoch": 5.43,
      "grad_norm": 2.0625,
      "learning_rate": 0.0006422818791946309,
      "loss": 6.1969,
      "step": 5430
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0006416107382550335,
      "loss": 7.3359,
      "step": 5440
    },
    {
      "epoch": 5.45,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006409395973154362,
      "loss": 5.4234,
      "step": 5450
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.69140625,
      "learning_rate": 0.000640268456375839,
      "loss": 5.25,
      "step": 5460
    },
    {
      "epoch": 5.47,
      "grad_norm": 1.984375,
      "learning_rate": 0.0006395973154362417,
      "loss": 6.3824,
      "step": 5470
    },
    {
      "epoch": 5.48,
      "grad_norm": 1.15625,
      "learning_rate": 0.0006389261744966444,
      "loss": 5.3344,
      "step": 5480
    },
    {
      "epoch": 5.49,
      "grad_norm": 1.046875,
      "learning_rate": 0.0006382550335570469,
      "loss": 4.6813,
      "step": 5490
    },
    {
      "epoch": 5.5,
      "grad_norm": 1.03125,
      "learning_rate": 0.0006375838926174496,
      "loss": 7.0367,
      "step": 5500
    },
    {
      "epoch": 5.51,
      "grad_norm": 0.703125,
      "learning_rate": 0.0006369127516778523,
      "loss": 7.1937,
      "step": 5510
    },
    {
      "epoch": 5.52,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0006362416107382551,
      "loss": 5.325,
      "step": 5520
    },
    {
      "epoch": 5.53,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0006355704697986578,
      "loss": 5.6469,
      "step": 5530
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.98828125,
      "learning_rate": 0.0006348993288590604,
      "loss": 5.0703,
      "step": 5540
    },
    {
      "epoch": 5.55,
      "grad_norm": 1.203125,
      "learning_rate": 0.0006342281879194631,
      "loss": 6.0531,
      "step": 5550
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.484375,
      "learning_rate": 0.0006335570469798658,
      "loss": 5.3505,
      "step": 5560
    },
    {
      "epoch": 5.57,
      "grad_norm": 0.91015625,
      "learning_rate": 0.0006328859060402686,
      "loss": 6.1922,
      "step": 5570
    },
    {
      "epoch": 5.58,
      "grad_norm": 1.25,
      "learning_rate": 0.0006322147651006712,
      "loss": 4.3828,
      "step": 5580
    },
    {
      "epoch": 5.59,
      "grad_norm": 0.71875,
      "learning_rate": 0.0006315436241610738,
      "loss": 5.2703,
      "step": 5590
    },
    {
      "epoch": 5.6,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0006308724832214765,
      "loss": 5.5922,
      "step": 5600
    },
    {
      "epoch": 5.61,
      "grad_norm": 1.0,
      "learning_rate": 0.0006302013422818792,
      "loss": 7.2536,
      "step": 5610
    },
    {
      "epoch": 5.62,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0006295302013422819,
      "loss": 7.0187,
      "step": 5620
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.71875,
      "learning_rate": 0.0006288590604026845,
      "loss": 6.9953,
      "step": 5630
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.9140625,
      "learning_rate": 0.0006281879194630873,
      "loss": 6.0594,
      "step": 5640
    },
    {
      "epoch": 5.65,
      "grad_norm": 0.71875,
      "learning_rate": 0.00062751677852349,
      "loss": 5.5094,
      "step": 5650
    },
    {
      "epoch": 5.66,
      "grad_norm": 1.25,
      "learning_rate": 0.0006268456375838927,
      "loss": 6.2875,
      "step": 5660
    },
    {
      "epoch": 5.67,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0006261744966442953,
      "loss": 5.3172,
      "step": 5670
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006255033557046979,
      "loss": 5.4125,
      "step": 5680
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 0.703125,
      "learning_rate": 0.0006248322147651006,
      "loss": 5.9656,
      "step": 5690
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0006241610738255034,
      "loss": 6.3078,
      "step": 5700
    },
    {
      "epoch": 5.71,
      "grad_norm": 1.03125,
      "learning_rate": 0.0006234899328859061,
      "loss": 6.3937,
      "step": 5710
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0006228187919463088,
      "loss": 6.1094,
      "step": 5720
    },
    {
      "epoch": 5.73,
      "grad_norm": 1.15625,
      "learning_rate": 0.0006221476510067114,
      "loss": 4.5234,
      "step": 5730
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0006214765100671141,
      "loss": 6.9313,
      "step": 5740
    },
    {
      "epoch": 5.75,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0006208053691275168,
      "loss": 4.7594,
      "step": 5750
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0006201342281879195,
      "loss": 6.9906,
      "step": 5760
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0006194630872483222,
      "loss": 4.6323,
      "step": 5770
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006187919463087248,
      "loss": 5.4453,
      "step": 5780
    },
    {
      "epoch": 5.79,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006181208053691275,
      "loss": 6.7438,
      "step": 5790
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.0,
      "learning_rate": 0.0006174496644295302,
      "loss": 5.6891,
      "step": 5800
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 0.953125,
      "learning_rate": 0.0006167785234899329,
      "loss": 5.0109,
      "step": 5810
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0006161073825503356,
      "loss": 6.4391,
      "step": 5820
    },
    {
      "epoch": 5.83,
      "grad_norm": 2.078125,
      "learning_rate": 0.0006154362416107383,
      "loss": 6.3937,
      "step": 5830
    },
    {
      "epoch": 5.84,
      "grad_norm": 1.15625,
      "learning_rate": 0.000614765100671141,
      "loss": 5.3072,
      "step": 5840
    },
    {
      "epoch": 5.85,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0006140939597315436,
      "loss": 5.2391,
      "step": 5850
    },
    {
      "epoch": 5.86,
      "grad_norm": 1.078125,
      "learning_rate": 0.0006134228187919463,
      "loss": 6.8875,
      "step": 5860
    },
    {
      "epoch": 5.87,
      "grad_norm": 1.53125,
      "learning_rate": 0.0006127516778523489,
      "loss": 6.2156,
      "step": 5870
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0006120805369127517,
      "loss": 6.1734,
      "step": 5880
    },
    {
      "epoch": 5.89,
      "grad_norm": 1.9375,
      "learning_rate": 0.0006114093959731544,
      "loss": 6.3047,
      "step": 5890
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006107382550335571,
      "loss": 7.5625,
      "step": 5900
    },
    {
      "epoch": 5.91,
      "grad_norm": 1.03125,
      "learning_rate": 0.0006100671140939598,
      "loss": 6.7984,
      "step": 5910
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0006093959731543624,
      "loss": 8.5359,
      "step": 5920
    },
    {
      "epoch": 5.93,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0006087248322147651,
      "loss": 6.125,
      "step": 5930
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0006080536912751679,
      "loss": 6.7234,
      "step": 5940
    },
    {
      "epoch": 5.95,
      "grad_norm": 1.375,
      "learning_rate": 0.0006073825503355705,
      "loss": 6.9141,
      "step": 5950
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0006067114093959732,
      "loss": 4.9344,
      "step": 5960
    },
    {
      "epoch": 5.97,
      "grad_norm": 1.375,
      "learning_rate": 0.0006060402684563758,
      "loss": 5.1355,
      "step": 5970
    },
    {
      "epoch": 5.98,
      "grad_norm": 2.046875,
      "learning_rate": 0.0006053691275167785,
      "loss": 6.2531,
      "step": 5980
    },
    {
      "epoch": 5.99,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0006046979865771812,
      "loss": 5.3109,
      "step": 5990
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.234375,
      "learning_rate": 0.000604026845637584,
      "loss": 6.9344,
      "step": 6000
    },
    {
      "epoch": 6.0,
      "eval_loss": 5.829062461853027,
      "eval_runtime": 24.006,
      "eval_samples_per_second": 4.166,
      "eval_steps_per_second": 4.166,
      "step": 6000
    },
    {
      "epoch": 6.01,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0006033557046979866,
      "loss": 6.8078,
      "step": 6010
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.72265625,
      "learning_rate": 0.0006026845637583893,
      "loss": 4.7266,
      "step": 6020
    },
    {
      "epoch": 6.03,
      "grad_norm": 0.6953125,
      "learning_rate": 0.000602013422818792,
      "loss": 4.8844,
      "step": 6030
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0006013422818791946,
      "loss": 4.3766,
      "step": 6040
    },
    {
      "epoch": 6.05,
      "grad_norm": 1.5078125,
      "learning_rate": 0.0006006711409395973,
      "loss": 5.7125,
      "step": 6050
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0006,
      "loss": 6.4188,
      "step": 6060
    },
    {
      "epoch": 6.07,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0005993288590604027,
      "loss": 6.725,
      "step": 6070
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0005986577181208054,
      "loss": 5.216,
      "step": 6080
    },
    {
      "epoch": 6.09,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0005979865771812081,
      "loss": 4.6212,
      "step": 6090
    },
    {
      "epoch": 6.1,
      "grad_norm": 1.28125,
      "learning_rate": 0.0005973154362416108,
      "loss": 5.7625,
      "step": 6100
    },
    {
      "epoch": 6.11,
      "grad_norm": 0.671875,
      "learning_rate": 0.0005966442953020134,
      "loss": 5.2422,
      "step": 6110
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0005959731543624162,
      "loss": 5.1,
      "step": 6120
    },
    {
      "epoch": 6.13,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0005953020134228188,
      "loss": 5.8672,
      "step": 6130
    },
    {
      "epoch": 6.14,
      "grad_norm": 1.3203125,
      "learning_rate": 0.0005946308724832215,
      "loss": 7.1656,
      "step": 6140
    },
    {
      "epoch": 6.15,
      "grad_norm": 1.1875,
      "learning_rate": 0.0005939597315436241,
      "loss": 8.3234,
      "step": 6150
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.6875,
      "learning_rate": 0.0005932885906040268,
      "loss": 6.225,
      "step": 6160
    },
    {
      "epoch": 6.17,
      "grad_norm": 1.3828125,
      "learning_rate": 0.0005926174496644295,
      "loss": 6.675,
      "step": 6170
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0005919463087248323,
      "loss": 5.7969,
      "step": 6180
    },
    {
      "epoch": 6.19,
      "grad_norm": 0.98046875,
      "learning_rate": 0.000591275167785235,
      "loss": 6.3266,
      "step": 6190
    },
    {
      "epoch": 6.2,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005906040268456376,
      "loss": 5.9016,
      "step": 6200
    },
    {
      "epoch": 6.21,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0005899328859060403,
      "loss": 6.1969,
      "step": 6210
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0005892617449664429,
      "loss": 4.9797,
      "step": 6220
    },
    {
      "epoch": 6.23,
      "grad_norm": 1.296875,
      "learning_rate": 0.0005885906040268456,
      "loss": 4.825,
      "step": 6230
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0005879194630872484,
      "loss": 7.2344,
      "step": 6240
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.71484375,
      "learning_rate": 0.000587248322147651,
      "loss": 5.1698,
      "step": 6250
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.6875,
      "learning_rate": 0.0005865771812080537,
      "loss": 4.9062,
      "step": 6260
    },
    {
      "epoch": 6.27,
      "grad_norm": 0.0,
      "learning_rate": 0.0005859060402684564,
      "loss": 5.5241,
      "step": 6270
    },
    {
      "epoch": 6.28,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0005852348993288591,
      "loss": 7.2109,
      "step": 6280
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0005845637583892618,
      "loss": 5.8469,
      "step": 6290
    },
    {
      "epoch": 6.3,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005838926174496644,
      "loss": 5.4172,
      "step": 6300
    },
    {
      "epoch": 6.31,
      "grad_norm": 0.921875,
      "learning_rate": 0.0005832214765100671,
      "loss": 4.8875,
      "step": 6310
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.6875,
      "learning_rate": 0.0005825503355704698,
      "loss": 6.9875,
      "step": 6320
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0005818791946308725,
      "loss": 4.5828,
      "step": 6330
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.9453125,
      "learning_rate": 0.0005812080536912751,
      "loss": 8.4094,
      "step": 6340
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0005805369127516778,
      "loss": 4.2313,
      "step": 6350
    },
    {
      "epoch": 6.36,
      "grad_norm": 2.046875,
      "learning_rate": 0.0005798657718120805,
      "loss": 5.4172,
      "step": 6360
    },
    {
      "epoch": 6.37,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0005791946308724833,
      "loss": 5.2016,
      "step": 6370
    },
    {
      "epoch": 6.38,
      "grad_norm": 1.015625,
      "learning_rate": 0.000578523489932886,
      "loss": 5.8375,
      "step": 6380
    },
    {
      "epoch": 6.39,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0005778523489932886,
      "loss": 5.1516,
      "step": 6390
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.703125,
      "learning_rate": 0.0005771812080536913,
      "loss": 5.4726,
      "step": 6400
    },
    {
      "epoch": 6.41,
      "grad_norm": 0.984375,
      "learning_rate": 0.0005765100671140939,
      "loss": 5.5984,
      "step": 6410
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0005758389261744966,
      "loss": 5.1125,
      "step": 6420
    },
    {
      "epoch": 6.43,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0005751677852348994,
      "loss": 5.9031,
      "step": 6430
    },
    {
      "epoch": 6.44,
      "grad_norm": 1.21875,
      "learning_rate": 0.000574496644295302,
      "loss": 6.9609,
      "step": 6440
    },
    {
      "epoch": 6.45,
      "grad_norm": 1.140625,
      "learning_rate": 0.0005738255033557047,
      "loss": 6.1094,
      "step": 6450
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0005731543624161074,
      "loss": 7.1312,
      "step": 6460
    },
    {
      "epoch": 6.47,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0005724832214765101,
      "loss": 5.7474,
      "step": 6470
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0005718120805369129,
      "loss": 5.7625,
      "step": 6480
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0005711409395973155,
      "loss": 5.4875,
      "step": 6490
    },
    {
      "epoch": 6.5,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0005704697986577181,
      "loss": 5.7094,
      "step": 6500
    },
    {
      "epoch": 6.51,
      "grad_norm": 0.98828125,
      "learning_rate": 0.0005697986577181208,
      "loss": 7.125,
      "step": 6510
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0005691275167785235,
      "loss": 5.2057,
      "step": 6520
    },
    {
      "epoch": 6.53,
      "grad_norm": 1.2265625,
      "learning_rate": 0.0005684563758389261,
      "loss": 7.2805,
      "step": 6530
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.89453125,
      "learning_rate": 0.0005677852348993288,
      "loss": 5.6531,
      "step": 6540
    },
    {
      "epoch": 6.55,
      "grad_norm": 1.203125,
      "learning_rate": 0.0005671140939597316,
      "loss": 5.0766,
      "step": 6550
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.984375,
      "learning_rate": 0.0005664429530201343,
      "loss": 5.8141,
      "step": 6560
    },
    {
      "epoch": 6.57,
      "grad_norm": 1.203125,
      "learning_rate": 0.000565771812080537,
      "loss": 6.1641,
      "step": 6570
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0005651006711409396,
      "loss": 4.8724,
      "step": 6580
    },
    {
      "epoch": 6.59,
      "grad_norm": 1.125,
      "learning_rate": 0.0005644295302013422,
      "loss": 4.9375,
      "step": 6590
    },
    {
      "epoch": 6.6,
      "grad_norm": 1.1171875,
      "learning_rate": 0.0005637583892617449,
      "loss": 5.525,
      "step": 6600
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.6484375,
      "learning_rate": 0.0005630872483221477,
      "loss": 5.7875,
      "step": 6610
    },
    {
      "epoch": 6.62,
      "grad_norm": 1.546875,
      "learning_rate": 0.0005624161073825504,
      "loss": 8.0969,
      "step": 6620
    },
    {
      "epoch": 6.63,
      "grad_norm": 1.953125,
      "learning_rate": 0.000561744966442953,
      "loss": 6.3797,
      "step": 6630
    },
    {
      "epoch": 6.64,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0005610738255033557,
      "loss": 6.8734,
      "step": 6640
    },
    {
      "epoch": 6.65,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0005604026845637584,
      "loss": 5.9406,
      "step": 6650
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.73828125,
      "learning_rate": 0.0005597315436241611,
      "loss": 5.2497,
      "step": 6660
    },
    {
      "epoch": 6.67,
      "grad_norm": 1.015625,
      "learning_rate": 0.0005590604026845639,
      "loss": 8.6881,
      "step": 6670
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0005583892617449664,
      "loss": 5.5094,
      "step": 6680
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 0.96875,
      "learning_rate": 0.0005577181208053691,
      "loss": 5.6234,
      "step": 6690
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0005570469798657718,
      "loss": 6.5156,
      "step": 6700
    },
    {
      "epoch": 6.71,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0005563758389261745,
      "loss": 4.1188,
      "step": 6710
    },
    {
      "epoch": 6.72,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005557046979865771,
      "loss": 7.1156,
      "step": 6720
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.6875,
      "learning_rate": 0.0005550335570469799,
      "loss": 5.4797,
      "step": 6730
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0005543624161073826,
      "loss": 4.0438,
      "step": 6740
    },
    {
      "epoch": 6.75,
      "grad_norm": 1.9921875,
      "learning_rate": 0.0005536912751677853,
      "loss": 6.2033,
      "step": 6750
    },
    {
      "epoch": 6.76,
      "grad_norm": 1.015625,
      "learning_rate": 0.000553020134228188,
      "loss": 6.8855,
      "step": 6760
    },
    {
      "epoch": 6.77,
      "grad_norm": 1.203125,
      "learning_rate": 0.0005523489932885905,
      "loss": 4.3391,
      "step": 6770
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.93359375,
      "learning_rate": 0.0005516778523489932,
      "loss": 7.2438,
      "step": 6780
    },
    {
      "epoch": 6.79,
      "grad_norm": 1.21875,
      "learning_rate": 0.000551006711409396,
      "loss": 4.8984,
      "step": 6790
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.53125,
      "learning_rate": 0.0005503355704697987,
      "loss": 5.8031,
      "step": 6800
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 1.171875,
      "learning_rate": 0.0005496644295302014,
      "loss": 7.2203,
      "step": 6810
    },
    {
      "epoch": 6.82,
      "grad_norm": 1.1796875,
      "learning_rate": 0.000548993288590604,
      "loss": 6.1141,
      "step": 6820
    },
    {
      "epoch": 6.83,
      "grad_norm": 1.03125,
      "learning_rate": 0.0005483221476510067,
      "loss": 6.0109,
      "step": 6830
    },
    {
      "epoch": 6.84,
      "grad_norm": 1.0,
      "learning_rate": 0.0005476510067114094,
      "loss": 6.5734,
      "step": 6840
    },
    {
      "epoch": 6.85,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0005469798657718122,
      "loss": 4.6281,
      "step": 6850
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.984375,
      "learning_rate": 0.0005463087248322149,
      "loss": 5.3172,
      "step": 6860
    },
    {
      "epoch": 6.87,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0005456375838926174,
      "loss": 6.0125,
      "step": 6870
    },
    {
      "epoch": 6.88,
      "grad_norm": 1.171875,
      "learning_rate": 0.0005449664429530201,
      "loss": 6.5844,
      "step": 6880
    },
    {
      "epoch": 6.89,
      "grad_norm": 1.015625,
      "learning_rate": 0.0005442953020134228,
      "loss": 6.4234,
      "step": 6890
    },
    {
      "epoch": 6.9,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005436241610738255,
      "loss": 5.2375,
      "step": 6900
    },
    {
      "epoch": 6.91,
      "grad_norm": 1.015625,
      "learning_rate": 0.0005429530201342282,
      "loss": 5.4391,
      "step": 6910
    },
    {
      "epoch": 6.92,
      "grad_norm": 1.140625,
      "learning_rate": 0.0005422818791946309,
      "loss": 5.4406,
      "step": 6920
    },
    {
      "epoch": 6.93,
      "grad_norm": 1.03125,
      "learning_rate": 0.0005416107382550336,
      "loss": 7.0484,
      "step": 6930
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 2.0,
      "learning_rate": 0.0005409395973154363,
      "loss": 6.2469,
      "step": 6940
    },
    {
      "epoch": 6.95,
      "grad_norm": 1.25,
      "learning_rate": 0.000540268456375839,
      "loss": 6.8141,
      "step": 6950
    },
    {
      "epoch": 6.96,
      "grad_norm": 1.1875,
      "learning_rate": 0.0005395973154362415,
      "loss": 6.6594,
      "step": 6960
    },
    {
      "epoch": 6.97,
      "grad_norm": 1.03125,
      "learning_rate": 0.0005389261744966442,
      "loss": 6.4391,
      "step": 6970
    },
    {
      "epoch": 6.98,
      "grad_norm": 1.0390625,
      "learning_rate": 0.000538255033557047,
      "loss": 5.3141,
      "step": 6980
    },
    {
      "epoch": 6.99,
      "grad_norm": 1.125,
      "learning_rate": 0.0005375838926174497,
      "loss": 5.9922,
      "step": 6990
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0005369127516778524,
      "loss": 5.3422,
      "step": 7000
    },
    {
      "epoch": 7.0,
      "eval_loss": 5.829687595367432,
      "eval_runtime": 24.0226,
      "eval_samples_per_second": 4.163,
      "eval_steps_per_second": 4.163,
      "step": 7000
    },
    {
      "epoch": 7.01,
      "grad_norm": 0.6953125,
      "learning_rate": 0.000536241610738255,
      "loss": 6.2891,
      "step": 7010
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.71875,
      "learning_rate": 0.0005355704697986577,
      "loss": 8.4016,
      "step": 7020
    },
    {
      "epoch": 7.03,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0005348993288590605,
      "loss": 7.5156,
      "step": 7030
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0005342281879194632,
      "loss": 4.5172,
      "step": 7040
    },
    {
      "epoch": 7.05,
      "grad_norm": 2.03125,
      "learning_rate": 0.0005335570469798658,
      "loss": 7.7938,
      "step": 7050
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.703125,
      "learning_rate": 0.0005328859060402684,
      "loss": 5.5266,
      "step": 7060
    },
    {
      "epoch": 7.07,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005322147651006711,
      "loss": 5.3547,
      "step": 7070
    },
    {
      "epoch": 7.08,
      "grad_norm": 1.234375,
      "learning_rate": 0.0005315436241610738,
      "loss": 5.6984,
      "step": 7080
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0005308724832214766,
      "loss": 4.7875,
      "step": 7090
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.0,
      "learning_rate": 0.0005302013422818792,
      "loss": 6.6722,
      "step": 7100
    },
    {
      "epoch": 7.11,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0005295302013422819,
      "loss": 6.1984,
      "step": 7110
    },
    {
      "epoch": 7.12,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0005288590604026846,
      "loss": 5.4172,
      "step": 7120
    },
    {
      "epoch": 7.13,
      "grad_norm": 0.8515625,
      "learning_rate": 0.0005281879194630873,
      "loss": 5.8406,
      "step": 7130
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0005275167785234899,
      "loss": 7.1437,
      "step": 7140
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.6875,
      "learning_rate": 0.0005268456375838925,
      "loss": 4.6,
      "step": 7150
    },
    {
      "epoch": 7.16,
      "grad_norm": 2.03125,
      "learning_rate": 0.0005261744966442953,
      "loss": 8.5156,
      "step": 7160
    },
    {
      "epoch": 7.17,
      "grad_norm": 2.046875,
      "learning_rate": 0.000525503355704698,
      "loss": 6.9139,
      "step": 7170
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0005248322147651007,
      "loss": 4.5828,
      "step": 7180
    },
    {
      "epoch": 7.19,
      "grad_norm": 0.9765625,
      "learning_rate": 0.0005241610738255034,
      "loss": 5.1734,
      "step": 7190
    },
    {
      "epoch": 7.2,
      "grad_norm": 1.0,
      "learning_rate": 0.000523489932885906,
      "loss": 5.432,
      "step": 7200
    },
    {
      "epoch": 7.21,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005228187919463088,
      "loss": 5.0578,
      "step": 7210
    },
    {
      "epoch": 7.22,
      "grad_norm": 1.203125,
      "learning_rate": 0.0005221476510067115,
      "loss": 5.9141,
      "step": 7220
    },
    {
      "epoch": 7.23,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005214765100671141,
      "loss": 5.9203,
      "step": 7230
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0005208053691275168,
      "loss": 5.4484,
      "step": 7240
    },
    {
      "epoch": 7.25,
      "grad_norm": 1.21875,
      "learning_rate": 0.0005201342281879194,
      "loss": 6.5156,
      "step": 7250
    },
    {
      "epoch": 7.26,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0005194630872483221,
      "loss": 5.4016,
      "step": 7260
    },
    {
      "epoch": 7.27,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0005187919463087248,
      "loss": 6.5156,
      "step": 7270
    },
    {
      "epoch": 7.28,
      "grad_norm": 1.03125,
      "learning_rate": 0.0005181208053691276,
      "loss": 6.0,
      "step": 7280
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0005174496644295302,
      "loss": 4.4203,
      "step": 7290
    },
    {
      "epoch": 7.3,
      "grad_norm": 1.2890625,
      "learning_rate": 0.0005167785234899329,
      "loss": 7.8281,
      "step": 7300
    },
    {
      "epoch": 7.31,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0005161073825503356,
      "loss": 4.7687,
      "step": 7310
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0005154362416107383,
      "loss": 4.9188,
      "step": 7320
    },
    {
      "epoch": 7.33,
      "grad_norm": 0.671875,
      "learning_rate": 0.000514765100671141,
      "loss": 6.175,
      "step": 7330
    },
    {
      "epoch": 7.34,
      "grad_norm": 1.46875,
      "learning_rate": 0.0005140939597315436,
      "loss": 6.3563,
      "step": 7340
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0005134228187919463,
      "loss": 8.0453,
      "step": 7350
    },
    {
      "epoch": 7.36,
      "grad_norm": 1.0234375,
      "learning_rate": 0.000512751677852349,
      "loss": 6.0187,
      "step": 7360
    },
    {
      "epoch": 7.37,
      "grad_norm": 1.203125,
      "learning_rate": 0.0005120805369127517,
      "loss": 6.3672,
      "step": 7370
    },
    {
      "epoch": 7.38,
      "grad_norm": 1.171875,
      "learning_rate": 0.0005114093959731544,
      "loss": 5.975,
      "step": 7380
    },
    {
      "epoch": 7.39,
      "grad_norm": 1.0625,
      "learning_rate": 0.000510738255033557,
      "loss": 4.0141,
      "step": 7390
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0005100671140939598,
      "loss": 5.2594,
      "step": 7400
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0005093959731543625,
      "loss": 5.8422,
      "step": 7410
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0005087248322147651,
      "loss": 5.8594,
      "step": 7420
    },
    {
      "epoch": 7.43,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0005080536912751678,
      "loss": 6.2109,
      "step": 7430
    },
    {
      "epoch": 7.44,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0005073825503355704,
      "loss": 5.4781,
      "step": 7440
    },
    {
      "epoch": 7.45,
      "grad_norm": 0.0,
      "learning_rate": 0.0005067114093959731,
      "loss": 6.1463,
      "step": 7450
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0005060402684563759,
      "loss": 6.5016,
      "step": 7460
    },
    {
      "epoch": 7.47,
      "grad_norm": 1.984375,
      "learning_rate": 0.0005053691275167786,
      "loss": 7.9188,
      "step": 7470
    },
    {
      "epoch": 7.48,
      "grad_norm": 2.015625,
      "learning_rate": 0.0005046979865771812,
      "loss": 4.9694,
      "step": 7480
    },
    {
      "epoch": 7.49,
      "grad_norm": 0.0,
      "learning_rate": 0.0005040268456375839,
      "loss": 4.4773,
      "step": 7490
    },
    {
      "epoch": 7.5,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0005033557046979866,
      "loss": 6.1541,
      "step": 7500
    },
    {
      "epoch": 7.51,
      "grad_norm": 0.92578125,
      "learning_rate": 0.0005026845637583892,
      "loss": 5.9844,
      "step": 7510
    },
    {
      "epoch": 7.52,
      "grad_norm": 1.4453125,
      "learning_rate": 0.000502013422818792,
      "loss": 7.4484,
      "step": 7520
    },
    {
      "epoch": 7.53,
      "grad_norm": 1.9375,
      "learning_rate": 0.0005013422818791946,
      "loss": 7.1328,
      "step": 7530
    },
    {
      "epoch": 7.54,
      "grad_norm": 1.171875,
      "learning_rate": 0.0005006711409395973,
      "loss": 6.5656,
      "step": 7540
    },
    {
      "epoch": 7.55,
      "grad_norm": 0.703125,
      "learning_rate": 0.0005,
      "loss": 5.4578,
      "step": 7550
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0004993288590604027,
      "loss": 5.0359,
      "step": 7560
    },
    {
      "epoch": 7.57,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0004986577181208054,
      "loss": 6.1969,
      "step": 7570
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.0,
      "learning_rate": 0.0004979865771812081,
      "loss": 4.9139,
      "step": 7580
    },
    {
      "epoch": 7.59,
      "grad_norm": 1.4453125,
      "learning_rate": 0.0004973154362416108,
      "loss": 5.7984,
      "step": 7590
    },
    {
      "epoch": 7.6,
      "grad_norm": 1.125,
      "learning_rate": 0.0004966442953020134,
      "loss": 4.7781,
      "step": 7600
    },
    {
      "epoch": 7.61,
      "grad_norm": 0.63671875,
      "learning_rate": 0.0004959731543624161,
      "loss": 7.0062,
      "step": 7610
    },
    {
      "epoch": 7.62,
      "grad_norm": 1.03125,
      "learning_rate": 0.0004953020134228188,
      "loss": 6.3187,
      "step": 7620
    },
    {
      "epoch": 7.63,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0004946308724832214,
      "loss": 6.1262,
      "step": 7630
    },
    {
      "epoch": 7.64,
      "grad_norm": 1.046875,
      "learning_rate": 0.0004939597315436242,
      "loss": 5.1359,
      "step": 7640
    },
    {
      "epoch": 7.65,
      "grad_norm": 1.421875,
      "learning_rate": 0.0004932885906040269,
      "loss": 6.4031,
      "step": 7650
    },
    {
      "epoch": 7.66,
      "grad_norm": 1.5,
      "learning_rate": 0.0004926174496644296,
      "loss": 5.4437,
      "step": 7660
    },
    {
      "epoch": 7.67,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0004919463087248322,
      "loss": 5.1594,
      "step": 7670
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0004912751677852349,
      "loss": 5.5375,
      "step": 7680
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0004906040268456376,
      "loss": 5.9484,
      "step": 7690
    },
    {
      "epoch": 7.7,
      "grad_norm": 1.1875,
      "learning_rate": 0.0004899328859060403,
      "loss": 4.9266,
      "step": 7700
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.734375,
      "learning_rate": 0.000489261744966443,
      "loss": 5.8141,
      "step": 7710
    },
    {
      "epoch": 7.72,
      "grad_norm": 1.9296875,
      "learning_rate": 0.0004885906040268456,
      "loss": 5.2992,
      "step": 7720
    },
    {
      "epoch": 7.73,
      "grad_norm": 0.65625,
      "learning_rate": 0.0004879194630872483,
      "loss": 5.8125,
      "step": 7730
    },
    {
      "epoch": 7.74,
      "grad_norm": 0.984375,
      "learning_rate": 0.000487248322147651,
      "loss": 4.8812,
      "step": 7740
    },
    {
      "epoch": 7.75,
      "grad_norm": 1.515625,
      "learning_rate": 0.00048657718120805374,
      "loss": 7.2031,
      "step": 7750
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00048590604026845635,
      "loss": 4.8578,
      "step": 7760
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0004852348993288591,
      "loss": 6.25,
      "step": 7770
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.83203125,
      "learning_rate": 0.00048456375838926174,
      "loss": 5.6641,
      "step": 7780
    },
    {
      "epoch": 7.79,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00048389261744966446,
      "loss": 6.8828,
      "step": 7790
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0004832214765100671,
      "loss": 6.6437,
      "step": 7800
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 1.203125,
      "learning_rate": 0.0004825503355704698,
      "loss": 5.2219,
      "step": 7810
    },
    {
      "epoch": 7.82,
      "grad_norm": 1.0546875,
      "learning_rate": 0.0004818791946308725,
      "loss": 5.9203,
      "step": 7820
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.703125,
      "learning_rate": 0.00048120805369127517,
      "loss": 6.7083,
      "step": 7830
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.703125,
      "learning_rate": 0.0004805369127516779,
      "loss": 6.2359,
      "step": 7840
    },
    {
      "epoch": 7.85,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0004798657718120805,
      "loss": 4.8797,
      "step": 7850
    },
    {
      "epoch": 7.86,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0004791946308724832,
      "loss": 5.1344,
      "step": 7860
    },
    {
      "epoch": 7.87,
      "grad_norm": 1.140625,
      "learning_rate": 0.00047852348993288594,
      "loss": 6.0734,
      "step": 7870
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.0,
      "learning_rate": 0.0004778523489932886,
      "loss": 6.1875,
      "step": 7880
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0004771812080536913,
      "loss": 5.7578,
      "step": 7890
    },
    {
      "epoch": 7.9,
      "grad_norm": 1.09375,
      "learning_rate": 0.00047651006711409394,
      "loss": 5.25,
      "step": 7900
    },
    {
      "epoch": 7.91,
      "grad_norm": 1.109375,
      "learning_rate": 0.00047583892617449665,
      "loss": 5.2203,
      "step": 7910
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0004751677852348993,
      "loss": 7.1453,
      "step": 7920
    },
    {
      "epoch": 7.93,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00047449664429530204,
      "loss": 7.0766,
      "step": 7930
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00047382550335570476,
      "loss": 4.6922,
      "step": 7940
    },
    {
      "epoch": 7.95,
      "grad_norm": 1.21875,
      "learning_rate": 0.00047315436241610737,
      "loss": 7.2578,
      "step": 7950
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0004724832214765101,
      "loss": 4.6984,
      "step": 7960
    },
    {
      "epoch": 7.97,
      "grad_norm": 1.21875,
      "learning_rate": 0.00047181208053691275,
      "loss": 5.45,
      "step": 7970
    },
    {
      "epoch": 7.98,
      "grad_norm": 1.515625,
      "learning_rate": 0.00047114093959731547,
      "loss": 5.7419,
      "step": 7980
    },
    {
      "epoch": 7.99,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0004704697986577181,
      "loss": 4.3301,
      "step": 7990
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.9609375,
      "learning_rate": 0.0004697986577181208,
      "loss": 5.2422,
      "step": 8000
    },
    {
      "epoch": 8.0,
      "eval_loss": 5.823437690734863,
      "eval_runtime": 23.9858,
      "eval_samples_per_second": 4.169,
      "eval_steps_per_second": 4.169,
      "step": 8000
    },
    {
      "epoch": 8.01,
      "grad_norm": 2.03125,
      "learning_rate": 0.0004691275167785235,
      "loss": 6.2656,
      "step": 8010
    },
    {
      "epoch": 8.02,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0004684563758389262,
      "loss": 6.4219,
      "step": 8020
    },
    {
      "epoch": 8.03,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0004677852348993289,
      "loss": 7.1219,
      "step": 8030
    },
    {
      "epoch": 8.04,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0004671140939597315,
      "loss": 5.3187,
      "step": 8040
    },
    {
      "epoch": 8.05,
      "grad_norm": 0.671875,
      "learning_rate": 0.00046644295302013423,
      "loss": 6.5078,
      "step": 8050
    },
    {
      "epoch": 8.06,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00046577181208053695,
      "loss": 5.5078,
      "step": 8060
    },
    {
      "epoch": 8.07,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0004651006711409396,
      "loss": 5.7781,
      "step": 8070
    },
    {
      "epoch": 8.08,
      "grad_norm": 1.515625,
      "learning_rate": 0.0004644295302013423,
      "loss": 6.6992,
      "step": 8080
    },
    {
      "epoch": 8.09,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00046375838926174495,
      "loss": 4.4766,
      "step": 8090
    },
    {
      "epoch": 8.1,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00046308724832214767,
      "loss": 5.7234,
      "step": 8100
    },
    {
      "epoch": 8.11,
      "grad_norm": 0.9765625,
      "learning_rate": 0.00046241610738255033,
      "loss": 6.3078,
      "step": 8110
    },
    {
      "epoch": 8.12,
      "grad_norm": 2.03125,
      "learning_rate": 0.00046174496644295305,
      "loss": 6.525,
      "step": 8120
    },
    {
      "epoch": 8.13,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0004610738255033557,
      "loss": 5.8344,
      "step": 8130
    },
    {
      "epoch": 8.14,
      "grad_norm": 0.75,
      "learning_rate": 0.0004604026845637584,
      "loss": 4.8969,
      "step": 8140
    },
    {
      "epoch": 8.15,
      "grad_norm": 1.1640625,
      "learning_rate": 0.0004597315436241611,
      "loss": 6.7594,
      "step": 8150
    },
    {
      "epoch": 8.16,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00045906040268456377,
      "loss": 5.9031,
      "step": 8160
    },
    {
      "epoch": 8.17,
      "grad_norm": 1.015625,
      "learning_rate": 0.0004583892617449665,
      "loss": 6.6188,
      "step": 8170
    },
    {
      "epoch": 8.18,
      "grad_norm": 1.4921875,
      "learning_rate": 0.0004577181208053691,
      "loss": 6.5469,
      "step": 8180
    },
    {
      "epoch": 8.19,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0004570469798657718,
      "loss": 4.8578,
      "step": 8190
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.453125,
      "learning_rate": 0.00045637583892617453,
      "loss": 7.2297,
      "step": 8200
    },
    {
      "epoch": 8.21,
      "grad_norm": 2.015625,
      "learning_rate": 0.0004557046979865772,
      "loss": 5.0141,
      "step": 8210
    },
    {
      "epoch": 8.22,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00045503355704697986,
      "loss": 6.8469,
      "step": 8220
    },
    {
      "epoch": 8.23,
      "grad_norm": 1.2734375,
      "learning_rate": 0.00045436241610738253,
      "loss": 8.5344,
      "step": 8230
    },
    {
      "epoch": 8.24,
      "grad_norm": 0.703125,
      "learning_rate": 0.00045369127516778525,
      "loss": 7.5438,
      "step": 8240
    },
    {
      "epoch": 8.25,
      "grad_norm": 1.140625,
      "learning_rate": 0.00045302013422818797,
      "loss": 6.0078,
      "step": 8250
    },
    {
      "epoch": 8.26,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00045234899328859063,
      "loss": 4.0891,
      "step": 8260
    },
    {
      "epoch": 8.27,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0004516778523489933,
      "loss": 6.4453,
      "step": 8270
    },
    {
      "epoch": 8.28,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00045100671140939596,
      "loss": 5.0328,
      "step": 8280
    },
    {
      "epoch": 8.29,
      "grad_norm": 0.703125,
      "learning_rate": 0.0004503355704697987,
      "loss": 4.4391,
      "step": 8290
    },
    {
      "epoch": 8.3,
      "grad_norm": 1.046875,
      "learning_rate": 0.00044966442953020135,
      "loss": 5.8031,
      "step": 8300
    },
    {
      "epoch": 8.31,
      "grad_norm": 0.921875,
      "learning_rate": 0.000448993288590604,
      "loss": 4.8187,
      "step": 8310
    },
    {
      "epoch": 8.32,
      "grad_norm": 1.421875,
      "learning_rate": 0.00044832214765100673,
      "loss": 7.5812,
      "step": 8320
    },
    {
      "epoch": 8.33,
      "grad_norm": 1.0703125,
      "learning_rate": 0.0004476510067114094,
      "loss": 6.65,
      "step": 8330
    },
    {
      "epoch": 8.34,
      "grad_norm": 0.0,
      "learning_rate": 0.0004469798657718121,
      "loss": 5.8607,
      "step": 8340
    },
    {
      "epoch": 8.35,
      "grad_norm": 0.703125,
      "learning_rate": 0.0004463087248322148,
      "loss": 5.1219,
      "step": 8350
    },
    {
      "epoch": 8.36,
      "grad_norm": 1.1640625,
      "learning_rate": 0.00044563758389261745,
      "loss": 6.0422,
      "step": 8360
    },
    {
      "epoch": 8.37,
      "grad_norm": 1.15625,
      "learning_rate": 0.0004449664429530201,
      "loss": 6.4141,
      "step": 8370
    },
    {
      "epoch": 8.38,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00044429530201342283,
      "loss": 5.7578,
      "step": 8380
    },
    {
      "epoch": 8.39,
      "grad_norm": 0.0,
      "learning_rate": 0.00044362416107382555,
      "loss": 5.7406,
      "step": 8390
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.703125,
      "learning_rate": 0.0004429530201342282,
      "loss": 6.7094,
      "step": 8400
    },
    {
      "epoch": 8.41,
      "grad_norm": 1.453125,
      "learning_rate": 0.0004422818791946309,
      "loss": 6.4188,
      "step": 8410
    },
    {
      "epoch": 8.42,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00044161073825503354,
      "loss": 6.3109,
      "step": 8420
    },
    {
      "epoch": 8.43,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00044093959731543626,
      "loss": 5.4266,
      "step": 8430
    },
    {
      "epoch": 8.44,
      "grad_norm": 0.890625,
      "learning_rate": 0.00044026845637583893,
      "loss": 5.5328,
      "step": 8440
    },
    {
      "epoch": 8.45,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0004395973154362416,
      "loss": 6.1562,
      "step": 8450
    },
    {
      "epoch": 8.46,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0004389261744966443,
      "loss": 5.8078,
      "step": 8460
    },
    {
      "epoch": 8.47,
      "grad_norm": 0.9765625,
      "learning_rate": 0.000438255033557047,
      "loss": 5.0891,
      "step": 8470
    },
    {
      "epoch": 8.48,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0004375838926174497,
      "loss": 5.5195,
      "step": 8480
    },
    {
      "epoch": 8.49,
      "grad_norm": 1.046875,
      "learning_rate": 0.00043691275167785236,
      "loss": 6.1594,
      "step": 8490
    },
    {
      "epoch": 8.5,
      "grad_norm": 1.1796875,
      "learning_rate": 0.000436241610738255,
      "loss": 6.4625,
      "step": 8500
    },
    {
      "epoch": 8.51,
      "grad_norm": 1.21875,
      "learning_rate": 0.00043557046979865775,
      "loss": 5.6719,
      "step": 8510
    },
    {
      "epoch": 8.52,
      "grad_norm": 1.171875,
      "learning_rate": 0.0004348993288590604,
      "loss": 5.7859,
      "step": 8520
    },
    {
      "epoch": 8.53,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00043422818791946313,
      "loss": 7.2344,
      "step": 8530
    },
    {
      "epoch": 8.54,
      "grad_norm": 0.6953125,
      "learning_rate": 0.00043355704697986574,
      "loss": 6.3875,
      "step": 8540
    },
    {
      "epoch": 8.55,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00043288590604026846,
      "loss": 6.0984,
      "step": 8550
    },
    {
      "epoch": 8.56,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0004322147651006711,
      "loss": 5.3688,
      "step": 8560
    },
    {
      "epoch": 8.57,
      "grad_norm": 1.234375,
      "learning_rate": 0.00043154362416107384,
      "loss": 6.2047,
      "step": 8570
    },
    {
      "epoch": 8.58,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00043087248322147656,
      "loss": 4.6953,
      "step": 8580
    },
    {
      "epoch": 8.59,
      "grad_norm": 1.234375,
      "learning_rate": 0.0004302013422818792,
      "loss": 5.6906,
      "step": 8590
    },
    {
      "epoch": 8.6,
      "grad_norm": 1.15625,
      "learning_rate": 0.0004295302013422819,
      "loss": 7.1052,
      "step": 8600
    },
    {
      "epoch": 8.61,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00042885906040268456,
      "loss": 4.5438,
      "step": 8610
    },
    {
      "epoch": 8.62,
      "grad_norm": 0.91796875,
      "learning_rate": 0.0004281879194630873,
      "loss": 4.8359,
      "step": 8620
    },
    {
      "epoch": 8.63,
      "grad_norm": 1.203125,
      "learning_rate": 0.00042751677852348994,
      "loss": 5.6781,
      "step": 8630
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0004268456375838926,
      "loss": 6.5797,
      "step": 8640
    },
    {
      "epoch": 8.65,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0004261744966442953,
      "loss": 4.1789,
      "step": 8650
    },
    {
      "epoch": 8.66,
      "grad_norm": 0.65625,
      "learning_rate": 0.000425503355704698,
      "loss": 5.4734,
      "step": 8660
    },
    {
      "epoch": 8.67,
      "grad_norm": 1.0,
      "learning_rate": 0.0004248322147651007,
      "loss": 4.8219,
      "step": 8670
    },
    {
      "epoch": 8.68,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0004241610738255033,
      "loss": 4.7797,
      "step": 8680
    },
    {
      "epoch": 8.69,
      "grad_norm": 1.234375,
      "learning_rate": 0.00042348993288590604,
      "loss": 4.4781,
      "step": 8690
    },
    {
      "epoch": 8.7,
      "grad_norm": 0.8984375,
      "learning_rate": 0.00042281879194630876,
      "loss": 5.7588,
      "step": 8700
    },
    {
      "epoch": 8.71,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0004221476510067114,
      "loss": 5.9156,
      "step": 8710
    },
    {
      "epoch": 8.72,
      "grad_norm": 0.96875,
      "learning_rate": 0.00042147651006711414,
      "loss": 5.1047,
      "step": 8720
    },
    {
      "epoch": 8.73,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00042080536912751675,
      "loss": 7.3406,
      "step": 8730
    },
    {
      "epoch": 8.74,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0004201342281879195,
      "loss": 6.0484,
      "step": 8740
    },
    {
      "epoch": 8.75,
      "grad_norm": 0.9609375,
      "learning_rate": 0.00041946308724832214,
      "loss": 5.0922,
      "step": 8750
    },
    {
      "epoch": 8.76,
      "grad_norm": 1.3125,
      "learning_rate": 0.00041879194630872486,
      "loss": 6.9188,
      "step": 8760
    },
    {
      "epoch": 8.77,
      "grad_norm": 1.03125,
      "learning_rate": 0.0004181208053691275,
      "loss": 6.2219,
      "step": 8770
    },
    {
      "epoch": 8.78,
      "grad_norm": 1.4609375,
      "learning_rate": 0.0004174496644295302,
      "loss": 5.5422,
      "step": 8780
    },
    {
      "epoch": 8.79,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0004167785234899329,
      "loss": 6.0734,
      "step": 8790
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00041610738255033557,
      "loss": 5.7687,
      "step": 8800
    },
    {
      "epoch": 8.81,
      "grad_norm": 1.390625,
      "learning_rate": 0.0004154362416107383,
      "loss": 5.9672,
      "step": 8810
    },
    {
      "epoch": 8.82,
      "grad_norm": 0.703125,
      "learning_rate": 0.0004147651006711409,
      "loss": 5.6734,
      "step": 8820
    },
    {
      "epoch": 8.83,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0004140939597315436,
      "loss": 4.6063,
      "step": 8830
    },
    {
      "epoch": 8.84,
      "grad_norm": 1.21875,
      "learning_rate": 0.00041342281879194634,
      "loss": 5.1531,
      "step": 8840
    },
    {
      "epoch": 8.85,
      "grad_norm": 1.4140625,
      "learning_rate": 0.000412751677852349,
      "loss": 5.8172,
      "step": 8850
    },
    {
      "epoch": 8.86,
      "grad_norm": 1.9375,
      "learning_rate": 0.0004120805369127517,
      "loss": 6.7703,
      "step": 8860
    },
    {
      "epoch": 8.87,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00041140939597315434,
      "loss": 5.6453,
      "step": 8870
    },
    {
      "epoch": 8.88,
      "grad_norm": 0.66796875,
      "learning_rate": 0.00041073825503355705,
      "loss": 5.2812,
      "step": 8880
    },
    {
      "epoch": 8.89,
      "grad_norm": 1.046875,
      "learning_rate": 0.0004100671140939598,
      "loss": 6.2672,
      "step": 8890
    },
    {
      "epoch": 8.9,
      "grad_norm": 1.171875,
      "learning_rate": 0.00040939597315436244,
      "loss": 6.0609,
      "step": 8900
    },
    {
      "epoch": 8.91,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0004087248322147651,
      "loss": 4.5469,
      "step": 8910
    },
    {
      "epoch": 8.92,
      "grad_norm": 1.0625,
      "learning_rate": 0.00040805369127516777,
      "loss": 6.1188,
      "step": 8920
    },
    {
      "epoch": 8.93,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0004073825503355705,
      "loss": 4.9016,
      "step": 8930
    },
    {
      "epoch": 8.94,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00040671140939597315,
      "loss": 6.0891,
      "step": 8940
    },
    {
      "epoch": 8.95,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00040604026845637587,
      "loss": 6.8328,
      "step": 8950
    },
    {
      "epoch": 8.96,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00040536912751677854,
      "loss": 5.7016,
      "step": 8960
    },
    {
      "epoch": 8.97,
      "grad_norm": 2.0,
      "learning_rate": 0.0004046979865771812,
      "loss": 6.7887,
      "step": 8970
    },
    {
      "epoch": 8.98,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0004040268456375839,
      "loss": 5.8563,
      "step": 8980
    },
    {
      "epoch": 8.99,
      "grad_norm": 0.71875,
      "learning_rate": 0.0004033557046979866,
      "loss": 6.775,
      "step": 8990
    },
    {
      "epoch": 9.0,
      "grad_norm": 1.15625,
      "learning_rate": 0.00040268456375838925,
      "loss": 5.7953,
      "step": 9000
    },
    {
      "epoch": 9.0,
      "eval_loss": 5.817343711853027,
      "eval_runtime": 24.0459,
      "eval_samples_per_second": 4.159,
      "eval_steps_per_second": 4.159,
      "step": 9000
    },
    {
      "epoch": 9.01,
      "grad_norm": 0.734375,
      "learning_rate": 0.0004020134228187919,
      "loss": 6.2625,
      "step": 9010
    },
    {
      "epoch": 9.02,
      "grad_norm": 1.15625,
      "learning_rate": 0.00040134228187919464,
      "loss": 6.6688,
      "step": 9020
    },
    {
      "epoch": 9.03,
      "grad_norm": 0.6640625,
      "learning_rate": 0.00040067114093959735,
      "loss": 5.9969,
      "step": 9030
    },
    {
      "epoch": 9.04,
      "grad_norm": 0.97265625,
      "learning_rate": 0.0004,
      "loss": 6.2562,
      "step": 9040
    },
    {
      "epoch": 9.05,
      "grad_norm": 0.90625,
      "learning_rate": 0.0003993288590604027,
      "loss": 5.9453,
      "step": 9050
    },
    {
      "epoch": 9.06,
      "grad_norm": 1.21875,
      "learning_rate": 0.00039865771812080535,
      "loss": 5.8016,
      "step": 9060
    },
    {
      "epoch": 9.07,
      "grad_norm": 1.4765625,
      "learning_rate": 0.00039798657718120807,
      "loss": 8.5219,
      "step": 9070
    },
    {
      "epoch": 9.08,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0003973154362416108,
      "loss": 5.6578,
      "step": 9080
    },
    {
      "epoch": 9.09,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00039664429530201345,
      "loss": 5.226,
      "step": 9090
    },
    {
      "epoch": 9.1,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0003959731543624161,
      "loss": 6.5359,
      "step": 9100
    },
    {
      "epoch": 9.11,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0003953020134228188,
      "loss": 4.9406,
      "step": 9110
    },
    {
      "epoch": 9.12,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0003946308724832215,
      "loss": 5.3203,
      "step": 9120
    },
    {
      "epoch": 9.13,
      "grad_norm": 1.2109375,
      "learning_rate": 0.00039395973154362417,
      "loss": 5.5297,
      "step": 9130
    },
    {
      "epoch": 9.14,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00039328859060402683,
      "loss": 5.2297,
      "step": 9140
    },
    {
      "epoch": 9.15,
      "grad_norm": 0.9296875,
      "learning_rate": 0.00039261744966442955,
      "loss": 5.7469,
      "step": 9150
    },
    {
      "epoch": 9.16,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0003919463087248322,
      "loss": 6.5281,
      "step": 9160
    },
    {
      "epoch": 9.17,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00039127516778523493,
      "loss": 4.6625,
      "step": 9170
    },
    {
      "epoch": 9.18,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0003906040268456376,
      "loss": 6.5,
      "step": 9180
    },
    {
      "epoch": 9.19,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00038993288590604026,
      "loss": 6.3719,
      "step": 9190
    },
    {
      "epoch": 9.2,
      "grad_norm": 1.15625,
      "learning_rate": 0.00038926174496644293,
      "loss": 5.443,
      "step": 9200
    },
    {
      "epoch": 9.21,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00038859060402684565,
      "loss": 5.0594,
      "step": 9210
    },
    {
      "epoch": 9.22,
      "grad_norm": 2.0,
      "learning_rate": 0.00038791946308724837,
      "loss": 6.2984,
      "step": 9220
    },
    {
      "epoch": 9.23,
      "grad_norm": 0.69921875,
      "learning_rate": 0.000387248322147651,
      "loss": 5.3625,
      "step": 9230
    },
    {
      "epoch": 9.24,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0003865771812080537,
      "loss": 5.2703,
      "step": 9240
    },
    {
      "epoch": 9.25,
      "grad_norm": 1.1875,
      "learning_rate": 0.00038590604026845636,
      "loss": 6.7391,
      "step": 9250
    },
    {
      "epoch": 9.26,
      "grad_norm": 1.078125,
      "learning_rate": 0.0003852348993288591,
      "loss": 7.0969,
      "step": 9260
    },
    {
      "epoch": 9.27,
      "grad_norm": 0.99609375,
      "learning_rate": 0.0003845637583892618,
      "loss": 4.225,
      "step": 9270
    },
    {
      "epoch": 9.28,
      "grad_norm": 2.0,
      "learning_rate": 0.0003838926174496644,
      "loss": 5.6453,
      "step": 9280
    },
    {
      "epoch": 9.29,
      "grad_norm": 1.484375,
      "learning_rate": 0.00038322147651006713,
      "loss": 5.9455,
      "step": 9290
    },
    {
      "epoch": 9.3,
      "grad_norm": 1.25,
      "learning_rate": 0.0003825503355704698,
      "loss": 4.8375,
      "step": 9300
    },
    {
      "epoch": 9.31,
      "grad_norm": 1.234375,
      "learning_rate": 0.0003818791946308725,
      "loss": 6.0828,
      "step": 9310
    },
    {
      "epoch": 9.32,
      "grad_norm": 1.25,
      "learning_rate": 0.0003812080536912752,
      "loss": 6.4484,
      "step": 9320
    },
    {
      "epoch": 9.33,
      "grad_norm": 0.671875,
      "learning_rate": 0.00038053691275167785,
      "loss": 6.0172,
      "step": 9330
    },
    {
      "epoch": 9.34,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00037986577181208056,
      "loss": 5.6859,
      "step": 9340
    },
    {
      "epoch": 9.35,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00037919463087248323,
      "loss": 5.8047,
      "step": 9350
    },
    {
      "epoch": 9.36,
      "grad_norm": 2.03125,
      "learning_rate": 0.00037852348993288595,
      "loss": 5.2766,
      "step": 9360
    },
    {
      "epoch": 9.37,
      "grad_norm": 2.0,
      "learning_rate": 0.00037785234899328856,
      "loss": 5.4859,
      "step": 9370
    },
    {
      "epoch": 9.38,
      "grad_norm": 0.703125,
      "learning_rate": 0.0003771812080536913,
      "loss": 3.8516,
      "step": 9380
    },
    {
      "epoch": 9.39,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00037651006711409394,
      "loss": 5.6984,
      "step": 9390
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.66796875,
      "learning_rate": 0.00037583892617449666,
      "loss": 5.7375,
      "step": 9400
    },
    {
      "epoch": 9.41,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0003751677852348994,
      "loss": 4.7406,
      "step": 9410
    },
    {
      "epoch": 9.42,
      "grad_norm": 0.66015625,
      "learning_rate": 0.000374496644295302,
      "loss": 5.3063,
      "step": 9420
    },
    {
      "epoch": 9.43,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0003738255033557047,
      "loss": 4.6219,
      "step": 9430
    },
    {
      "epoch": 9.44,
      "grad_norm": 2.046875,
      "learning_rate": 0.0003731543624161074,
      "loss": 6.0062,
      "step": 9440
    },
    {
      "epoch": 9.45,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0003724832214765101,
      "loss": 5.8094,
      "step": 9450
    },
    {
      "epoch": 9.46,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0003718120805369127,
      "loss": 6.0844,
      "step": 9460
    },
    {
      "epoch": 9.47,
      "grad_norm": 1.0625,
      "learning_rate": 0.0003711409395973154,
      "loss": 4.9313,
      "step": 9470
    },
    {
      "epoch": 9.48,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00037046979865771815,
      "loss": 6.3406,
      "step": 9480
    },
    {
      "epoch": 9.49,
      "grad_norm": 1.265625,
      "learning_rate": 0.0003697986577181208,
      "loss": 5.3953,
      "step": 9490
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.6640625,
      "learning_rate": 0.00036912751677852353,
      "loss": 6.5531,
      "step": 9500
    },
    {
      "epoch": 9.51,
      "grad_norm": 0.6875,
      "learning_rate": 0.00036845637583892614,
      "loss": 6.0766,
      "step": 9510
    },
    {
      "epoch": 9.52,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00036778523489932886,
      "loss": 5.4891,
      "step": 9520
    },
    {
      "epoch": 9.53,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0003671140939597316,
      "loss": 4.6719,
      "step": 9530
    },
    {
      "epoch": 9.54,
      "grad_norm": 2.046875,
      "learning_rate": 0.00036644295302013424,
      "loss": 5.0422,
      "step": 9540
    },
    {
      "epoch": 9.55,
      "grad_norm": 0.6640625,
      "learning_rate": 0.00036577181208053696,
      "loss": 7.4516,
      "step": 9550
    },
    {
      "epoch": 9.56,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0003651006711409396,
      "loss": 7.5906,
      "step": 9560
    },
    {
      "epoch": 9.57,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0003644295302013423,
      "loss": 5.9125,
      "step": 9570
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.66796875,
      "learning_rate": 0.00036375838926174496,
      "loss": 5.3953,
      "step": 9580
    },
    {
      "epoch": 9.59,
      "grad_norm": 0.984375,
      "learning_rate": 0.0003630872483221477,
      "loss": 6.875,
      "step": 9590
    },
    {
      "epoch": 9.6,
      "grad_norm": 1.171875,
      "learning_rate": 0.00036241610738255034,
      "loss": 6.9375,
      "step": 9600
    },
    {
      "epoch": 9.61,
      "grad_norm": 2.0,
      "learning_rate": 0.000361744966442953,
      "loss": 5.9219,
      "step": 9610
    },
    {
      "epoch": 9.62,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0003610738255033557,
      "loss": 3.9688,
      "step": 9620
    },
    {
      "epoch": 9.63,
      "grad_norm": 2.046875,
      "learning_rate": 0.0003604026845637584,
      "loss": 6.7906,
      "step": 9630
    },
    {
      "epoch": 9.64,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0003597315436241611,
      "loss": 5.6647,
      "step": 9640
    },
    {
      "epoch": 9.65,
      "grad_norm": 1.171875,
      "learning_rate": 0.0003590604026845637,
      "loss": 4.9375,
      "step": 9650
    },
    {
      "epoch": 9.66,
      "grad_norm": 0.6640625,
      "learning_rate": 0.00035838926174496644,
      "loss": 5.8406,
      "step": 9660
    },
    {
      "epoch": 9.67,
      "grad_norm": 0.703125,
      "learning_rate": 0.00035771812080536916,
      "loss": 6.1203,
      "step": 9670
    },
    {
      "epoch": 9.68,
      "grad_norm": 1.203125,
      "learning_rate": 0.0003570469798657718,
      "loss": 6.0578,
      "step": 9680
    },
    {
      "epoch": 9.69,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0003563758389261745,
      "loss": 5.8594,
      "step": 9690
    },
    {
      "epoch": 9.7,
      "grad_norm": 1.03125,
      "learning_rate": 0.00035570469798657715,
      "loss": 4.3469,
      "step": 9700
    },
    {
      "epoch": 9.71,
      "grad_norm": 1.09375,
      "learning_rate": 0.0003550335570469799,
      "loss": 8.101,
      "step": 9710
    },
    {
      "epoch": 9.72,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0003543624161073826,
      "loss": 5.3604,
      "step": 9720
    },
    {
      "epoch": 9.73,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00035369127516778526,
      "loss": 6.4719,
      "step": 9730
    },
    {
      "epoch": 9.74,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0003530201342281879,
      "loss": 6.6917,
      "step": 9740
    },
    {
      "epoch": 9.75,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0003523489932885906,
      "loss": 6.8859,
      "step": 9750
    },
    {
      "epoch": 9.76,
      "grad_norm": 0.890625,
      "learning_rate": 0.0003516778523489933,
      "loss": 6.9984,
      "step": 9760
    },
    {
      "epoch": 9.77,
      "grad_norm": 1.234375,
      "learning_rate": 0.00035100671140939597,
      "loss": 5.9219,
      "step": 9770
    },
    {
      "epoch": 9.78,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0003503355704697987,
      "loss": 7.4826,
      "step": 9780
    },
    {
      "epoch": 9.79,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00034966442953020136,
      "loss": 6.4313,
      "step": 9790
    },
    {
      "epoch": 9.8,
      "grad_norm": 1.0078125,
      "learning_rate": 0.000348993288590604,
      "loss": 6.3066,
      "step": 9800
    },
    {
      "epoch": 9.81,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00034832214765100674,
      "loss": 5.8094,
      "step": 9810
    },
    {
      "epoch": 9.82,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0003476510067114094,
      "loss": 6.4391,
      "step": 9820
    },
    {
      "epoch": 9.83,
      "grad_norm": 1.21875,
      "learning_rate": 0.00034697986577181207,
      "loss": 5.6039,
      "step": 9830
    },
    {
      "epoch": 9.84,
      "grad_norm": 1.0546875,
      "learning_rate": 0.00034630872483221474,
      "loss": 6.0875,
      "step": 9840
    },
    {
      "epoch": 9.85,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00034563758389261745,
      "loss": 4.8266,
      "step": 9850
    },
    {
      "epoch": 9.86,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0003449664429530202,
      "loss": 5.4082,
      "step": 9860
    },
    {
      "epoch": 9.87,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00034429530201342284,
      "loss": 6.6719,
      "step": 9870
    },
    {
      "epoch": 9.88,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0003436241610738255,
      "loss": 5.9469,
      "step": 9880
    },
    {
      "epoch": 9.89,
      "grad_norm": 0.96875,
      "learning_rate": 0.00034295302013422817,
      "loss": 5.4375,
      "step": 9890
    },
    {
      "epoch": 9.9,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0003422818791946309,
      "loss": 6.1688,
      "step": 9900
    },
    {
      "epoch": 9.91,
      "grad_norm": 0.6875,
      "learning_rate": 0.0003416107382550336,
      "loss": 5.8641,
      "step": 9910
    },
    {
      "epoch": 9.92,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0003409395973154362,
      "loss": 5.8609,
      "step": 9920
    },
    {
      "epoch": 9.93,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00034026845637583894,
      "loss": 6.7453,
      "step": 9930
    },
    {
      "epoch": 9.94,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0003395973154362416,
      "loss": 5.5378,
      "step": 9940
    },
    {
      "epoch": 9.95,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0003389261744966443,
      "loss": 6.8488,
      "step": 9950
    },
    {
      "epoch": 9.96,
      "grad_norm": 2.0625,
      "learning_rate": 0.000338255033557047,
      "loss": 6.0312,
      "step": 9960
    },
    {
      "epoch": 9.97,
      "grad_norm": 0.90234375,
      "learning_rate": 0.00033758389261744965,
      "loss": 5.3461,
      "step": 9970
    },
    {
      "epoch": 9.98,
      "grad_norm": 0.99609375,
      "learning_rate": 0.00033691275167785237,
      "loss": 4.3672,
      "step": 9980
    },
    {
      "epoch": 9.99,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00033624161073825504,
      "loss": 4.9703,
      "step": 9990
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00033557046979865775,
      "loss": 5.7734,
      "step": 10000
    },
    {
      "epoch": 10.0,
      "eval_loss": 5.821718692779541,
      "eval_runtime": 23.989,
      "eval_samples_per_second": 4.169,
      "eval_steps_per_second": 4.169,
      "step": 10000
    },
    {
      "epoch": 10.01,
      "grad_norm": 1.375,
      "learning_rate": 0.0003348993288590604,
      "loss": 4.9813,
      "step": 10010
    },
    {
      "epoch": 10.02,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0003342281879194631,
      "loss": 6.7453,
      "step": 10020
    },
    {
      "epoch": 10.03,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00033355704697986575,
      "loss": 6.2969,
      "step": 10030
    },
    {
      "epoch": 10.04,
      "grad_norm": 0.703125,
      "learning_rate": 0.00033288590604026847,
      "loss": 5.6734,
      "step": 10040
    },
    {
      "epoch": 10.05,
      "grad_norm": 0.703125,
      "learning_rate": 0.0003322147651006712,
      "loss": 6.7562,
      "step": 10050
    },
    {
      "epoch": 10.06,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0003315436241610738,
      "loss": 4.9234,
      "step": 10060
    },
    {
      "epoch": 10.07,
      "grad_norm": 1.21875,
      "learning_rate": 0.0003308724832214765,
      "loss": 5.3547,
      "step": 10070
    },
    {
      "epoch": 10.08,
      "grad_norm": 0.703125,
      "learning_rate": 0.0003302013422818792,
      "loss": 6.6312,
      "step": 10080
    },
    {
      "epoch": 10.09,
      "grad_norm": 1.171875,
      "learning_rate": 0.0003295302013422819,
      "loss": 7.0516,
      "step": 10090
    },
    {
      "epoch": 10.1,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0003288590604026846,
      "loss": 4.7703,
      "step": 10100
    },
    {
      "epoch": 10.11,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00032818791946308723,
      "loss": 6.8141,
      "step": 10110
    },
    {
      "epoch": 10.12,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00032751677852348995,
      "loss": 7.3594,
      "step": 10120
    },
    {
      "epoch": 10.13,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0003268456375838926,
      "loss": 5.9594,
      "step": 10130
    },
    {
      "epoch": 10.14,
      "grad_norm": 1.15625,
      "learning_rate": 0.00032617449664429533,
      "loss": 5.3219,
      "step": 10140
    },
    {
      "epoch": 10.15,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00032550335570469795,
      "loss": 5.4141,
      "step": 10150
    },
    {
      "epoch": 10.16,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00032483221476510066,
      "loss": 5.5316,
      "step": 10160
    },
    {
      "epoch": 10.17,
      "grad_norm": 1.109375,
      "learning_rate": 0.0003241610738255034,
      "loss": 6.0297,
      "step": 10170
    },
    {
      "epoch": 10.18,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00032348993288590605,
      "loss": 6.2531,
      "step": 10180
    },
    {
      "epoch": 10.19,
      "grad_norm": 1.9609375,
      "learning_rate": 0.00032281879194630877,
      "loss": 4.5672,
      "step": 10190
    },
    {
      "epoch": 10.2,
      "grad_norm": 0.0,
      "learning_rate": 0.0003221476510067114,
      "loss": 5.8713,
      "step": 10200
    },
    {
      "epoch": 10.21,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0003214765100671141,
      "loss": 7.0625,
      "step": 10210
    },
    {
      "epoch": 10.22,
      "grad_norm": 1.078125,
      "learning_rate": 0.00032080536912751676,
      "loss": 6.3281,
      "step": 10220
    },
    {
      "epoch": 10.23,
      "grad_norm": 1.0390625,
      "learning_rate": 0.0003201342281879195,
      "loss": 5.4891,
      "step": 10230
    },
    {
      "epoch": 10.24,
      "grad_norm": 1.265625,
      "learning_rate": 0.0003194630872483222,
      "loss": 8.9313,
      "step": 10240
    },
    {
      "epoch": 10.25,
      "grad_norm": 0.703125,
      "learning_rate": 0.0003187919463087248,
      "loss": 5.5703,
      "step": 10250
    },
    {
      "epoch": 10.26,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00031812080536912753,
      "loss": 5.7016,
      "step": 10260
    },
    {
      "epoch": 10.27,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0003174496644295302,
      "loss": 4.775,
      "step": 10270
    },
    {
      "epoch": 10.28,
      "grad_norm": 0.69140625,
      "learning_rate": 0.0003167785234899329,
      "loss": 6.4422,
      "step": 10280
    },
    {
      "epoch": 10.29,
      "grad_norm": 0.6875,
      "learning_rate": 0.0003161073825503356,
      "loss": 5.8794,
      "step": 10290
    },
    {
      "epoch": 10.3,
      "grad_norm": 1.015625,
      "learning_rate": 0.00031543624161073825,
      "loss": 5.6813,
      "step": 10300
    },
    {
      "epoch": 10.31,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00031476510067114096,
      "loss": 6.725,
      "step": 10310
    },
    {
      "epoch": 10.32,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00031409395973154363,
      "loss": 5.6992,
      "step": 10320
    },
    {
      "epoch": 10.33,
      "grad_norm": 1.40625,
      "learning_rate": 0.00031342281879194635,
      "loss": 5.7516,
      "step": 10330
    },
    {
      "epoch": 10.34,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00031275167785234896,
      "loss": 5.131,
      "step": 10340
    },
    {
      "epoch": 10.35,
      "grad_norm": 1.953125,
      "learning_rate": 0.0003120805369127517,
      "loss": 6.1172,
      "step": 10350
    },
    {
      "epoch": 10.36,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0003114093959731544,
      "loss": 4.8574,
      "step": 10360
    },
    {
      "epoch": 10.37,
      "grad_norm": 0.73828125,
      "learning_rate": 0.00031073825503355706,
      "loss": 6.2875,
      "step": 10370
    },
    {
      "epoch": 10.38,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00031006711409395973,
      "loss": 4.9781,
      "step": 10380
    },
    {
      "epoch": 10.39,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0003093959731543624,
      "loss": 6.8172,
      "step": 10390
    },
    {
      "epoch": 10.4,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0003087248322147651,
      "loss": 5.0658,
      "step": 10400
    },
    {
      "epoch": 10.41,
      "grad_norm": 0.70703125,
      "learning_rate": 0.0003080536912751678,
      "loss": 6.1688,
      "step": 10410
    },
    {
      "epoch": 10.42,
      "grad_norm": 2.046875,
      "learning_rate": 0.0003073825503355705,
      "loss": 5.7156,
      "step": 10420
    },
    {
      "epoch": 10.43,
      "grad_norm": 1.015625,
      "learning_rate": 0.00030671140939597316,
      "loss": 4.8781,
      "step": 10430
    },
    {
      "epoch": 10.44,
      "grad_norm": 0.6640625,
      "learning_rate": 0.0003060402684563758,
      "loss": 6.2914,
      "step": 10440
    },
    {
      "epoch": 10.45,
      "grad_norm": 0.92578125,
      "learning_rate": 0.00030536912751677855,
      "loss": 5.3969,
      "step": 10450
    },
    {
      "epoch": 10.46,
      "grad_norm": 1.0,
      "learning_rate": 0.0003046979865771812,
      "loss": 7.1641,
      "step": 10460
    },
    {
      "epoch": 10.47,
      "grad_norm": 0.0,
      "learning_rate": 0.00030402684563758393,
      "loss": 5.1391,
      "step": 10470
    },
    {
      "epoch": 10.48,
      "grad_norm": 1.15625,
      "learning_rate": 0.0003033557046979866,
      "loss": 6.1395,
      "step": 10480
    },
    {
      "epoch": 10.49,
      "grad_norm": 0.6875,
      "learning_rate": 0.00030268456375838926,
      "loss": 6.3875,
      "step": 10490
    },
    {
      "epoch": 10.5,
      "grad_norm": 1.4453125,
      "learning_rate": 0.000302013422818792,
      "loss": 7.9719,
      "step": 10500
    },
    {
      "epoch": 10.51,
      "grad_norm": 0.66015625,
      "learning_rate": 0.00030134228187919464,
      "loss": 4.3531,
      "step": 10510
    },
    {
      "epoch": 10.52,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0003006711409395973,
      "loss": 6.4531,
      "step": 10520
    },
    {
      "epoch": 10.53,
      "grad_norm": 0.65234375,
      "learning_rate": 0.0003,
      "loss": 3.985,
      "step": 10530
    },
    {
      "epoch": 10.54,
      "grad_norm": 0.953125,
      "learning_rate": 0.0002993288590604027,
      "loss": 6.5438,
      "step": 10540
    },
    {
      "epoch": 10.55,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0002986577181208054,
      "loss": 5.7125,
      "step": 10550
    },
    {
      "epoch": 10.56,
      "grad_norm": 0.8984375,
      "learning_rate": 0.0002979865771812081,
      "loss": 4.5016,
      "step": 10560
    },
    {
      "epoch": 10.57,
      "grad_norm": 1.171875,
      "learning_rate": 0.00029731543624161074,
      "loss": 6.3982,
      "step": 10570
    },
    {
      "epoch": 10.58,
      "grad_norm": 1.3671875,
      "learning_rate": 0.0002966442953020134,
      "loss": 6.1893,
      "step": 10580
    },
    {
      "epoch": 10.59,
      "grad_norm": 1.015625,
      "learning_rate": 0.0002959731543624161,
      "loss": 3.773,
      "step": 10590
    },
    {
      "epoch": 10.6,
      "grad_norm": 1.03125,
      "learning_rate": 0.0002953020134228188,
      "loss": 6.2013,
      "step": 10600
    },
    {
      "epoch": 10.61,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00029463087248322146,
      "loss": 5.8391,
      "step": 10610
    },
    {
      "epoch": 10.62,
      "grad_norm": 1.484375,
      "learning_rate": 0.0002939597315436242,
      "loss": 7.1375,
      "step": 10620
    },
    {
      "epoch": 10.63,
      "grad_norm": 1.0,
      "learning_rate": 0.00029328859060402684,
      "loss": 4.1674,
      "step": 10630
    },
    {
      "epoch": 10.64,
      "grad_norm": 0.66796875,
      "learning_rate": 0.00029261744966442956,
      "loss": 5.3375,
      "step": 10640
    },
    {
      "epoch": 10.65,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0002919463087248322,
      "loss": 6.2594,
      "step": 10650
    },
    {
      "epoch": 10.66,
      "grad_norm": 0.65625,
      "learning_rate": 0.0002912751677852349,
      "loss": 5.7797,
      "step": 10660
    },
    {
      "epoch": 10.67,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00029060402684563755,
      "loss": 5.7969,
      "step": 10670
    },
    {
      "epoch": 10.68,
      "grad_norm": 0.671875,
      "learning_rate": 0.0002899328859060403,
      "loss": 7.3703,
      "step": 10680
    },
    {
      "epoch": 10.69,
      "grad_norm": 0.69140625,
      "learning_rate": 0.000289261744966443,
      "loss": 4.3781,
      "step": 10690
    },
    {
      "epoch": 10.7,
      "grad_norm": 0.72265625,
      "learning_rate": 0.00028859060402684566,
      "loss": 5.2531,
      "step": 10700
    },
    {
      "epoch": 10.71,
      "grad_norm": 1.25,
      "learning_rate": 0.0002879194630872483,
      "loss": 6.2406,
      "step": 10710
    },
    {
      "epoch": 10.72,
      "grad_norm": 0.69140625,
      "learning_rate": 0.000287248322147651,
      "loss": 5.6094,
      "step": 10720
    },
    {
      "epoch": 10.73,
      "grad_norm": 1.203125,
      "learning_rate": 0.0002865771812080537,
      "loss": 6.7109,
      "step": 10730
    },
    {
      "epoch": 10.74,
      "grad_norm": 1.203125,
      "learning_rate": 0.0002859060402684564,
      "loss": 6.6359,
      "step": 10740
    },
    {
      "epoch": 10.75,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00028523489932885904,
      "loss": 6.4953,
      "step": 10750
    },
    {
      "epoch": 10.76,
      "grad_norm": 1.1640625,
      "learning_rate": 0.00028456375838926176,
      "loss": 4.1328,
      "step": 10760
    },
    {
      "epoch": 10.77,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0002838926174496644,
      "loss": 5.8281,
      "step": 10770
    },
    {
      "epoch": 10.78,
      "grad_norm": 1.03125,
      "learning_rate": 0.00028322147651006714,
      "loss": 5.9641,
      "step": 10780
    },
    {
      "epoch": 10.79,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0002825503355704698,
      "loss": 5.4641,
      "step": 10790
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00028187919463087247,
      "loss": 6.0609,
      "step": 10800
    },
    {
      "epoch": 10.81,
      "grad_norm": 1.28125,
      "learning_rate": 0.0002812080536912752,
      "loss": 6.6375,
      "step": 10810
    },
    {
      "epoch": 10.82,
      "grad_norm": 1.421875,
      "learning_rate": 0.00028053691275167785,
      "loss": 7.4938,
      "step": 10820
    },
    {
      "epoch": 10.83,
      "grad_norm": 0.625,
      "learning_rate": 0.0002798657718120806,
      "loss": 7.3578,
      "step": 10830
    },
    {
      "epoch": 10.84,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0002791946308724832,
      "loss": 6.6078,
      "step": 10840
    },
    {
      "epoch": 10.85,
      "grad_norm": 1.2421875,
      "learning_rate": 0.0002785234899328859,
      "loss": 6.0406,
      "step": 10850
    },
    {
      "epoch": 10.86,
      "grad_norm": 0.96484375,
      "learning_rate": 0.00027785234899328857,
      "loss": 5.2422,
      "step": 10860
    },
    {
      "epoch": 10.87,
      "grad_norm": 0.68359375,
      "learning_rate": 0.0002771812080536913,
      "loss": 5.8844,
      "step": 10870
    },
    {
      "epoch": 10.88,
      "grad_norm": 1.1875,
      "learning_rate": 0.000276510067114094,
      "loss": 5.4703,
      "step": 10880
    },
    {
      "epoch": 10.89,
      "grad_norm": 1.03125,
      "learning_rate": 0.0002758389261744966,
      "loss": 5.5297,
      "step": 10890
    },
    {
      "epoch": 10.9,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00027516778523489934,
      "loss": 5.6516,
      "step": 10900
    },
    {
      "epoch": 10.91,
      "grad_norm": 0.96875,
      "learning_rate": 0.000274496644295302,
      "loss": 4.7969,
      "step": 10910
    },
    {
      "epoch": 10.92,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0002738255033557047,
      "loss": 4.7109,
      "step": 10920
    },
    {
      "epoch": 10.93,
      "grad_norm": 1.53125,
      "learning_rate": 0.00027315436241610744,
      "loss": 6.0391,
      "step": 10930
    },
    {
      "epoch": 10.94,
      "grad_norm": 0.61328125,
      "learning_rate": 0.00027248322147651005,
      "loss": 5.1844,
      "step": 10940
    },
    {
      "epoch": 10.95,
      "grad_norm": 1.203125,
      "learning_rate": 0.00027181208053691277,
      "loss": 5.6891,
      "step": 10950
    },
    {
      "epoch": 10.96,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00027114093959731544,
      "loss": 5.1063,
      "step": 10960
    },
    {
      "epoch": 10.97,
      "grad_norm": 0.71484375,
      "learning_rate": 0.00027046979865771815,
      "loss": 6.8328,
      "step": 10970
    },
    {
      "epoch": 10.98,
      "grad_norm": 1.15625,
      "learning_rate": 0.00026979865771812076,
      "loss": 6.5469,
      "step": 10980
    },
    {
      "epoch": 10.99,
      "grad_norm": 2.0,
      "learning_rate": 0.0002691275167785235,
      "loss": 5.9,
      "step": 10990
    },
    {
      "epoch": 11.0,
      "grad_norm": 1.0625,
      "learning_rate": 0.0002684563758389262,
      "loss": 4.9328,
      "step": 11000
    },
    {
      "epoch": 11.0,
      "eval_loss": 5.81515645980835,
      "eval_runtime": 24.0022,
      "eval_samples_per_second": 4.166,
      "eval_steps_per_second": 4.166,
      "step": 11000
    },
    {
      "epoch": 11.01,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00026778523489932887,
      "loss": 5.5328,
      "step": 11010
    },
    {
      "epoch": 11.02,
      "grad_norm": 1.125,
      "learning_rate": 0.0002671140939597316,
      "loss": 7.7234,
      "step": 11020
    },
    {
      "epoch": 11.03,
      "grad_norm": 0.69921875,
      "learning_rate": 0.0002664429530201342,
      "loss": 6.7703,
      "step": 11030
    },
    {
      "epoch": 11.04,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0002657718120805369,
      "loss": 5.2391,
      "step": 11040
    },
    {
      "epoch": 11.05,
      "grad_norm": 0.62890625,
      "learning_rate": 0.0002651006711409396,
      "loss": 7.4437,
      "step": 11050
    },
    {
      "epoch": 11.06,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0002644295302013423,
      "loss": 4.6969,
      "step": 11060
    },
    {
      "epoch": 11.07,
      "grad_norm": 0.95703125,
      "learning_rate": 0.00026375838926174497,
      "loss": 5.8656,
      "step": 11070
    },
    {
      "epoch": 11.08,
      "grad_norm": 1.203125,
      "learning_rate": 0.00026308724832214763,
      "loss": 5.4516,
      "step": 11080
    },
    {
      "epoch": 11.09,
      "grad_norm": 0.9609375,
      "learning_rate": 0.00026241610738255035,
      "loss": 6.0195,
      "step": 11090
    },
    {
      "epoch": 11.1,
      "grad_norm": 1.0078125,
      "learning_rate": 0.000261744966442953,
      "loss": 8.0844,
      "step": 11100
    },
    {
      "epoch": 11.11,
      "grad_norm": 0.98828125,
      "learning_rate": 0.00026107382550335573,
      "loss": 5.0594,
      "step": 11110
    },
    {
      "epoch": 11.12,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0002604026845637584,
      "loss": 6.1516,
      "step": 11120
    },
    {
      "epoch": 11.13,
      "grad_norm": 0.671875,
      "learning_rate": 0.00025973154362416106,
      "loss": 5.3781,
      "step": 11130
    },
    {
      "epoch": 11.14,
      "grad_norm": 0.671875,
      "learning_rate": 0.0002590604026845638,
      "loss": 5.8563,
      "step": 11140
    },
    {
      "epoch": 11.15,
      "grad_norm": 2.03125,
      "learning_rate": 0.00025838926174496645,
      "loss": 5.4813,
      "step": 11150
    },
    {
      "epoch": 11.16,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00025771812080536917,
      "loss": 5.6531,
      "step": 11160
    },
    {
      "epoch": 11.17,
      "grad_norm": 0.66015625,
      "learning_rate": 0.0002570469798657718,
      "loss": 5.2971,
      "step": 11170
    },
    {
      "epoch": 11.18,
      "grad_norm": 1.03125,
      "learning_rate": 0.0002563758389261745,
      "loss": 6.2462,
      "step": 11180
    },
    {
      "epoch": 11.19,
      "grad_norm": 0.671875,
      "learning_rate": 0.0002557046979865772,
      "loss": 4.7344,
      "step": 11190
    },
    {
      "epoch": 11.2,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0002550335570469799,
      "loss": 5.2641,
      "step": 11200
    },
    {
      "epoch": 11.21,
      "grad_norm": 2.0,
      "learning_rate": 0.00025436241610738255,
      "loss": 5.875,
      "step": 11210
    },
    {
      "epoch": 11.22,
      "grad_norm": 2.015625,
      "learning_rate": 0.0002536912751677852,
      "loss": 6.2281,
      "step": 11220
    },
    {
      "epoch": 11.23,
      "grad_norm": 0.83984375,
      "learning_rate": 0.00025302013422818793,
      "loss": 6.2328,
      "step": 11230
    },
    {
      "epoch": 11.24,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0002523489932885906,
      "loss": 5.4234,
      "step": 11240
    },
    {
      "epoch": 11.25,
      "grad_norm": 2.046875,
      "learning_rate": 0.0002516778523489933,
      "loss": 7.5215,
      "step": 11250
    },
    {
      "epoch": 11.26,
      "grad_norm": 0.6953125,
      "learning_rate": 0.000251006711409396,
      "loss": 7.1859,
      "step": 11260
    },
    {
      "epoch": 11.27,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00025033557046979865,
      "loss": 5.9656,
      "step": 11270
    },
    {
      "epoch": 11.28,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00024966442953020136,
      "loss": 3.9261,
      "step": 11280
    },
    {
      "epoch": 11.29,
      "grad_norm": 0.703125,
      "learning_rate": 0.00024899328859060403,
      "loss": 4.4984,
      "step": 11290
    },
    {
      "epoch": 11.3,
      "grad_norm": 1.171875,
      "learning_rate": 0.0002483221476510067,
      "loss": 6.2094,
      "step": 11300
    },
    {
      "epoch": 11.31,
      "grad_norm": 0.7109375,
      "learning_rate": 0.0002476510067114094,
      "loss": 5.4922,
      "step": 11310
    },
    {
      "epoch": 11.32,
      "grad_norm": 0.6875,
      "learning_rate": 0.0002469798657718121,
      "loss": 6.2672,
      "step": 11320
    },
    {
      "epoch": 11.33,
      "grad_norm": 1.1796875,
      "learning_rate": 0.0002463087248322148,
      "loss": 5.7266,
      "step": 11330
    },
    {
      "epoch": 11.34,
      "grad_norm": 1.40625,
      "learning_rate": 0.00024563758389261746,
      "loss": 6.3531,
      "step": 11340
    },
    {
      "epoch": 11.35,
      "grad_norm": 0.6953125,
      "learning_rate": 0.00024496644295302013,
      "loss": 7.2922,
      "step": 11350
    },
    {
      "epoch": 11.36,
      "grad_norm": 0.6875,
      "learning_rate": 0.0002442953020134228,
      "loss": 5.6172,
      "step": 11360
    },
    {
      "epoch": 11.37,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0002436241610738255,
      "loss": 6.4266,
      "step": 11370
    },
    {
      "epoch": 11.38,
      "grad_norm": 0.71875,
      "learning_rate": 0.00024295302013422818,
      "loss": 4.5766,
      "step": 11380
    },
    {
      "epoch": 11.39,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00024228187919463087,
      "loss": 6.6609,
      "step": 11390
    },
    {
      "epoch": 11.4,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00024161073825503356,
      "loss": 5.2031,
      "step": 11400
    },
    {
      "epoch": 11.41,
      "grad_norm": 0.66796875,
      "learning_rate": 0.00024093959731543625,
      "loss": 6.225,
      "step": 11410
    },
    {
      "epoch": 11.42,
      "grad_norm": 1.15625,
      "learning_rate": 0.00024026845637583895,
      "loss": 4.6891,
      "step": 11420
    },
    {
      "epoch": 11.43,
      "grad_norm": 0.61328125,
      "learning_rate": 0.0002395973154362416,
      "loss": 6.1141,
      "step": 11430
    },
    {
      "epoch": 11.44,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0002389261744966443,
      "loss": 4.9219,
      "step": 11440
    },
    {
      "epoch": 11.45,
      "grad_norm": 0.703125,
      "learning_rate": 0.00023825503355704697,
      "loss": 6.2188,
      "step": 11450
    },
    {
      "epoch": 11.46,
      "grad_norm": 1.1875,
      "learning_rate": 0.00023758389261744966,
      "loss": 5.1692,
      "step": 11460
    },
    {
      "epoch": 11.47,
      "grad_norm": 1.171875,
      "learning_rate": 0.00023691275167785238,
      "loss": 4.6188,
      "step": 11470
    },
    {
      "epoch": 11.48,
      "grad_norm": 1.296875,
      "learning_rate": 0.00023624161073825504,
      "loss": 6.3203,
      "step": 11480
    },
    {
      "epoch": 11.49,
      "grad_norm": 0.671875,
      "learning_rate": 0.00023557046979865774,
      "loss": 7.3734,
      "step": 11490
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0002348993288590604,
      "loss": 6.5062,
      "step": 11500
    },
    {
      "epoch": 11.51,
      "grad_norm": 1.03125,
      "learning_rate": 0.0002342281879194631,
      "loss": 4.9375,
      "step": 11510
    },
    {
      "epoch": 11.52,
      "grad_norm": 1.015625,
      "learning_rate": 0.00023355704697986576,
      "loss": 6.8734,
      "step": 11520
    },
    {
      "epoch": 11.53,
      "grad_norm": 1.171875,
      "learning_rate": 0.00023288590604026848,
      "loss": 6.5391,
      "step": 11530
    },
    {
      "epoch": 11.54,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00023221476510067114,
      "loss": 4.1031,
      "step": 11540
    },
    {
      "epoch": 11.55,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00023154362416107383,
      "loss": 5.9344,
      "step": 11550
    },
    {
      "epoch": 11.56,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00023087248322147653,
      "loss": 4.2469,
      "step": 11560
    },
    {
      "epoch": 11.57,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0002302013422818792,
      "loss": 4.9646,
      "step": 11570
    },
    {
      "epoch": 11.58,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00022953020134228188,
      "loss": 6.2058,
      "step": 11580
    },
    {
      "epoch": 11.59,
      "grad_norm": 1.2109375,
      "learning_rate": 0.00022885906040268455,
      "loss": 6.6188,
      "step": 11590
    },
    {
      "epoch": 11.6,
      "grad_norm": 1.40625,
      "learning_rate": 0.00022818791946308727,
      "loss": 4.5422,
      "step": 11600
    },
    {
      "epoch": 11.61,
      "grad_norm": 2.0,
      "learning_rate": 0.00022751677852348993,
      "loss": 6.5047,
      "step": 11610
    },
    {
      "epoch": 11.62,
      "grad_norm": 1.9296875,
      "learning_rate": 0.00022684563758389262,
      "loss": 5.0719,
      "step": 11620
    },
    {
      "epoch": 11.63,
      "grad_norm": 1.015625,
      "learning_rate": 0.00022617449664429532,
      "loss": 5.0422,
      "step": 11630
    },
    {
      "epoch": 11.64,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00022550335570469798,
      "loss": 7.1016,
      "step": 11640
    },
    {
      "epoch": 11.65,
      "grad_norm": 2.046875,
      "learning_rate": 0.00022483221476510067,
      "loss": 6.4578,
      "step": 11650
    },
    {
      "epoch": 11.66,
      "grad_norm": 1.03125,
      "learning_rate": 0.00022416107382550337,
      "loss": 5.1688,
      "step": 11660
    },
    {
      "epoch": 11.67,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00022348993288590606,
      "loss": 5.5734,
      "step": 11670
    },
    {
      "epoch": 11.68,
      "grad_norm": 0.97265625,
      "learning_rate": 0.00022281879194630872,
      "loss": 4.8719,
      "step": 11680
    },
    {
      "epoch": 11.69,
      "grad_norm": 1.984375,
      "learning_rate": 0.00022214765100671141,
      "loss": 5.7328,
      "step": 11690
    },
    {
      "epoch": 11.7,
      "grad_norm": 0.703125,
      "learning_rate": 0.0002214765100671141,
      "loss": 4.4875,
      "step": 11700
    },
    {
      "epoch": 11.71,
      "grad_norm": 0.0,
      "learning_rate": 0.00022080536912751677,
      "loss": 4.797,
      "step": 11710
    },
    {
      "epoch": 11.72,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00022013422818791946,
      "loss": 5.6703,
      "step": 11720
    },
    {
      "epoch": 11.73,
      "grad_norm": 0.91796875,
      "learning_rate": 0.00021946308724832216,
      "loss": 5.6875,
      "step": 11730
    },
    {
      "epoch": 11.74,
      "grad_norm": 0.8828125,
      "learning_rate": 0.00021879194630872485,
      "loss": 4.1141,
      "step": 11740
    },
    {
      "epoch": 11.75,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0002181208053691275,
      "loss": 6.6047,
      "step": 11750
    },
    {
      "epoch": 11.76,
      "grad_norm": 0.66796875,
      "learning_rate": 0.0002174496644295302,
      "loss": 7.3,
      "step": 11760
    },
    {
      "epoch": 11.77,
      "grad_norm": 1.2578125,
      "learning_rate": 0.00021677852348993287,
      "loss": 5.5687,
      "step": 11770
    },
    {
      "epoch": 11.78,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00021610738255033556,
      "loss": 6.2132,
      "step": 11780
    },
    {
      "epoch": 11.79,
      "grad_norm": 1.4453125,
      "learning_rate": 0.00021543624161073828,
      "loss": 5.9953,
      "step": 11790
    },
    {
      "epoch": 11.8,
      "grad_norm": 0.69140625,
      "learning_rate": 0.00021476510067114095,
      "loss": 5.1594,
      "step": 11800
    },
    {
      "epoch": 11.81,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00021409395973154364,
      "loss": 6.0344,
      "step": 11810
    },
    {
      "epoch": 11.82,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0002134228187919463,
      "loss": 6.2719,
      "step": 11820
    },
    {
      "epoch": 11.83,
      "grad_norm": 1.2265625,
      "learning_rate": 0.000212751677852349,
      "loss": 5.6891,
      "step": 11830
    },
    {
      "epoch": 11.84,
      "grad_norm": 1.0390625,
      "learning_rate": 0.00021208053691275166,
      "loss": 9.4344,
      "step": 11840
    },
    {
      "epoch": 11.85,
      "grad_norm": 1.1875,
      "learning_rate": 0.00021140939597315438,
      "loss": 6.5734,
      "step": 11850
    },
    {
      "epoch": 11.86,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00021073825503355707,
      "loss": 6.6125,
      "step": 11860
    },
    {
      "epoch": 11.87,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00021006711409395974,
      "loss": 5.7687,
      "step": 11870
    },
    {
      "epoch": 11.88,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00020939597315436243,
      "loss": 5.8552,
      "step": 11880
    },
    {
      "epoch": 11.89,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0002087248322147651,
      "loss": 7.4016,
      "step": 11890
    },
    {
      "epoch": 11.9,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00020805369127516779,
      "loss": 5.7938,
      "step": 11900
    },
    {
      "epoch": 11.91,
      "grad_norm": 0.6875,
      "learning_rate": 0.00020738255033557045,
      "loss": 4.5719,
      "step": 11910
    },
    {
      "epoch": 11.92,
      "grad_norm": 1.0390625,
      "learning_rate": 0.00020671140939597317,
      "loss": 6.7266,
      "step": 11920
    },
    {
      "epoch": 11.93,
      "grad_norm": 0.671875,
      "learning_rate": 0.00020604026845637586,
      "loss": 5.9297,
      "step": 11930
    },
    {
      "epoch": 11.94,
      "grad_norm": 2.015625,
      "learning_rate": 0.00020536912751677853,
      "loss": 6.1953,
      "step": 11940
    },
    {
      "epoch": 11.95,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00020469798657718122,
      "loss": 5.6672,
      "step": 11950
    },
    {
      "epoch": 11.96,
      "grad_norm": 0.6484375,
      "learning_rate": 0.00020402684563758388,
      "loss": 6.0594,
      "step": 11960
    },
    {
      "epoch": 11.97,
      "grad_norm": 1.21875,
      "learning_rate": 0.00020335570469798658,
      "loss": 5.4891,
      "step": 11970
    },
    {
      "epoch": 11.98,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00020268456375838927,
      "loss": 5.5516,
      "step": 11980
    },
    {
      "epoch": 11.99,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00020201342281879196,
      "loss": 6.5469,
      "step": 11990
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.671875,
      "learning_rate": 0.00020134228187919463,
      "loss": 6.2109,
      "step": 12000
    },
    {
      "epoch": 12.0,
      "eval_loss": 5.819843769073486,
      "eval_runtime": 24.0425,
      "eval_samples_per_second": 4.159,
      "eval_steps_per_second": 4.159,
      "step": 12000
    },
    {
      "epoch": 12.01,
      "grad_norm": 1.046875,
      "learning_rate": 0.00020067114093959732,
      "loss": 5.1391,
      "step": 12010
    },
    {
      "epoch": 12.02,
      "grad_norm": 1.390625,
      "learning_rate": 0.0002,
      "loss": 4.9016,
      "step": 12020
    },
    {
      "epoch": 12.03,
      "grad_norm": 1.2578125,
      "learning_rate": 0.00019932885906040267,
      "loss": 4.8016,
      "step": 12030
    },
    {
      "epoch": 12.04,
      "grad_norm": 1.4765625,
      "learning_rate": 0.0001986577181208054,
      "loss": 6.7438,
      "step": 12040
    },
    {
      "epoch": 12.05,
      "grad_norm": 0.99609375,
      "learning_rate": 0.00019798657718120806,
      "loss": 6.2359,
      "step": 12050
    },
    {
      "epoch": 12.06,
      "grad_norm": 1.421875,
      "learning_rate": 0.00019731543624161075,
      "loss": 5.2313,
      "step": 12060
    },
    {
      "epoch": 12.07,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00019664429530201342,
      "loss": 5.8984,
      "step": 12070
    },
    {
      "epoch": 12.08,
      "grad_norm": 1.265625,
      "learning_rate": 0.0001959731543624161,
      "loss": 5.2906,
      "step": 12080
    },
    {
      "epoch": 12.09,
      "grad_norm": 1.453125,
      "learning_rate": 0.0001953020134228188,
      "loss": 9.1422,
      "step": 12090
    },
    {
      "epoch": 12.1,
      "grad_norm": 0.98046875,
      "learning_rate": 0.00019463087248322146,
      "loss": 4.8406,
      "step": 12100
    },
    {
      "epoch": 12.11,
      "grad_norm": 0.83203125,
      "learning_rate": 0.00019395973154362418,
      "loss": 7.2777,
      "step": 12110
    },
    {
      "epoch": 12.12,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00019328859060402685,
      "loss": 7.1,
      "step": 12120
    },
    {
      "epoch": 12.13,
      "grad_norm": 2.078125,
      "learning_rate": 0.00019261744966442954,
      "loss": 5.9766,
      "step": 12130
    },
    {
      "epoch": 12.14,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0001919463087248322,
      "loss": 5.3547,
      "step": 12140
    },
    {
      "epoch": 12.15,
      "grad_norm": 1.96875,
      "learning_rate": 0.0001912751677852349,
      "loss": 5.5542,
      "step": 12150
    },
    {
      "epoch": 12.16,
      "grad_norm": 0.6796875,
      "learning_rate": 0.0001906040268456376,
      "loss": 6.9516,
      "step": 12160
    },
    {
      "epoch": 12.17,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00018993288590604028,
      "loss": 5.8391,
      "step": 12170
    },
    {
      "epoch": 12.18,
      "grad_norm": 1.171875,
      "learning_rate": 0.00018926174496644297,
      "loss": 3.8953,
      "step": 12180
    },
    {
      "epoch": 12.19,
      "grad_norm": 0.71875,
      "learning_rate": 0.00018859060402684564,
      "loss": 4.9453,
      "step": 12190
    },
    {
      "epoch": 12.2,
      "grad_norm": 1.2109375,
      "learning_rate": 0.00018791946308724833,
      "loss": 6.2703,
      "step": 12200
    },
    {
      "epoch": 12.21,
      "grad_norm": 2.0,
      "learning_rate": 0.000187248322147651,
      "loss": 6.4203,
      "step": 12210
    },
    {
      "epoch": 12.22,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0001865771812080537,
      "loss": 6.2734,
      "step": 12220
    },
    {
      "epoch": 12.23,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00018590604026845635,
      "loss": 5.4813,
      "step": 12230
    },
    {
      "epoch": 12.24,
      "grad_norm": 0.703125,
      "learning_rate": 0.00018523489932885907,
      "loss": 6.1922,
      "step": 12240
    },
    {
      "epoch": 12.25,
      "grad_norm": 0.6875,
      "learning_rate": 0.00018456375838926176,
      "loss": 5.0844,
      "step": 12250
    },
    {
      "epoch": 12.26,
      "grad_norm": 1.46875,
      "learning_rate": 0.00018389261744966443,
      "loss": 6.2484,
      "step": 12260
    },
    {
      "epoch": 12.27,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00018322147651006712,
      "loss": 6.325,
      "step": 12270
    },
    {
      "epoch": 12.28,
      "grad_norm": 0.6875,
      "learning_rate": 0.0001825503355704698,
      "loss": 5.0563,
      "step": 12280
    },
    {
      "epoch": 12.29,
      "grad_norm": 0.89453125,
      "learning_rate": 0.00018187919463087248,
      "loss": 4.5105,
      "step": 12290
    },
    {
      "epoch": 12.3,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00018120805369127517,
      "loss": 5.9578,
      "step": 12300
    },
    {
      "epoch": 12.31,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00018053691275167786,
      "loss": 5.1198,
      "step": 12310
    },
    {
      "epoch": 12.32,
      "grad_norm": 1.28125,
      "learning_rate": 0.00017986577181208056,
      "loss": 7.4328,
      "step": 12320
    },
    {
      "epoch": 12.33,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00017919463087248322,
      "loss": 4.9766,
      "step": 12330
    },
    {
      "epoch": 12.34,
      "grad_norm": 1.4375,
      "learning_rate": 0.0001785234899328859,
      "loss": 6.8781,
      "step": 12340
    },
    {
      "epoch": 12.35,
      "grad_norm": 0.96484375,
      "learning_rate": 0.00017785234899328858,
      "loss": 6.8859,
      "step": 12350
    },
    {
      "epoch": 12.36,
      "grad_norm": 1.234375,
      "learning_rate": 0.0001771812080536913,
      "loss": 6.0932,
      "step": 12360
    },
    {
      "epoch": 12.37,
      "grad_norm": 2.046875,
      "learning_rate": 0.00017651006711409396,
      "loss": 7.7344,
      "step": 12370
    },
    {
      "epoch": 12.38,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00017583892617449665,
      "loss": 6.5141,
      "step": 12380
    },
    {
      "epoch": 12.39,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00017516778523489935,
      "loss": 6.875,
      "step": 12390
    },
    {
      "epoch": 12.4,
      "grad_norm": 0.96875,
      "learning_rate": 0.000174496644295302,
      "loss": 5.4938,
      "step": 12400
    },
    {
      "epoch": 12.41,
      "grad_norm": 0.7265625,
      "learning_rate": 0.0001738255033557047,
      "loss": 6.1484,
      "step": 12410
    },
    {
      "epoch": 12.42,
      "grad_norm": 1.234375,
      "learning_rate": 0.00017315436241610737,
      "loss": 6.2824,
      "step": 12420
    },
    {
      "epoch": 12.43,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0001724832214765101,
      "loss": 4.7219,
      "step": 12430
    },
    {
      "epoch": 12.44,
      "grad_norm": 0.7109375,
      "learning_rate": 0.00017181208053691275,
      "loss": 5.2297,
      "step": 12440
    },
    {
      "epoch": 12.45,
      "grad_norm": 1.03125,
      "learning_rate": 0.00017114093959731544,
      "loss": 5.1234,
      "step": 12450
    },
    {
      "epoch": 12.46,
      "grad_norm": 0.71484375,
      "learning_rate": 0.0001704697986577181,
      "loss": 3.9609,
      "step": 12460
    },
    {
      "epoch": 12.47,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0001697986577181208,
      "loss": 4.9359,
      "step": 12470
    },
    {
      "epoch": 12.48,
      "grad_norm": 1.203125,
      "learning_rate": 0.0001691275167785235,
      "loss": 8.0375,
      "step": 12480
    },
    {
      "epoch": 12.49,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00016845637583892619,
      "loss": 5.4031,
      "step": 12490
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.6875,
      "learning_rate": 0.00016778523489932888,
      "loss": 6.1625,
      "step": 12500
    },
    {
      "epoch": 12.51,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00016711409395973154,
      "loss": 6.4359,
      "step": 12510
    },
    {
      "epoch": 12.52,
      "grad_norm": 1.1875,
      "learning_rate": 0.00016644295302013423,
      "loss": 4.6141,
      "step": 12520
    },
    {
      "epoch": 12.53,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0001657718120805369,
      "loss": 4.8838,
      "step": 12530
    },
    {
      "epoch": 12.54,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0001651006711409396,
      "loss": 6.0516,
      "step": 12540
    },
    {
      "epoch": 12.55,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0001644295302013423,
      "loss": 4.7547,
      "step": 12550
    },
    {
      "epoch": 12.56,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00016375838926174498,
      "loss": 4.7625,
      "step": 12560
    },
    {
      "epoch": 12.57,
      "grad_norm": 0.87890625,
      "learning_rate": 0.00016308724832214767,
      "loss": 5.1641,
      "step": 12570
    },
    {
      "epoch": 12.58,
      "grad_norm": 1.203125,
      "learning_rate": 0.00016241610738255033,
      "loss": 4.9109,
      "step": 12580
    },
    {
      "epoch": 12.59,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00016174496644295302,
      "loss": 5.5812,
      "step": 12590
    },
    {
      "epoch": 12.6,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0001610738255033557,
      "loss": 5.9625,
      "step": 12600
    },
    {
      "epoch": 12.61,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00016040268456375838,
      "loss": 6.7844,
      "step": 12610
    },
    {
      "epoch": 12.62,
      "grad_norm": 1.3984375,
      "learning_rate": 0.0001597315436241611,
      "loss": 5.6797,
      "step": 12620
    },
    {
      "epoch": 12.63,
      "grad_norm": 0.671875,
      "learning_rate": 0.00015906040268456377,
      "loss": 5.2625,
      "step": 12630
    },
    {
      "epoch": 12.64,
      "grad_norm": 0.6796875,
      "learning_rate": 0.00015838926174496646,
      "loss": 6.2781,
      "step": 12640
    },
    {
      "epoch": 12.65,
      "grad_norm": 1.453125,
      "learning_rate": 0.00015771812080536912,
      "loss": 7.4828,
      "step": 12650
    },
    {
      "epoch": 12.66,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00015704697986577181,
      "loss": 6.2453,
      "step": 12660
    },
    {
      "epoch": 12.67,
      "grad_norm": 1.046875,
      "learning_rate": 0.00015637583892617448,
      "loss": 5.8531,
      "step": 12670
    },
    {
      "epoch": 12.68,
      "grad_norm": 0.8984375,
      "learning_rate": 0.0001557046979865772,
      "loss": 4.9938,
      "step": 12680
    },
    {
      "epoch": 12.69,
      "grad_norm": 0.94140625,
      "learning_rate": 0.00015503355704697986,
      "loss": 6.5219,
      "step": 12690
    },
    {
      "epoch": 12.7,
      "grad_norm": 0.9609375,
      "learning_rate": 0.00015436241610738256,
      "loss": 5.1489,
      "step": 12700
    },
    {
      "epoch": 12.71,
      "grad_norm": 0.9921875,
      "learning_rate": 0.00015369127516778525,
      "loss": 6.3578,
      "step": 12710
    },
    {
      "epoch": 12.72,
      "grad_norm": 0.703125,
      "learning_rate": 0.0001530201342281879,
      "loss": 6.0094,
      "step": 12720
    },
    {
      "epoch": 12.73,
      "grad_norm": 0.6953125,
      "learning_rate": 0.0001523489932885906,
      "loss": 7.7219,
      "step": 12730
    },
    {
      "epoch": 12.74,
      "grad_norm": 1.140625,
      "learning_rate": 0.0001516778523489933,
      "loss": 5.8422,
      "step": 12740
    },
    {
      "epoch": 12.75,
      "grad_norm": 0.6796875,
      "learning_rate": 0.000151006711409396,
      "loss": 5.937,
      "step": 12750
    },
    {
      "epoch": 12.76,
      "grad_norm": 1.1953125,
      "learning_rate": 0.00015033557046979865,
      "loss": 5.0609,
      "step": 12760
    },
    {
      "epoch": 12.77,
      "grad_norm": 0.6875,
      "learning_rate": 0.00014966442953020135,
      "loss": 5.3569,
      "step": 12770
    },
    {
      "epoch": 12.78,
      "grad_norm": 1.03125,
      "learning_rate": 0.00014899328859060404,
      "loss": 5.1141,
      "step": 12780
    },
    {
      "epoch": 12.79,
      "grad_norm": 1.21875,
      "learning_rate": 0.0001483221476510067,
      "loss": 6.7609,
      "step": 12790
    },
    {
      "epoch": 12.8,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0001476510067114094,
      "loss": 6.1672,
      "step": 12800
    },
    {
      "epoch": 12.81,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0001469798657718121,
      "loss": 5.1068,
      "step": 12810
    },
    {
      "epoch": 12.82,
      "grad_norm": 2.0,
      "learning_rate": 0.00014630872483221478,
      "loss": 5.4875,
      "step": 12820
    },
    {
      "epoch": 12.83,
      "grad_norm": 0.67578125,
      "learning_rate": 0.00014563758389261744,
      "loss": 5.2234,
      "step": 12830
    },
    {
      "epoch": 12.84,
      "grad_norm": 0.68359375,
      "learning_rate": 0.00014496644295302014,
      "loss": 5.3453,
      "step": 12840
    },
    {
      "epoch": 12.85,
      "grad_norm": 1.171875,
      "learning_rate": 0.00014429530201342283,
      "loss": 6.0109,
      "step": 12850
    },
    {
      "epoch": 12.86,
      "grad_norm": 0.703125,
      "learning_rate": 0.0001436241610738255,
      "loss": 6.3466,
      "step": 12860
    },
    {
      "epoch": 12.87,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0001429530201342282,
      "loss": 6.5172,
      "step": 12870
    },
    {
      "epoch": 12.88,
      "grad_norm": 2.0,
      "learning_rate": 0.00014228187919463088,
      "loss": 6.5094,
      "step": 12880
    },
    {
      "epoch": 12.89,
      "grad_norm": 0.69921875,
      "learning_rate": 0.00014161073825503357,
      "loss": 5.873,
      "step": 12890
    },
    {
      "epoch": 12.9,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00014093959731543624,
      "loss": 6.65,
      "step": 12900
    },
    {
      "epoch": 12.91,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00014026845637583893,
      "loss": 4.3156,
      "step": 12910
    },
    {
      "epoch": 12.92,
      "grad_norm": 1.15625,
      "learning_rate": 0.0001395973154362416,
      "loss": 5.1578,
      "step": 12920
    },
    {
      "epoch": 12.93,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00013892617449664428,
      "loss": 5.9367,
      "step": 12930
    },
    {
      "epoch": 12.94,
      "grad_norm": 0.0,
      "learning_rate": 0.000138255033557047,
      "loss": 7.1706,
      "step": 12940
    },
    {
      "epoch": 12.95,
      "grad_norm": 1.171875,
      "learning_rate": 0.00013758389261744967,
      "loss": 5.5922,
      "step": 12950
    },
    {
      "epoch": 12.96,
      "grad_norm": 0.70703125,
      "learning_rate": 0.00013691275167785236,
      "loss": 4.6922,
      "step": 12960
    },
    {
      "epoch": 12.97,
      "grad_norm": 2.03125,
      "learning_rate": 0.00013624161073825503,
      "loss": 7.5781,
      "step": 12970
    },
    {
      "epoch": 12.98,
      "grad_norm": 0.99609375,
      "learning_rate": 0.00013557046979865772,
      "loss": 7.0281,
      "step": 12980
    },
    {
      "epoch": 12.99,
      "grad_norm": 1.3984375,
      "learning_rate": 0.00013489932885906038,
      "loss": 5.2297,
      "step": 12990
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.109375,
      "learning_rate": 0.0001342281879194631,
      "loss": 6.8812,
      "step": 13000
    },
    {
      "epoch": 13.0,
      "eval_loss": 5.815000057220459,
      "eval_runtime": 24.0033,
      "eval_samples_per_second": 4.166,
      "eval_steps_per_second": 4.166,
      "step": 13000
    }
  ],
  "logging_steps": 10,
  "max_steps": 15000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.514295583220403e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
